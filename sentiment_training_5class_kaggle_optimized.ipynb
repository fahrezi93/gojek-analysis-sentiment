{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d2a6a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# CELL 1: SETUP & GPU CHECK\n",
    "# ============================================\n",
    "\n",
    "!pip install transformers accelerate -q\n",
    "\n",
    "import torch\n",
    "import os\n",
    "\n",
    "print('='*60)\n",
    "print('üñ•Ô∏è  SYSTEM INFO')\n",
    "print('='*60)\n",
    "print(f'PyTorch: {torch.__version__}')\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    n_gpu = torch.cuda.device_count()\n",
    "    print(f'‚úÖ GPU Available: {n_gpu} GPU(s)')\n",
    "    for i in range(n_gpu):\n",
    "        gpu_name = torch.cuda.get_device_name(i)\n",
    "        gpu_mem = torch.cuda.get_device_properties(i).total_memory / 1024**3\n",
    "        print(f'   GPU {i}: {gpu_name} ({gpu_mem:.1f} GB)')\n",
    "else:\n",
    "    print('‚ö†Ô∏è GPU not available!')\n",
    "\n",
    "# List input files\n",
    "print('\\nüìÅ Input files:')\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(f'   {os.path.join(dirname, filename)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1677d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# CELL 2: IMPORTS & SEED\n",
    "# ============================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "from transformers import BertTokenizer, BertModel, get_linear_schedule_with_warmup\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_recall_fscore_support,\n",
    "    classification_report, confusion_matrix, f1_score\n",
    ")\n",
    "import random\n",
    "import copy\n",
    "import json\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Reproducibility\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "# Device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'üéÆ Using device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d86c0fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# CELL 3: LOAD DATA\n",
    "# ============================================\n",
    "\n",
    "# Auto-detect data file\n",
    "DATA_PATH = None\n",
    "search_patterns = ['gojek_reviews_5class_clean', 'gojek_reviews_5class', 'gojek']\n",
    "\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for pattern in search_patterns:\n",
    "        for filename in filenames:\n",
    "            if pattern in filename and filename.endswith('.csv'):\n",
    "                DATA_PATH = os.path.join(dirname, filename)\n",
    "                break\n",
    "        if DATA_PATH:\n",
    "            break\n",
    "    if DATA_PATH:\n",
    "        break\n",
    "\n",
    "if DATA_PATH:\n",
    "    print(f'‚úÖ Found: {DATA_PATH}')\n",
    "    df = pd.read_csv(DATA_PATH)\n",
    "else:\n",
    "    print('‚ùå Data not found! Available files:')\n",
    "    for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "        for f in filenames:\n",
    "            print(f'   {os.path.join(dirname, f)}')\n",
    "    raise FileNotFoundError('Please upload gojek_reviews_5class_clean.csv')\n",
    "\n",
    "# Data overview\n",
    "print('\\n' + '='*60)\n",
    "print('üìä DATA OVERVIEW')\n",
    "print('='*60)\n",
    "print(f'Total samples: {len(df):,}')\n",
    "print(f'Columns: {df.columns.tolist()}')\n",
    "\n",
    "# Check required columns\n",
    "text_col = 'content_clean' if 'content_clean' in df.columns else 'content'\n",
    "print(f'\\nText column: {text_col}')\n",
    "print(f'\\nüìà Sentiment Distribution:')\n",
    "print(df['sentiment'].value_counts())\n",
    "\n",
    "# Visualize\n",
    "fig, ax = plt.subplots(figsize=(10, 4))\n",
    "colors = {\n",
    "    'sangat_negatif': '#c0392b', \n",
    "    'negatif': '#e74c3c',\n",
    "    'netral': '#f39c12',\n",
    "    'positif': '#2ecc71',\n",
    "    'sangat_positif': '#27ae60'\n",
    "}\n",
    "order = ['sangat_negatif', 'negatif', 'netral', 'positif', 'sangat_positif']\n",
    "counts = df['sentiment'].value_counts().reindex(order)\n",
    "bars = ax.bar(counts.index, counts.values, color=[colors.get(s, '#3498db') for s in counts.index])\n",
    "ax.set_title('Sentiment Distribution (5-Class)', fontsize=14, fontweight='bold')\n",
    "ax.set_ylabel('Count')\n",
    "plt.xticks(rotation=15)\n",
    "for bar, count in zip(bars, counts.values):\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 50, \n",
    "            f'{count:,}', ha='center', fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa756ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# CELL 4: CONFIGURATION\n",
    "# ============================================\n",
    "\n",
    "# Label mapping for 5-class\n",
    "LABEL_MAP = {\n",
    "    'sangat_negatif': 0, \n",
    "    'negatif': 1, \n",
    "    'netral': 2, \n",
    "    'positif': 3, \n",
    "    'sangat_positif': 4\n",
    "}\n",
    "LABEL_NAMES = ['sangat_negatif', 'negatif', 'netral', 'positif', 'sangat_positif']\n",
    "NUM_CLASSES = 5\n",
    "\n",
    "# Optimized config for Kaggle 2xT4 - 5-class needs more capacity\n",
    "CONFIG = {\n",
    "    # Model\n",
    "    'model_name': 'indobenchmark/indobert-base-p1',\n",
    "    'max_length': 128,\n",
    "    'num_classes': NUM_CLASSES,\n",
    "    \n",
    "    # Training - Slightly lower LR for 5-class stability\n",
    "    'batch_size': 32,\n",
    "    'epochs': 25,  # More epochs for 5-class\n",
    "    'learning_rate': 1.5e-5,  # Slightly lower for 5-class\n",
    "    \n",
    "    # Anti-Overfitting\n",
    "    'dropout_rate': 0.35,\n",
    "    'attention_dropout': 0.15,\n",
    "    'weight_decay': 0.01,\n",
    "    'label_smoothing': 0.1,\n",
    "    'warmup_ratio': 0.1,\n",
    "    'max_grad_norm': 1.0,\n",
    "    'early_stopping_patience': 6,  # More patience for 5-class\n",
    "    \n",
    "    # Layer Freezing - Fewer frozen for 5-class (needs more capacity)\n",
    "    'freeze_embeddings': True,\n",
    "    'freeze_layers': 6,  # Freeze 0-5, train 6-11 (more trainable)\n",
    "    \n",
    "    # R-Drop regularization\n",
    "    'use_rdrop': True,\n",
    "    'rdrop_alpha': 0.5,\n",
    "    \n",
    "    # Data augmentation\n",
    "    'augment_train': True,\n",
    "    'word_dropout_prob': 0.1,\n",
    "}\n",
    "\n",
    "print('='*60)\n",
    "print('‚öôÔ∏è  TRAINING CONFIGURATION (5-CLASS)')\n",
    "print('='*60)\n",
    "for k, v in CONFIG.items():\n",
    "    print(f'  {k}: {v}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acfc3ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# CELL 5: PREPARE DATA SPLITS\n",
    "# ============================================\n",
    "\n",
    "# Add label column\n",
    "df['label'] = df['sentiment'].map(LABEL_MAP)\n",
    "\n",
    "# Stratified split: 80% train, 10% val, 10% test\n",
    "train_df, temp_df = train_test_split(\n",
    "    df, test_size=0.2, random_state=42, stratify=df['label']\n",
    ")\n",
    "val_df, test_df = train_test_split(\n",
    "    temp_df, test_size=0.5, random_state=42, stratify=temp_df['label']\n",
    ")\n",
    "\n",
    "print('='*60)\n",
    "print('üìÇ DATA SPLITS (Stratified)')\n",
    "print('='*60)\n",
    "print(f'Train: {len(train_df):,} ({len(train_df)/len(df)*100:.0f}%)')\n",
    "print(f'Val:   {len(val_df):,} ({len(val_df)/len(df)*100:.0f}%)')\n",
    "print(f'Test:  {len(test_df):,} ({len(test_df)/len(df)*100:.0f}%)')\n",
    "\n",
    "print(f'\\nüìä Distribution per split:')\n",
    "for name, split_df in [('Train', train_df), ('Val', val_df), ('Test', test_df)]:\n",
    "    dist = split_df['sentiment'].value_counts()\n",
    "    print(f'  {name}: {dict(dist)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b5acbea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# CELL 6: DATASET CLASS\n",
    "# ============================================\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(CONFIG['model_name'])\n",
    "print(f'‚úÖ Tokenizer loaded: {CONFIG[\"model_name\"]}')\n",
    "\n",
    "class SentimentDataset(Dataset):\n",
    "    \"\"\"Dataset with optional text augmentation\"\"\"\n",
    "    \n",
    "    def __init__(self, texts, labels, tokenizer, max_length=128, \n",
    "                 augment=False, word_dropout_prob=0.1):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.augment = augment\n",
    "        self.word_dropout_prob = word_dropout_prob\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def _augment_text(self, text):\n",
    "        if not self.augment or random.random() > 0.5:\n",
    "            return text\n",
    "        \n",
    "        words = str(text).split()\n",
    "        if len(words) <= 3:\n",
    "            return text\n",
    "        \n",
    "        # Random word dropout\n",
    "        if random.random() < 0.5:\n",
    "            words = [w for w in words if random.random() > self.word_dropout_prob]\n",
    "        # Random word swap\n",
    "        elif len(words) > 2:\n",
    "            idx = random.randint(0, len(words) - 2)\n",
    "            words[idx], words[idx + 1] = words[idx + 1], words[idx]\n",
    "        \n",
    "        return ' '.join(words) if words else text\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = self._augment_text(self.texts[idx])\n",
    "        \n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            str(text),\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'label': torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = SentimentDataset(\n",
    "    train_df[text_col].values, train_df['label'].values, tokenizer,\n",
    "    max_length=CONFIG['max_length'], augment=CONFIG['augment_train'],\n",
    "    word_dropout_prob=CONFIG['word_dropout_prob']\n",
    ")\n",
    "val_dataset = SentimentDataset(\n",
    "    val_df[text_col].values, val_df['label'].values, tokenizer,\n",
    "    max_length=CONFIG['max_length'], augment=False\n",
    ")\n",
    "test_dataset = SentimentDataset(\n",
    "    test_df[text_col].values, test_df['label'].values, tokenizer,\n",
    "    max_length=CONFIG['max_length'], augment=False\n",
    ")\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=CONFIG['batch_size'], shuffle=True, num_workers=2, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=CONFIG['batch_size'], shuffle=False, num_workers=2, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=CONFIG['batch_size'], shuffle=False, num_workers=2, pin_memory=True)\n",
    "\n",
    "print(f'\\n‚úÖ DataLoaders created:')\n",
    "print(f'  Train: {len(train_dataset):,} samples, {len(train_loader)} batches')\n",
    "print(f'  Val:   {len(val_dataset):,} samples, {len(val_loader)} batches')\n",
    "print(f'  Test:  {len(test_dataset):,} samples, {len(test_loader)} batches')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f9e22c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# CELL 7: MODEL ARCHITECTURE\n",
    "# ============================================\n",
    "\n",
    "class IndoBERTClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    IndoBERT for 5-Class Sentiment Classification\n",
    "    - More trainable layers for 5-class complexity\n",
    "    - Dropout regularization\n",
    "    - Hidden layer for better representation\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model_name, num_classes, dropout_rate=0.35,\n",
    "                 freeze_embeddings=True, freeze_layers=6):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.bert = BertModel.from_pretrained(model_name)\n",
    "        self.hidden_size = self.bert.config.hidden_size\n",
    "        \n",
    "        # Freeze embeddings\n",
    "        if freeze_embeddings:\n",
    "            for param in self.bert.embeddings.parameters():\n",
    "                param.requires_grad = False\n",
    "        \n",
    "        # Freeze first N encoder layers\n",
    "        for i in range(freeze_layers):\n",
    "            for param in self.bert.encoder.layer[i].parameters():\n",
    "                param.requires_grad = False\n",
    "        \n",
    "        # Classifier head with hidden layer for 5-class\n",
    "        self.dropout1 = nn.Dropout(dropout_rate)\n",
    "        self.hidden = nn.Linear(self.hidden_size, 256)\n",
    "        self.dropout2 = nn.Dropout(dropout_rate * 0.5)\n",
    "        self.classifier = nn.Linear(256, num_classes)\n",
    "        \n",
    "        # Initialize\n",
    "        nn.init.xavier_uniform_(self.hidden.weight)\n",
    "        nn.init.zeros_(self.hidden.bias)\n",
    "        nn.init.xavier_uniform_(self.classifier.weight)\n",
    "        nn.init.zeros_(self.classifier.bias)\n",
    "        \n",
    "        print(f'‚úÖ Model initialized (5-Class)')\n",
    "        print(f'   Embeddings frozen: {freeze_embeddings}')\n",
    "        print(f'   Layers frozen: 0-{freeze_layers-1} (training {freeze_layers}-11)')\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled = outputs.pooler_output\n",
    "        \n",
    "        x = self.dropout1(pooled)\n",
    "        x = F.gelu(self.hidden(x))  # GELU activation\n",
    "        x = self.dropout2(x)\n",
    "        logits = self.classifier(x)\n",
    "        return logits\n",
    "\n",
    "# Initialize model\n",
    "model = IndoBERTClassifier(\n",
    "    model_name=CONFIG['model_name'],\n",
    "    num_classes=CONFIG['num_classes'],\n",
    "    dropout_rate=CONFIG['dropout_rate'],\n",
    "    freeze_embeddings=CONFIG['freeze_embeddings'],\n",
    "    freeze_layers=CONFIG['freeze_layers']\n",
    ").to(device)\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'\\nüìä Parameters:')\n",
    "print(f'   Total: {total_params:,}')\n",
    "print(f'   Trainable: {trainable_params:,} ({trainable_params/total_params*100:.1f}%)')\n",
    "print(f'   Frozen: {total_params - trainable_params:,}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a21620a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# CELL 8: OPTIMIZER & SCHEDULER\n",
    "# ============================================\n",
    "\n",
    "# Loss with label smoothing\n",
    "criterion = nn.CrossEntropyLoss(label_smoothing=CONFIG['label_smoothing'])\n",
    "\n",
    "# Optimizer - only trainable parameters\n",
    "no_decay = ['bias', 'LayerNorm.weight']\n",
    "optimizer_params = [\n",
    "    {\n",
    "        'params': [p for n, p in model.named_parameters() \n",
    "                   if p.requires_grad and not any(nd in n for nd in no_decay)],\n",
    "        'weight_decay': CONFIG['weight_decay']\n",
    "    },\n",
    "    {\n",
    "        'params': [p for n, p in model.named_parameters() \n",
    "                   if p.requires_grad and any(nd in n for nd in no_decay)],\n",
    "        'weight_decay': 0.0\n",
    "    }\n",
    "]\n",
    "\n",
    "optimizer = AdamW(optimizer_params, lr=CONFIG['learning_rate'])\n",
    "\n",
    "# Scheduler with warmup\n",
    "total_steps = len(train_loader) * CONFIG['epochs']\n",
    "warmup_steps = int(total_steps * CONFIG['warmup_ratio'])\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer, num_warmup_steps=warmup_steps, num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "print('‚úÖ Optimizer & Scheduler configured')\n",
    "print(f'   LR: {CONFIG[\"learning_rate\"]}')\n",
    "print(f'   Weight Decay: {CONFIG[\"weight_decay\"]}')\n",
    "print(f'   Warmup Steps: {warmup_steps}')\n",
    "print(f'   Total Steps: {total_steps}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b88225",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# CELL 9: TRAINING FUNCTIONS\n",
    "# ============================================\n",
    "\n",
    "def compute_kl_loss(p, q):\n",
    "    \"\"\"KL divergence for R-Drop\"\"\"\n",
    "    p_loss = F.kl_div(F.log_softmax(p, dim=-1), F.softmax(q, dim=-1), reduction='batchmean')\n",
    "    q_loss = F.kl_div(F.log_softmax(q, dim=-1), F.softmax(p, dim=-1), reduction='batchmean')\n",
    "    return (p_loss + q_loss) / 2\n",
    "\n",
    "def train_epoch(model, loader, criterion, optimizer, scheduler, device, \n",
    "                use_rdrop=True, rdrop_alpha=0.5, max_grad_norm=1.0):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    all_preds, all_labels = [], []\n",
    "    \n",
    "    pbar = tqdm(loader, desc='Training')\n",
    "    for batch in pbar:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        if use_rdrop:\n",
    "            # R-Drop: 2 forward passes\n",
    "            logits1 = model(input_ids, attention_mask)\n",
    "            logits2 = model(input_ids, attention_mask)\n",
    "            \n",
    "            ce_loss = (criterion(logits1, labels) + criterion(logits2, labels)) / 2\n",
    "            kl_loss = compute_kl_loss(logits1, logits2)\n",
    "            loss = ce_loss + rdrop_alpha * kl_loss\n",
    "            \n",
    "            logits = (logits1 + logits2) / 2\n",
    "        else:\n",
    "            logits = model(input_ids, attention_mask)\n",
    "            loss = criterion(logits, labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        preds = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "        all_preds.extend(preds)\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "    \n",
    "    avg_loss = total_loss / len(loader)\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "    \n",
    "    return avg_loss, accuracy, f1\n",
    "\n",
    "def evaluate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_preds, all_labels, all_probs = [], [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "            \n",
    "            logits = model(input_ids, attention_mask)\n",
    "            loss = criterion(logits, labels)\n",
    "            \n",
    "            probs = F.softmax(logits, dim=1)\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_probs.extend(probs.cpu().numpy())\n",
    "    \n",
    "    avg_loss = total_loss / len(loader)\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "    \n",
    "    return avg_loss, accuracy, f1, all_preds, all_labels, all_probs\n",
    "\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=6, min_delta=0.001):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.best_model = None\n",
    "        self.early_stop = False\n",
    "    \n",
    "    def __call__(self, score, model):\n",
    "        if self.best_score is None or score > self.best_score + self.min_delta:\n",
    "            self.best_score = score\n",
    "            self.best_model = copy.deepcopy(model.state_dict())\n",
    "            self.counter = 0\n",
    "            return True\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "            return False\n",
    "\n",
    "print('‚úÖ Training functions defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e83865bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# CELL 10: TRAINING LOOP\n",
    "# ============================================\n",
    "\n",
    "history = {\n",
    "    'train_loss': [], 'train_acc': [], 'train_f1': [],\n",
    "    'val_loss': [], 'val_acc': [], 'val_f1': []\n",
    "}\n",
    "\n",
    "early_stopping = EarlyStopping(patience=CONFIG['early_stopping_patience'])\n",
    "\n",
    "print('='*60)\n",
    "print('üöÄ TRAINING STARTED (5-CLASS)')\n",
    "print('='*60)\n",
    "print(f'Epochs: {CONFIG[\"epochs\"]} | Batch: {CONFIG[\"batch_size\"]} | LR: {CONFIG[\"learning_rate\"]}')\n",
    "print(f'R-Drop: {CONFIG[\"use_rdrop\"]} | Early Stop Patience: {CONFIG[\"early_stopping_patience\"]}')\n",
    "print('-'*60)\n",
    "\n",
    "start_time = time.time()\n",
    "best_val_f1 = 0\n",
    "best_epoch = 0\n",
    "\n",
    "for epoch in range(CONFIG['epochs']):\n",
    "    print(f'\\nüìç Epoch {epoch + 1}/{CONFIG[\"epochs\"]}')\n",
    "    \n",
    "    # Train\n",
    "    train_loss, train_acc, train_f1 = train_epoch(\n",
    "        model, train_loader, criterion, optimizer, scheduler, device,\n",
    "        use_rdrop=CONFIG['use_rdrop'], rdrop_alpha=CONFIG['rdrop_alpha'],\n",
    "        max_grad_norm=CONFIG['max_grad_norm']\n",
    "    )\n",
    "    \n",
    "    # Validate\n",
    "    val_loss, val_acc, val_f1, _, _, _ = evaluate(model, val_loader, criterion, device)\n",
    "    \n",
    "    # Save history\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['train_acc'].append(train_acc)\n",
    "    history['train_f1'].append(train_f1)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['val_acc'].append(val_acc)\n",
    "    history['val_f1'].append(val_f1)\n",
    "    \n",
    "    # Calculate gap\n",
    "    gap = train_acc - val_acc\n",
    "    \n",
    "    # Print metrics\n",
    "    print(f'  Train | Loss: {train_loss:.4f} | Acc: {train_acc:.4f} | F1: {train_f1:.4f}')\n",
    "    print(f'  Val   | Loss: {val_loss:.4f} | Acc: {val_acc:.4f} | F1: {val_f1:.4f}')\n",
    "    print(f'  Gap   | {gap*100:.2f}%', end='')\n",
    "    \n",
    "    if gap > 0.10:\n",
    "        print(' ‚ö†Ô∏è Overfitting!')\n",
    "    elif gap > 0.05:\n",
    "        print(' ‚ö° Watch gap')\n",
    "    else:\n",
    "        print(' ‚úÖ Good')\n",
    "    \n",
    "    # Early stopping check\n",
    "    improved = early_stopping(val_f1, model)\n",
    "    if improved:\n",
    "        best_val_f1 = val_f1\n",
    "        best_epoch = epoch + 1\n",
    "        print(f'  ‚≠ê New best model! F1: {val_f1:.4f}')\n",
    "    \n",
    "    if early_stopping.early_stop:\n",
    "        print(f'\\nüõë Early stopping at epoch {epoch + 1}')\n",
    "        break\n",
    "\n",
    "# Load best model\n",
    "model.load_state_dict(early_stopping.best_model)\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "print(f'\\n‚úÖ Training completed in {total_time/60:.1f} minutes')\n",
    "print(f'   Best epoch: {best_epoch}')\n",
    "print(f'   Best val F1: {best_val_f1:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e5504d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# CELL 11: TRAINING VISUALIZATION\n",
    "# ============================================\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "epochs_range = range(1, len(history['train_loss']) + 1)\n",
    "\n",
    "# Loss\n",
    "axes[0].plot(epochs_range, history['train_loss'], 'b-o', label='Train', markersize=4)\n",
    "axes[0].plot(epochs_range, history['val_loss'], 'r-s', label='Val', markersize=4)\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('Loss (5-Class)', fontweight='bold')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Accuracy\n",
    "axes[1].plot(epochs_range, history['train_acc'], 'b-o', label='Train', markersize=4)\n",
    "axes[1].plot(epochs_range, history['val_acc'], 'r-s', label='Val', markersize=4)\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Accuracy')\n",
    "axes[1].set_title('Accuracy (5-Class)', fontweight='bold')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# F1 Score\n",
    "axes[2].plot(epochs_range, history['train_f1'], 'b-o', label='Train', markersize=4)\n",
    "axes[2].plot(epochs_range, history['val_f1'], 'r-s', label='Val', markersize=4)\n",
    "axes[2].axhline(y=best_val_f1, color='g', linestyle='--', label=f'Best: {best_val_f1:.4f}')\n",
    "axes[2].set_xlabel('Epoch')\n",
    "axes[2].set_ylabel('F1 Score')\n",
    "axes[2].set_title('F1 Score (5-Class)', fontweight='bold')\n",
    "axes[2].legend()\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('training_history_5class.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Gap analysis\n",
    "plt.figure(figsize=(8, 4))\n",
    "gaps = [t - v for t, v in zip(history['train_acc'], history['val_acc'])]\n",
    "colors = ['red' if g > 0.10 else 'orange' if g > 0.05 else 'green' for g in gaps]\n",
    "plt.bar(epochs_range, [g*100 for g in gaps], color=colors)\n",
    "plt.axhline(y=10, color='red', linestyle='--', label='Overfitting threshold (10%)')\n",
    "plt.axhline(y=5, color='orange', linestyle='--', label='Warning threshold (5%)')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Train-Val Gap (%)')\n",
    "plt.title('Overfitting Analysis (5-Class)', fontweight='bold')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig('overfitting_analysis_5class.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "491aec3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# CELL 12: TEST EVALUATION\n",
    "# ============================================\n",
    "\n",
    "print('='*60)\n",
    "print('üß™ FINAL TEST EVALUATION (5-CLASS)')\n",
    "print('='*60)\n",
    "\n",
    "test_loss, test_acc, test_f1, test_preds, test_labels, test_probs = evaluate(\n",
    "    model, test_loader, criterion, device\n",
    ")\n",
    "\n",
    "print(f'\\nüìä Test Results:')\n",
    "print(f'   Loss: {test_loss:.4f}')\n",
    "print(f'   Accuracy: {test_acc:.4f} ({test_acc*100:.2f}%)')\n",
    "print(f'   F1 Score: {test_f1:.4f}')\n",
    "\n",
    "# Classification report\n",
    "print('\\nüìã Classification Report:')\n",
    "print(classification_report(test_labels, test_preds, target_names=LABEL_NAMES, digits=4))\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(test_labels, test_preds)\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=LABEL_NAMES, yticklabels=LABEL_NAMES)\n",
    "plt.title('Confusion Matrix - Test Set (5-Class)', fontweight='bold')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.xticks(rotation=20)\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.savefig('confusion_matrix_5class.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Per-class accuracy\n",
    "print('\\nüìä Per-Class Accuracy:')\n",
    "for i, name in enumerate(LABEL_NAMES):\n",
    "    class_mask = np.array(test_labels) == i\n",
    "    class_acc = np.mean(np.array(test_preds)[class_mask] == i)\n",
    "    print(f'   {name}: {class_acc:.4f} ({class_acc*100:.2f}%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc653a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# CELL 13: SAVE MODEL\n",
    "# ============================================\n",
    "\n",
    "# Save model\n",
    "save_path = '/kaggle/working/indobert_sentiment_5class.pt'\n",
    "\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'config': CONFIG,\n",
    "    'label_map': LABEL_MAP,\n",
    "    'label_names': LABEL_NAMES,\n",
    "    'metrics': {\n",
    "        'test_accuracy': test_acc,\n",
    "        'test_f1': test_f1,\n",
    "        'best_val_f1': best_val_f1,\n",
    "        'best_epoch': best_epoch\n",
    "    },\n",
    "    'history': history\n",
    "}, save_path)\n",
    "\n",
    "print(f'‚úÖ Model saved to: {save_path}')\n",
    "print(f'   File size: {os.path.getsize(save_path) / 1024 / 1024:.1f} MB')\n",
    "\n",
    "# Save training history\n",
    "history_path = '/kaggle/working/training_history_5class.json'\n",
    "with open(history_path, 'w') as f:\n",
    "    json.dump(history, f, indent=2)\n",
    "print(f'‚úÖ History saved to: {history_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "747aead1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# CELL 14: INFERENCE FUNCTION\n",
    "# ============================================\n",
    "\n",
    "def predict_sentiment(text, model, tokenizer, device, label_names):\n",
    "    \"\"\"Predict sentiment for a single text\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    encoding = tokenizer.encode_plus(\n",
    "        str(text),\n",
    "        add_special_tokens=True,\n",
    "        max_length=128,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_attention_mask=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    \n",
    "    input_ids = encoding['input_ids'].to(device)\n",
    "    attention_mask = encoding['attention_mask'].to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        logits = model(input_ids, attention_mask)\n",
    "        probs = F.softmax(logits, dim=1)\n",
    "        pred = torch.argmax(logits, dim=1).item()\n",
    "    \n",
    "    return {\n",
    "        'sentiment': label_names[pred],\n",
    "        'confidence': probs[0][pred].item(),\n",
    "        'probabilities': {name: probs[0][i].item() for i, name in enumerate(label_names)}\n",
    "    }\n",
    "\n",
    "# Test predictions\n",
    "print('='*60)\n",
    "print('üîÆ SAMPLE PREDICTIONS (5-CLASS)')\n",
    "print('='*60)\n",
    "\n",
    "test_texts = [\n",
    "    \"Aplikasi sangat membantu, driver ramah dan cepat sampai. Sangat puas!\",\n",
    "    \"Lumayan bagus, tapi masih ada yang perlu diperbaiki\",\n",
    "    \"Biasa saja, tidak ada yang istimewa\",\n",
    "    \"Agak kecewa dengan pelayanan driver kali ini\",\n",
    "    \"Aplikasi error terus, driver tidak profesional, sangat mengecewakan. Parah!\"\n",
    "]\n",
    "\n",
    "for text in test_texts:\n",
    "    result = predict_sentiment(text, model, tokenizer, device, LABEL_NAMES)\n",
    "    print(f'\\nüìù \"{text[:60]}...\"' if len(text) > 60 else f'\\nüìù \"{text}\"')\n",
    "    print(f'   Sentiment: {result[\"sentiment\"].upper()}')\n",
    "    print(f'   Confidence: {result[\"confidence\"]*100:.1f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deecfba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# CELL 15: FINAL SUMMARY\n",
    "# ============================================\n",
    "\n",
    "print('='*60)\n",
    "print('üìä TRAINING SUMMARY (5-CLASS)')\n",
    "print('='*60)\n",
    "\n",
    "summary = f\"\"\"\n",
    "üéØ MODEL: IndoBERT Sentiment 5-Class\n",
    "\n",
    "üìà METRICS:\n",
    "   ‚Ä¢ Test Accuracy: {test_acc*100:.2f}%\n",
    "   ‚Ä¢ Test F1 Score: {test_f1:.4f}\n",
    "   ‚Ä¢ Best Val F1:   {best_val_f1:.4f} (epoch {best_epoch})\n",
    "\n",
    "‚öôÔ∏è CONFIGURATION:\n",
    "   ‚Ä¢ Model: {CONFIG['model_name']}\n",
    "   ‚Ä¢ Epochs trained: {len(history['train_loss'])}\n",
    "   ‚Ä¢ Batch size: {CONFIG['batch_size']}\n",
    "   ‚Ä¢ Learning rate: {CONFIG['learning_rate']}\n",
    "   ‚Ä¢ Frozen layers: {CONFIG['freeze_layers']}/12\n",
    "   ‚Ä¢ Dropout: {CONFIG['dropout_rate']}\n",
    "   ‚Ä¢ R-Drop alpha: {CONFIG['rdrop_alpha']}\n",
    "\n",
    "üìÇ DATA:\n",
    "   ‚Ä¢ Train: {len(train_df):,}\n",
    "   ‚Ä¢ Val: {len(val_df):,}\n",
    "   ‚Ä¢ Test: {len(test_df):,}\n",
    "\n",
    "üíæ SAVED FILES:\n",
    "   ‚Ä¢ Model: indobert_sentiment_5class.pt\n",
    "   ‚Ä¢ History: training_history_5class.json\n",
    "   ‚Ä¢ Plots: training_history_5class.png, confusion_matrix_5class.png\n",
    "\"\"\"\n",
    "\n",
    "print(summary)\n",
    "\n",
    "# Check overfitting\n",
    "final_gap = history['train_acc'][-1] - history['val_acc'][-1]\n",
    "print(f'\\nüîç OVERFITTING CHECK:')\n",
    "print(f'   Final train-val gap: {final_gap*100:.2f}%')\n",
    "if final_gap < 0.05:\n",
    "    print('   ‚úÖ Excellent generalization!')\n",
    "elif final_gap < 0.10:\n",
    "    print('   ‚ö° Good generalization with minor gap')\n",
    "else:\n",
    "    print('   ‚ö†Ô∏è Some overfitting detected')\n",
    "\n",
    "print('\\n' + '='*60)\n",
    "print('‚úÖ Training complete! Download model from /kaggle/working/')\n",
    "print('='*60)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
