{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d424b084",
   "metadata": {},
   "source": [
    "# üöÄ IndoBERT Sentiment Analysis 3 Kelas - Google Colab Version\n",
    "\n",
    "**Dataset**: gojek_reviews_3class_clean.csv  \n",
    "**Model**: IndoBERT (indobenchmark/indobert-base-p1)  \n",
    "**Target**: Akurasi tinggi dengan generalisasi yang baik (tidak overfitting)\n",
    "\n",
    "## üìã Persiapan Sebelum Running:\n",
    "\n",
    "1. **Upload file data ke folder `skripsi` di Google Drive:**\n",
    "```\n",
    "MyDrive/\n",
    "‚îî‚îÄ‚îÄ skripsi/\n",
    "    ‚îú‚îÄ‚îÄ gojek_reviews_3class_clean.csv   ‚Üê Upload file ini\n",
    "    ‚îú‚îÄ‚îÄ models/                           ‚Üê Akan dibuat otomatis\n",
    "    ‚îî‚îÄ‚îÄ (notebook ini jika mau)\n",
    "```\n",
    "\n",
    "2. **Pastikan Runtime GPU aktif**: Runtime ‚Üí Change runtime type ‚Üí GPU\n",
    "\n",
    "---\n",
    "\n",
    "### Teknik Anti-Overfitting yang Digunakan:\n",
    "1. **Data Balancing** - Undersampling ke kelas minoritas\n",
    "2. **Dropout** - 0.3 untuk regularisasi\n",
    "3. **Label Smoothing** - 0.1 untuk soft labels\n",
    "4. **Early Stopping** - Stop jika val_loss tidak membaik\n",
    "5. **Weight Decay** - L2 regularization (0.01)\n",
    "6. **Learning Rate Warmup** - Gradual increase\n",
    "7. **Gradient Clipping** - Mencegah exploding gradients\n",
    "8. **Data Augmentation** - Random word dropout\n",
    "\n",
    "### Kelas Sentiment:\n",
    "- **0 = Negative** (Score 1-2)\n",
    "- **1 = Neutral** (Score 3)\n",
    "- **2 = Positive** (Score 4-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3758e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# SETUP GOOGLE COLAB\n",
    "# ============================================\n",
    "\n",
    "# Install dependencies\n",
    "!pip install transformers torch pandas numpy scikit-learn matplotlib seaborn tqdm -q\n",
    "\n",
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Path ke folder skripsi di Google Drive\n",
    "DRIVE_PATH = '/content/drive/MyDrive/skripsi'\n",
    "\n",
    "import os\n",
    "\n",
    "# Check apakah folder exists\n",
    "if os.path.exists(DRIVE_PATH):\n",
    "    os.chdir(DRIVE_PATH)\n",
    "    print(f'‚úì Working directory: {os.getcwd()}')\n",
    "    print(f'‚úì Files in folder skripsi:')\n",
    "    for f in os.listdir('.'):\n",
    "        print(f'   - {f}')\n",
    "else:\n",
    "    print(f'‚ùå Folder tidak ditemukan: {DRIVE_PATH}')\n",
    "    print('Pastikan folder \"skripsi\" ada di Google Drive kamu')\n",
    "\n",
    "# Check GPU\n",
    "import torch\n",
    "if torch.cuda.is_available():\n",
    "    print(f'\\n‚úì GPU Available: {torch.cuda.get_device_name(0)}')\n",
    "    print(f'‚úì GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB')\n",
    "else:\n",
    "    print('\\n‚ö†Ô∏è GPU not available, using CPU (akan lebih lambat)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b23c5d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "from transformers import BertTokenizer, BertModel, get_linear_schedule_with_warmup\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_recall_fscore_support, \n",
    "    classification_report, confusion_matrix, f1_score\n",
    ")\n",
    "from sklearn.utils import resample\n",
    "import random\n",
    "import os\n",
    "import copy\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Reproducibility\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "# Device setup (sudah di-check di cell sebelumnya)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'üñ•Ô∏è  Device: {device}')\n",
    "if torch.cuda.is_available():\n",
    "    print(f'üéÆ GPU: {torch.cuda.get_device_name(0)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7dbb95b",
   "metadata": {},
   "source": [
    "## üìä 1. Load & Explore Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dec10da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load cleaned data dari Google Drive folder skripsi\n",
    "# Prioritaskan file augmented (15,000 samples) jika ada\n",
    "DATA_FILES = [\n",
    "    'gojek_reviews_final_augmented.csv',  # 15,000 samples - RECOMMENDED\n",
    "    'gojek_reviews_3class_balanced.csv',\n",
    "    'gojek_reviews_3class_clean.csv',\n",
    "]\n",
    "\n",
    "DATA_PATH = None\n",
    "for f in DATA_FILES:\n",
    "    if os.path.exists(f):\n",
    "        DATA_PATH = f\n",
    "        break\n",
    "    # Check in data folder too\n",
    "    if os.path.exists(f'data/{f}'):\n",
    "        DATA_PATH = f'data/{f}'\n",
    "        break\n",
    "\n",
    "if DATA_PATH is None:\n",
    "    print('‚ùå Data file tidak ditemukan!')\n",
    "    print(f'\\nüìÅ Files yang ada di folder skripsi:')\n",
    "    for f in os.listdir('.'):\n",
    "        print(f'   - {f}')\n",
    "    if os.path.exists('data'):\n",
    "        print(f'\\nüìÅ Files di folder data:')\n",
    "        for f in os.listdir('data'):\n",
    "            print(f'   - data/{f}')\n",
    "    print(f'\\nüí° Upload file \"gojek_reviews_final_augmented.csv\" ke folder skripsi')\n",
    "else:\n",
    "    print(f'‚úì Using data file: {DATA_PATH}')\n",
    "    df = pd.read_csv(DATA_PATH)\n",
    "    \n",
    "    print('=' * 60)\n",
    "    print('üìä DATA OVERVIEW')\n",
    "    print('=' * 60)\n",
    "    print(f'Total samples: {len(df):,}')\n",
    "    print(f'\\nColumns: {df.columns.tolist()}')\n",
    "    print(f'\\nüìà Sentiment Distribution:')\n",
    "    print(df['sentiment'].value_counts())\n",
    "    \n",
    "    # Visualize\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "    \n",
    "    # Bar plot\n",
    "    colors = {'negative': '#e74c3c', 'neutral': '#95a5a6', 'positive': '#2ecc71'}\n",
    "    sentiment_counts = df['sentiment'].value_counts()\n",
    "    axes[0].bar(sentiment_counts.index, sentiment_counts.values, \n",
    "                color=[colors[s] for s in sentiment_counts.index])\n",
    "    axes[0].set_title('Sentiment Distribution')\n",
    "    axes[0].set_ylabel('Count')\n",
    "    \n",
    "    # Pie chart\n",
    "    axes[1].pie(sentiment_counts.values, labels=sentiment_counts.index, \n",
    "                autopct='%1.1f%%', colors=[colors[s] for s in sentiment_counts.index])\n",
    "    axes[1].set_title('Sentiment Percentage')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8c35280",
   "metadata": {},
   "source": [
    "## ‚öñÔ∏è 2. Balance Data (Undersampling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b96c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if data is already balanced\n",
    "counts = df['sentiment'].value_counts()\n",
    "min_count = counts.min()\n",
    "max_count = counts.max()\n",
    "\n",
    "# If already balanced (difference < 10%), skip undersampling\n",
    "if (max_count - min_count) / max_count < 0.1:\n",
    "    print('‚úì Data sudah balanced! Skip undersampling.')\n",
    "    df_balanced = df.copy()\n",
    "else:\n",
    "    # Balance data menggunakan undersampling\n",
    "    print(f'‚ö†Ô∏è Data tidak balanced. Melakukan undersampling...')\n",
    "    print(f'Kelas minoritas: {min_count} samples')\n",
    "    \n",
    "    df_balanced = pd.DataFrame()\n",
    "    for sentiment in ['negative', 'neutral', 'positive']:\n",
    "        df_class = df[df['sentiment'] == sentiment]\n",
    "        df_sampled = resample(df_class, replace=False, n_samples=min_count, random_state=42)\n",
    "        df_balanced = pd.concat([df_balanced, df_sampled])\n",
    "    \n",
    "    # Shuffle\n",
    "    df_balanced = df_balanced.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "print('\\n' + '=' * 60)\n",
    "print('‚öñÔ∏è  DATA UNTUK TRAINING')\n",
    "print('=' * 60)\n",
    "print(f'Total: {len(df_balanced):,}')\n",
    "print(df_balanced['sentiment'].value_counts())\n",
    "\n",
    "# Visualize balanced\n",
    "plt.figure(figsize=(8, 4))\n",
    "balanced_counts = df_balanced['sentiment'].value_counts()\n",
    "colors = {'negative': '#e74c3c', 'neutral': '#95a5a6', 'positive': '#2ecc71'}\n",
    "plt.bar(balanced_counts.index, balanced_counts.values, \n",
    "        color=[colors[s] for s in balanced_counts.index])\n",
    "plt.title('Sentiment Distribution for Training')\n",
    "plt.ylabel('Count')\n",
    "for i, (label, count) in enumerate(balanced_counts.items()):\n",
    "    plt.text(i, count + 50, str(count), ha='center', fontweight='bold')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd5a9692",
   "metadata": {},
   "source": [
    "## üè∑Ô∏è 3. Prepare Labels & Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf731b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label mapping\n",
    "LABEL_MAP = {'negative': 0, 'neutral': 1, 'positive': 2}\n",
    "LABEL_NAMES = ['negative', 'neutral', 'positive']\n",
    "NUM_CLASSES = 3\n",
    "\n",
    "df_balanced['label'] = df_balanced['sentiment'].map(LABEL_MAP)\n",
    "\n",
    "# Split: 70% train, 15% validation, 15% test (stratified)\n",
    "# Stratified split memastikan distribusi kelas sama di setiap split\n",
    "train_df, temp_df = train_test_split(\n",
    "    df_balanced, test_size=0.3, random_state=42, \n",
    "    stratify=df_balanced['label']\n",
    ")\n",
    "val_df, test_df = train_test_split(\n",
    "    temp_df, test_size=0.5, random_state=42, \n",
    "    stratify=temp_df['label']\n",
    ")\n",
    "\n",
    "print('=' * 60)\n",
    "print('üìÇ DATA SPLITS')\n",
    "print('=' * 60)\n",
    "print(f'Train: {len(train_df):,} samples ({len(train_df)/len(df_balanced)*100:.1f}%)')\n",
    "print(f'Val:   {len(val_df):,} samples ({len(val_df)/len(df_balanced)*100:.1f}%)')\n",
    "print(f'Test:  {len(test_df):,} samples ({len(test_df)/len(df_balanced)*100:.1f}%)')\n",
    "\n",
    "print(f'\\nüìä Train label distribution:')\n",
    "print(train_df['sentiment'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b7adbaf",
   "metadata": {},
   "source": [
    "## üîß 4. Hyperparameters & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d7aaccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === HYPERPARAMETERS ===\n",
    "# OPTIMIZED untuk menghindari overfitting dan meningkatkan akurasi\n",
    "\n",
    "CONFIG = {\n",
    "    # Model\n",
    "    'model_name': 'indobenchmark/indobert-base-p1',\n",
    "    'max_length': 128,\n",
    "    'num_classes': NUM_CLASSES,\n",
    "    \n",
    "    # Training - ADJUSTED\n",
    "    'batch_size': 32,  # Larger batch untuk stabilitas\n",
    "    'epochs': 20,  # Lebih banyak epoch, early stopping akan handle\n",
    "    'learning_rate': 1e-5,  # Lebih kecil untuk fine-tuning BERT\n",
    "    \n",
    "    # Anti-Overfitting - MORE AGGRESSIVE\n",
    "    'dropout_rate': 0.5,  # Tingkatkan dropout\n",
    "    'weight_decay': 0.02,  # Tingkatkan L2 regularization\n",
    "    'label_smoothing': 0.15,  # Tingkatkan label smoothing\n",
    "    'warmup_ratio': 0.1,\n",
    "    'max_grad_norm': 1.0,\n",
    "    'early_stopping_patience': 5,  # Lebih sabar menunggu improvement\n",
    "    \n",
    "    # Data Augmentation\n",
    "    'word_dropout_prob': 0.15,  # Tingkatkan word dropout\n",
    "    \n",
    "    # Layer Freezing - BERT layers to freeze (0-11)\n",
    "    'freeze_layers': 6,  # Freeze 6 layer pertama dari 12 layer BERT\n",
    "}\n",
    "\n",
    "print('=' * 60)\n",
    "print('‚öôÔ∏è  OPTIMIZED CONFIGURATION')\n",
    "print('=' * 60)\n",
    "for key, value in CONFIG.items():\n",
    "    print(f'{key}: {value}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e2d2e8",
   "metadata": {},
   "source": [
    "## üì¶ 5. Dataset Class with Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f78d58e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(CONFIG['model_name'])\n",
    "print(f'‚úì Tokenizer loaded: {CONFIG[\"model_name\"]}')\n",
    "\n",
    "class SentimentDataset(Dataset):\n",
    "    \"\"\"Dataset dengan ENHANCED augmentation\"\"\"\n",
    "    \n",
    "    def __init__(self, texts, labels, tokenizer, max_length=128, \n",
    "                 augment=False, word_dropout_prob=0.15):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.augment = augment\n",
    "        self.word_dropout_prob = word_dropout_prob\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def _augment_text(self, text):\n",
    "        \"\"\"Enhanced augmentation dengan multiple techniques\"\"\"\n",
    "        if not self.augment:\n",
    "            return text\n",
    "            \n",
    "        text = str(text)\n",
    "        words = text.split()\n",
    "        \n",
    "        if len(words) <= 3:\n",
    "            return text\n",
    "        \n",
    "        # Technique 1: Word dropout\n",
    "        if random.random() < 0.5:\n",
    "            words = [w for w in words if random.random() > self.word_dropout_prob]\n",
    "        \n",
    "        # Technique 2: Word swap (swap adjacent words)\n",
    "        if random.random() < 0.3 and len(words) > 2:\n",
    "            idx = random.randint(0, len(words) - 2)\n",
    "            words[idx], words[idx + 1] = words[idx + 1], words[idx]\n",
    "        \n",
    "        # Technique 3: Random word duplication\n",
    "        if random.random() < 0.2 and len(words) > 1:\n",
    "            idx = random.randint(0, len(words) - 1)\n",
    "            words.insert(idx, words[idx])\n",
    "        \n",
    "        return ' '.join(words) if words else text\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = self._augment_text(self.texts[idx])\n",
    "        \n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            str(text),\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'label': torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = SentimentDataset(\n",
    "    train_df['content_clean'].values,\n",
    "    train_df['label'].values,\n",
    "    tokenizer,\n",
    "    max_length=CONFIG['max_length'],\n",
    "    augment=True,  # Enable augmentation for training\n",
    "    word_dropout_prob=CONFIG['word_dropout_prob']\n",
    ")\n",
    "\n",
    "val_dataset = SentimentDataset(\n",
    "    val_df['content_clean'].values,\n",
    "    val_df['label'].values,\n",
    "    tokenizer,\n",
    "    max_length=CONFIG['max_length'],\n",
    "    augment=False\n",
    ")\n",
    "\n",
    "test_dataset = SentimentDataset(\n",
    "    test_df['content_clean'].values,\n",
    "    test_df['label'].values,\n",
    "    tokenizer,\n",
    "    max_length=CONFIG['max_length'],\n",
    "    augment=False\n",
    ")\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=CONFIG['batch_size'], shuffle=True, drop_last=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=CONFIG['batch_size'], shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=CONFIG['batch_size'], shuffle=False)\n",
    "\n",
    "print(f'\\n‚úì Datasets created:')\n",
    "print(f'  Train: {len(train_dataset)} samples, {len(train_loader)} batches')\n",
    "print(f'  Val:   {len(val_dataset)} samples, {len(val_loader)} batches')\n",
    "print(f'  Test:  {len(test_dataset)} samples, {len(test_loader)} batches')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "097b6f0c",
   "metadata": {},
   "source": [
    "## üß† 6. Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "630ce4ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IndoBERTSentimentClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    IndoBERT dengan regularisasi AGRESIF untuk mencegah overfitting:\n",
    "    - Freeze beberapa layer BERT awal\n",
    "    - Dropout tinggi (0.5)\n",
    "    - Multi-layer dropout\n",
    "    - Layer normalization\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model_name, num_classes, dropout_rate=0.5, freeze_layers=6):\n",
    "        super(IndoBERTSentimentClassifier, self).__init__()\n",
    "        \n",
    "        # Load pretrained BERT\n",
    "        self.bert = BertModel.from_pretrained(model_name)\n",
    "        self.hidden_size = self.bert.config.hidden_size\n",
    "        \n",
    "        # === FREEZE BERT LAYERS ===\n",
    "        # Freeze embeddings\n",
    "        for param in self.bert.embeddings.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        # Freeze first N encoder layers\n",
    "        for i in range(freeze_layers):\n",
    "            for param in self.bert.encoder.layer[i].parameters():\n",
    "                param.requires_grad = False\n",
    "        \n",
    "        print(f'‚úì Froze embeddings and first {freeze_layers} encoder layers')\n",
    "        \n",
    "        # Regularization layers - MORE AGGRESSIVE\n",
    "        self.dropout1 = nn.Dropout(dropout_rate)\n",
    "        self.dropout2 = nn.Dropout(dropout_rate * 0.6)  # Second dropout\n",
    "        self.layer_norm = nn.LayerNorm(self.hidden_size)\n",
    "        \n",
    "        # Classifier dengan hidden layer untuk lebih banyak kapasitas\n",
    "        self.fc1 = nn.Linear(self.hidden_size, self.hidden_size // 2)\n",
    "        self.fc2 = nn.Linear(self.hidden_size // 2, num_classes)\n",
    "        \n",
    "        # Activation\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        # Initialize weights\n",
    "        nn.init.xavier_uniform_(self.fc1.weight)\n",
    "        nn.init.zeros_(self.fc1.bias)\n",
    "        nn.init.xavier_uniform_(self.fc2.weight)\n",
    "        nn.init.zeros_(self.fc2.bias)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        # Get BERT output\n",
    "        outputs = self.bert(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask\n",
    "        )\n",
    "        \n",
    "        # Use [CLS] token representation\n",
    "        pooled_output = outputs.pooler_output\n",
    "        \n",
    "        # Apply regularization pipeline\n",
    "        x = self.layer_norm(pooled_output)\n",
    "        x = self.dropout1(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        logits = self.fc2(x)\n",
    "        \n",
    "        return logits\n",
    "\n",
    "# Initialize model\n",
    "model = IndoBERTSentimentClassifier(\n",
    "    model_name=CONFIG['model_name'],\n",
    "    num_classes=CONFIG['num_classes'],\n",
    "    dropout_rate=CONFIG['dropout_rate'],\n",
    "    freeze_layers=CONFIG['freeze_layers']\n",
    ").to(device)\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "frozen_params = total_params - trainable_params\n",
    "\n",
    "print(f'\\n‚úì Model initialized')\n",
    "print(f'  Total parameters: {total_params:,}')\n",
    "print(f'  Trainable parameters: {trainable_params:,} ({trainable_params/total_params*100:.1f}%)')\n",
    "print(f'  Frozen parameters: {frozen_params:,} ({frozen_params/total_params*100:.1f}%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbd46a93",
   "metadata": {},
   "source": [
    "## üìâ 7. Loss Function, Optimizer & Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e71b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function dengan label smoothing\n",
    "criterion = nn.CrossEntropyLoss(label_smoothing=CONFIG['label_smoothing'])\n",
    "\n",
    "# Optimizer - ONLY for trainable parameters\n",
    "# Pisahkan parameter yang perlu weight decay dan yang tidak\n",
    "no_decay = ['bias', 'LayerNorm.weight', 'layer_norm.weight']\n",
    "\n",
    "# Filter hanya parameter yang requires_grad=True\n",
    "trainable_params_list = [(n, p) for n, p in model.named_parameters() if p.requires_grad]\n",
    "\n",
    "optimizer_grouped_parameters = [\n",
    "    {\n",
    "        'params': [p for n, p in trainable_params_list if not any(nd in n for nd in no_decay)],\n",
    "        'weight_decay': CONFIG['weight_decay']\n",
    "    },\n",
    "    {\n",
    "        'params': [p for n, p in trainable_params_list if any(nd in n for nd in no_decay)],\n",
    "        'weight_decay': 0.0\n",
    "    }\n",
    "]\n",
    "\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=CONFIG['learning_rate'])\n",
    "\n",
    "# Learning rate scheduler dengan warmup\n",
    "total_steps = len(train_loader) * CONFIG['epochs']\n",
    "warmup_steps = int(total_steps * CONFIG['warmup_ratio'])\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=warmup_steps,\n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "print(f'‚úì Optimizer: AdamW (lr={CONFIG[\"learning_rate\"]}, weight_decay={CONFIG[\"weight_decay\"]})')\n",
    "print(f'‚úì Scheduler: Linear warmup ({warmup_steps} warmup steps, {total_steps} total steps)')\n",
    "print(f'‚úì Loss: CrossEntropy with label_smoothing={CONFIG[\"label_smoothing\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3339bb5",
   "metadata": {},
   "source": [
    "## üèãÔ∏è 8. Training Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5077bb3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, dataloader, criterion, optimizer, scheduler, device, max_grad_norm):\n",
    "    \"\"\"Train untuk satu epoch\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    progress_bar = tqdm(dataloader, desc='Training', leave=False)\n",
    "    \n",
    "    for batch in progress_bar:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        logits = model(input_ids, attention_mask)\n",
    "        loss = criterion(logits, labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "        \n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        preds = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "        all_preds.extend(preds)\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        progress_bar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "    \n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "    \n",
    "    return avg_loss, accuracy, f1\n",
    "\n",
    "\n",
    "def evaluate(model, dataloader, criterion, device):\n",
    "    \"\"\"Evaluasi model\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "            \n",
    "            logits = model(input_ids, attention_mask)\n",
    "            loss = criterion(logits, labels)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            preds = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "            all_preds.extend(preds)\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "    \n",
    "    return avg_loss, accuracy, f1, all_preds, all_labels\n",
    "\n",
    "\n",
    "class EarlyStopping:\n",
    "    \"\"\"Early stopping dengan metric monitoring yang lebih baik\"\"\"\n",
    "    \n",
    "    def __init__(self, patience=5, min_delta=0.001, mode='min'):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.mode = mode  # 'min' for loss, 'max' for accuracy\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.best_model = None\n",
    "    \n",
    "    def __call__(self, score, model):\n",
    "        if self.mode == 'min':\n",
    "            is_improvement = self.best_score is None or score < self.best_score - self.min_delta\n",
    "        else:\n",
    "            is_improvement = self.best_score is None or score > self.best_score + self.min_delta\n",
    "        \n",
    "        if is_improvement:\n",
    "            self.best_score = score\n",
    "            self.best_model = copy.deepcopy(model.state_dict())\n",
    "            self.counter = 0\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        \n",
    "        return self.early_stop\n",
    "\n",
    "print('‚úì Training functions defined')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a3e23c",
   "metadata": {},
   "source": [
    "## üöÄ 9. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16cda797",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training history\n",
    "history = {\n",
    "    'train_loss': [], 'train_acc': [], 'train_f1': [],\n",
    "    'val_loss': [], 'val_acc': [], 'val_f1': []\n",
    "}\n",
    "\n",
    "# Early stopping - monitor validation F1 (mode='max')\n",
    "early_stopping = EarlyStopping(patience=CONFIG['early_stopping_patience'], mode='max')\n",
    "\n",
    "print('=' * 60)\n",
    "print('üöÄ TRAINING STARTED')\n",
    "print('=' * 60)\n",
    "print(f'Epochs: {CONFIG[\"epochs\"]} | Early Stopping Patience: {CONFIG[\"early_stopping_patience\"]}')\n",
    "print(f'Learning Rate: {CONFIG[\"learning_rate\"]} | Batch Size: {CONFIG[\"batch_size\"]}')\n",
    "print(f'Frozen Layers: {CONFIG[\"freeze_layers\"]} | Dropout: {CONFIG[\"dropout_rate\"]}')\n",
    "print('-' * 60)\n",
    "\n",
    "best_val_f1 = 0\n",
    "best_epoch = 0\n",
    "\n",
    "for epoch in range(CONFIG['epochs']):\n",
    "    print(f'\\nüìç Epoch {epoch + 1}/{CONFIG[\"epochs\"]}')\n",
    "    \n",
    "    # Train\n",
    "    train_loss, train_acc, train_f1 = train_epoch(\n",
    "        model, train_loader, criterion, optimizer, scheduler, \n",
    "        device, CONFIG['max_grad_norm']\n",
    "    )\n",
    "    \n",
    "    # Validate\n",
    "    val_loss, val_acc, val_f1, _, _ = evaluate(\n",
    "        model, val_loader, criterion, device\n",
    "    )\n",
    "    \n",
    "    # Save history\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['train_acc'].append(train_acc)\n",
    "    history['train_f1'].append(train_f1)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['val_acc'].append(val_acc)\n",
    "    history['val_f1'].append(val_f1)\n",
    "    \n",
    "    # Print metrics\n",
    "    print(f'  Train - Loss: {train_loss:.4f} | Acc: {train_acc:.4f} | F1: {train_f1:.4f}')\n",
    "    print(f'  Val   - Loss: {val_loss:.4f} | Acc: {val_acc:.4f} | F1: {val_f1:.4f}')\n",
    "    \n",
    "    # Track best model based on F1\n",
    "    if val_f1 > best_val_f1:\n",
    "        best_val_f1 = val_f1\n",
    "        best_epoch = epoch + 1\n",
    "        print(f'  ‚≠ê New best validation F1!')\n",
    "    \n",
    "    # Check overfitting\n",
    "    gap = train_acc - val_acc\n",
    "    print(f'  üìä Train-Val Gap: {gap:.4f}', end='')\n",
    "    if gap > 0.10:\n",
    "        print(' ‚ö†Ô∏è Overfitting!')\n",
    "    elif gap > 0.05:\n",
    "        print(' ‚ö° Slight gap')\n",
    "    else:\n",
    "        print(' ‚úÖ Good')\n",
    "    \n",
    "    # Early stopping check - based on val_f1\n",
    "    if early_stopping(val_f1, model):\n",
    "        print(f'\\nüõë Early stopping triggered at epoch {epoch + 1}')\n",
    "        print(f'   Best F1 was at epoch {best_epoch}')\n",
    "        break\n",
    "\n",
    "# Load best model\n",
    "model.load_state_dict(early_stopping.best_model)\n",
    "print(f'\\n‚úì Loaded best model from epoch {best_epoch}')\n",
    "print(f'  Best Val F1: {best_val_f1:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "032317be",
   "metadata": {},
   "source": [
    "## üìà 10. Training Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e739f346",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "epochs_range = range(1, len(history['train_loss']) + 1)\n",
    "\n",
    "# Loss\n",
    "axes[0].plot(epochs_range, history['train_loss'], 'b-', label='Train Loss', marker='o')\n",
    "axes[0].plot(epochs_range, history['val_loss'], 'r-', label='Val Loss', marker='s')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('Training & Validation Loss')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Accuracy\n",
    "axes[1].plot(epochs_range, history['train_acc'], 'b-', label='Train Acc', marker='o')\n",
    "axes[1].plot(epochs_range, history['val_acc'], 'r-', label='Val Acc', marker='s')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Accuracy')\n",
    "axes[1].set_title('Training & Validation Accuracy')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# F1 Score\n",
    "axes[2].plot(epochs_range, history['train_f1'], 'b-', label='Train F1', marker='o')\n",
    "axes[2].plot(epochs_range, history['val_f1'], 'r-', label='Val F1', marker='s')\n",
    "axes[2].set_xlabel('Epoch')\n",
    "axes[2].set_ylabel('F1 Score')\n",
    "axes[2].set_title('Training & Validation F1 Score')\n",
    "axes[2].legend()\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('training_history.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Check for overfitting\n",
    "final_gap = history['train_acc'][-1] - history['val_acc'][-1]\n",
    "print(f'\\nüìä Overfitting Analysis:')\n",
    "print(f'  Final Train Accuracy: {history[\"train_acc\"][-1]:.4f}')\n",
    "print(f'  Final Val Accuracy:   {history[\"val_acc\"][-1]:.4f}')\n",
    "print(f'  Gap (Train - Val):    {final_gap:.4f}')\n",
    "\n",
    "if final_gap < 0.03:\n",
    "    print('  ‚úÖ Model is NOT overfitting (gap < 3%)')\n",
    "elif final_gap < 0.05:\n",
    "    print('  ‚ö†Ô∏è  Slight overfitting (gap 3-5%)')\n",
    "else:\n",
    "    print('  ‚ùå Model is overfitting (gap > 5%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c53cad3",
   "metadata": {},
   "source": [
    "## üß™ 11. Test Set Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f0ff16f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test set\n",
    "print('=' * 60)\n",
    "print('üß™ TEST SET EVALUATION')\n",
    "print('=' * 60)\n",
    "\n",
    "test_loss, test_acc, test_f1, test_preds, test_labels = evaluate(\n",
    "    model, test_loader, criterion, device\n",
    ")\n",
    "\n",
    "print(f'\\nüìä Test Results:')\n",
    "print(f'  Loss:     {test_loss:.4f}')\n",
    "print(f'  Accuracy: {test_acc:.4f} ({test_acc*100:.2f}%)')\n",
    "print(f'  F1 Score: {test_f1:.4f}')\n",
    "\n",
    "# Classification report\n",
    "print('\\n' + '=' * 60)\n",
    "print('üìã CLASSIFICATION REPORT')\n",
    "print('=' * 60)\n",
    "print(classification_report(test_labels, test_preds, target_names=LABEL_NAMES, digits=4))\n",
    "\n",
    "# Per-class metrics\n",
    "precision, recall, f1, support = precision_recall_fscore_support(\n",
    "    test_labels, test_preds, average=None, labels=[0, 1, 2]\n",
    ")\n",
    "\n",
    "print('\\nüìä Per-Class Metrics:')\n",
    "for i, label in enumerate(LABEL_NAMES):\n",
    "    print(f'  {label.upper():10} - P: {precision[i]:.4f} | R: {recall[i]:.4f} | F1: {f1[i]:.4f} | N: {support[i]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df405746",
   "metadata": {},
   "source": [
    "## üî• 12. Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "024ddfdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "cm = confusion_matrix(test_labels, test_preds)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Absolute numbers\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=LABEL_NAMES, yticklabels=LABEL_NAMES, ax=axes[0])\n",
    "axes[0].set_xlabel('Predicted')\n",
    "axes[0].set_ylabel('Actual')\n",
    "axes[0].set_title('Confusion Matrix (Counts)')\n",
    "\n",
    "# Normalized (percentages)\n",
    "cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] * 100\n",
    "sns.heatmap(cm_normalized, annot=True, fmt='.1f', cmap='Blues',\n",
    "            xticklabels=LABEL_NAMES, yticklabels=LABEL_NAMES, ax=axes[1])\n",
    "axes[1].set_xlabel('Predicted')\n",
    "axes[1].set_ylabel('Actual')\n",
    "axes[1].set_title('Confusion Matrix (Percentages %)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('confusion_matrix.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Analysis\n",
    "print('\\nüìä Confusion Matrix Analysis:')\n",
    "for i, label in enumerate(LABEL_NAMES):\n",
    "    correct = cm[i, i]\n",
    "    total = cm[i].sum()\n",
    "    print(f'  {label.upper():10} - Correct: {correct}/{total} ({correct/total*100:.1f}%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13cbe767",
   "metadata": {},
   "source": [
    "## üíæ 13. Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "748403d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create models directory di folder skripsi (Google Drive)\n",
    "os.makedirs('models', exist_ok=True)\n",
    "\n",
    "# Save model ke Google Drive\n",
    "model_path = 'models/indobert_sentiment_3class.pt'\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'config': CONFIG,\n",
    "    'label_map': LABEL_MAP,\n",
    "    'label_names': LABEL_NAMES,\n",
    "    'test_accuracy': test_acc,\n",
    "    'test_f1': test_f1,\n",
    "    'history': history,\n",
    "}, model_path)\n",
    "print(f'‚úì Model saved to: {DRIVE_PATH}/{model_path}')\n",
    "\n",
    "# Save training history\n",
    "history_path = 'models/training_history.json'\n",
    "with open(history_path, 'w') as f:\n",
    "    json.dump(history, f, indent=2)\n",
    "print(f'‚úì History saved to: {DRIVE_PATH}/{history_path}')\n",
    "\n",
    "# Save tokenizer\n",
    "tokenizer.save_pretrained('models/tokenizer')\n",
    "print(f'‚úì Tokenizer saved to: {DRIVE_PATH}/models/tokenizer/')\n",
    "\n",
    "print(f'\\n‚úÖ Semua file tersimpan di Google Drive folder: {DRIVE_PATH}/models/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e057cda6",
   "metadata": {},
   "source": [
    "## üîÆ 14. Inference Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "904a4469",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentiment(text, model, tokenizer, device, label_names):\n",
    "    \"\"\"Prediksi sentiment untuk satu teks\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    encoding = tokenizer.encode_plus(\n",
    "        text,\n",
    "        add_special_tokens=True,\n",
    "        max_length=128,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_attention_mask=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    \n",
    "    input_ids = encoding['input_ids'].to(device)\n",
    "    attention_mask = encoding['attention_mask'].to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        logits = model(input_ids, attention_mask)\n",
    "        probs = torch.softmax(logits, dim=1)\n",
    "        pred = torch.argmax(probs, dim=1).item()\n",
    "    \n",
    "    return {\n",
    "        'sentiment': label_names[pred],\n",
    "        'confidence': probs[0][pred].item(),\n",
    "        'probabilities': {\n",
    "            label_names[i]: probs[0][i].item() \n",
    "            for i in range(len(label_names))\n",
    "        }\n",
    "    }\n",
    "\n",
    "# Test dengan contoh\n",
    "test_reviews = [\n",
    "    \"Aplikasi gojek sangat membantu, driver ramah dan cepat\",\n",
    "    \"Driver nya lama banget, udah nunggu 1 jam gak datang datang\",\n",
    "    \"Biasa aja sih aplikasinya\",\n",
    "    \"Pelayanan buruk, driver tidak sopan, tidak akan pakai lagi\",\n",
    "    \"Mantap, makanan sampai dengan selamat dan masih hangat\",\n",
    "    \"Ongkirnya agak mahal tapi ya lumayan lah\"\n",
    "]\n",
    "\n",
    "print('=' * 60)\n",
    "print('üîÆ INFERENCE DEMO')\n",
    "print('=' * 60)\n",
    "\n",
    "for review in test_reviews:\n",
    "    result = predict_sentiment(review, model, tokenizer, device, LABEL_NAMES)\n",
    "    emoji = {'negative': 'üò†', 'neutral': 'üòê', 'positive': 'üòä'}[result['sentiment']]\n",
    "    print(f'\\nüìù \"{review[:50]}...\"' if len(review) > 50 else f'\\nüìù \"{review}\"')\n",
    "    print(f'   {emoji} Sentiment: {result[\"sentiment\"].upper()} (Confidence: {result[\"confidence\"]*100:.1f}%)')\n",
    "    print(f'   Probabilities: Neg={result[\"probabilities\"][\"negative\"]*100:.1f}% | '\n",
    "          f'Neu={result[\"probabilities\"][\"neutral\"]*100:.1f}% | '\n",
    "          f'Pos={result[\"probabilities\"][\"positive\"]*100:.1f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d417e4f",
   "metadata": {},
   "source": [
    "## üìä 15. Final Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9dca419",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary\n",
    "print('=' * 60)\n",
    "print('üìä FINAL SUMMARY')\n",
    "print('=' * 60)\n",
    "\n",
    "# Calculate final gap\n",
    "final_gap = history['train_acc'][-1] - history['val_acc'][-1]\n",
    "best_val_acc = max(history['val_acc'])\n",
    "\n",
    "print(f'''\n",
    "üéØ MODEL PERFORMANCE:\n",
    "   ‚Ä¢ Test Accuracy: {test_acc*100:.2f}%\n",
    "   ‚Ä¢ Test F1 Score: {test_f1*100:.2f}%\n",
    "   ‚Ä¢ Best Validation Accuracy: {best_val_acc*100:.2f}%\n",
    "\n",
    "üìà OVERFITTING CHECK:\n",
    "   ‚Ä¢ Train-Val Gap: {final_gap*100:.2f}%\n",
    "   ‚Ä¢ Status: {\"‚úÖ Not Overfitting\" if final_gap < 0.05 else \"‚ö†Ô∏è Potential Overfitting\"}\n",
    "\n",
    "‚öôÔ∏è ANTI-OVERFITTING TECHNIQUES USED:\n",
    "   ‚Ä¢ Data Balancing (Undersampling)\n",
    "   ‚Ä¢ Layer Freezing: {CONFIG['freeze_layers']} layers\n",
    "   ‚Ä¢ Dropout Rate: {CONFIG['dropout_rate']}\n",
    "   ‚Ä¢ Label Smoothing: {CONFIG['label_smoothing']}\n",
    "   ‚Ä¢ Weight Decay: {CONFIG['weight_decay']}\n",
    "   ‚Ä¢ Early Stopping (Patience: {CONFIG['early_stopping_patience']})\n",
    "   ‚Ä¢ Learning Rate Warmup\n",
    "   ‚Ä¢ Gradient Clipping\n",
    "   ‚Ä¢ Word Dropout Augmentation\n",
    "   ‚Ä¢ Word Swap Augmentation\n",
    "   ‚Ä¢ Batch Size: {CONFIG['batch_size']}\n",
    "\n",
    "üíæ SAVED FILES:\n",
    "   ‚Ä¢ Model: {DRIVE_PATH}/models/indobert_sentiment_3class.pt\n",
    "   ‚Ä¢ Tokenizer: {DRIVE_PATH}/models/tokenizer/\n",
    "   ‚Ä¢ History: {DRIVE_PATH}/models/training_history.json\n",
    "''')\n",
    "\n",
    "print('=' * 60)\n",
    "print('‚úÖ Training completed successfully!')\n",
    "print('=' * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3575efe7",
   "metadata": {},
   "source": [
    "## üì• 16. Download Model (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df3ac3f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download model dari Colab ke komputer lokal\n",
    "from google.colab import files\n",
    "\n",
    "# Zip model folder untuk download\n",
    "!cd /content && zip -r model_sentiment_3class.zip models/\n",
    "\n",
    "# Download\n",
    "files.download('/content/model_sentiment_3class.zip')\n",
    "print('‚úì Model siap di-download!')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
