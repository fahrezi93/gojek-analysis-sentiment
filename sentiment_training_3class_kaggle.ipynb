{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d424b084",
   "metadata": {},
   "source": [
    "# üöÄ IndoBERT Sentiment Analysis 3 Kelas - Kaggle Version\n",
    "\n",
    "**Dataset**: gojek_reviews_3class_clean.csv  \n",
    "**Model**: IndoBERT (indobenchmark/indobert-base-p1)  \n",
    "**Target**: Akurasi tinggi dengan generalisasi yang baik (tidak overfitting)\n",
    "\n",
    "## üìã Persiapan Sebelum Running:\n",
    "\n",
    "1. **Upload dataset** ke Kaggle sebagai Dataset atau langsung upload file\n",
    "2. **Aktifkan GPU**: Settings ‚Üí Accelerator ‚Üí GPU T4 x2 atau P100\n",
    "3. **Enable Internet**: Settings ‚Üí Internet ‚Üí On\n",
    "\n",
    "---\n",
    "\n",
    "### Teknik Anti-Overfitting yang Digunakan:\n",
    "1. **Layer Freezing** - Freeze 6 layer BERT pertama\n",
    "2. **Data Balancing** - Undersampling ke kelas minoritas\n",
    "3. **High Dropout** - 0.5 untuk regularisasi agresif\n",
    "4. **Label Smoothing** - 0.15 untuk soft labels\n",
    "5. **Early Stopping** - Patience 5, monitor F1 score\n",
    "6. **Weight Decay** - L2 regularization (0.02)\n",
    "7. **Learning Rate Warmup** - Gradual increase\n",
    "8. **Gradient Clipping** - Mencegah exploding gradients\n",
    "9. **Data Augmentation** - Word dropout, swap, duplication\n",
    "\n",
    "### Kelas Sentiment:\n",
    "- **0 = Negative** (Score 1-2)\n",
    "- **1 = Neutral** (Score 3)\n",
    "- **2 = Positive** (Score 4-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3758e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# SETUP KAGGLE\n",
    "# ============================================\n",
    "\n",
    "# Install dependencies\n",
    "!pip install transformers -q\n",
    "\n",
    "# Check GPU\n",
    "import torch\n",
    "print(f'PyTorch version: {torch.__version__}')\n",
    "if torch.cuda.is_available():\n",
    "    print(f'‚úì GPU Available: {torch.cuda.get_device_name(0)}')\n",
    "    print(f'‚úì GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB')\n",
    "else:\n",
    "    print('‚ö†Ô∏è GPU not available, using CPU (akan lebih lambat)')\n",
    "\n",
    "# List input files\n",
    "import os\n",
    "print('\\nüìÅ Input files:')\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b23c5d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "from transformers import BertTokenizer, BertModel, get_linear_schedule_with_warmup\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_recall_fscore_support, \n",
    "    classification_report, confusion_matrix, f1_score\n",
    ")\n",
    "from sklearn.utils import resample\n",
    "import random\n",
    "import os\n",
    "import copy\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Reproducibility\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "# Device setup (sudah di-check di cell sebelumnya)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'üñ•Ô∏è  Device: {device}')\n",
    "if torch.cuda.is_available():\n",
    "    print(f'üéÆ GPU: {torch.cuda.get_device_name(0)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7dbb95b",
   "metadata": {},
   "source": [
    "## üìä 1. Load & Explore Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dec10da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# LOAD DATA - SESUAIKAN PATH SESUAI DATASET KAMU\n",
    "# ============================================\n",
    "# File yang direkomendasikan: gojek_reviews_final_augmented.csv (15,000 samples, 5000/kelas)\n",
    "\n",
    "# Cari file otomatis (prioritaskan file augmented)\n",
    "DATA_PATH = None\n",
    "priority_files = [\n",
    "    'gojek_reviews_final_augmented',  # 15,000 samples - RECOMMENDED\n",
    "    'gojek_reviews_3class_balanced',\n",
    "    'gojek_reviews_3class_clean',\n",
    "    'gojek_reviews'\n",
    "]\n",
    "\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for priority in priority_files:\n",
    "        for filename in filenames:\n",
    "            if priority in filename and filename.endswith('.csv'):\n",
    "                DATA_PATH = os.path.join(dirname, filename)\n",
    "                print(f'‚úì Found data file: {DATA_PATH}')\n",
    "                break\n",
    "        if DATA_PATH:\n",
    "            break\n",
    "    if DATA_PATH:\n",
    "        break\n",
    "\n",
    "if DATA_PATH is None:\n",
    "    print('‚ùå Data file tidak ditemukan!')\n",
    "    print('\\nüìÅ Files yang tersedia:')\n",
    "    for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "        for filename in filenames:\n",
    "            print(f'   {os.path.join(dirname, filename)}')\n",
    "    print('\\nüí° Upload file gojek_reviews_final_augmented.csv ke Kaggle')\n",
    "else:\n",
    "    df = pd.read_csv(DATA_PATH)\n",
    "    \n",
    "    print('=' * 60)\n",
    "    print('üìä DATA OVERVIEW')\n",
    "    print('=' * 60)\n",
    "    print(f'Total samples: {len(df):,}')\n",
    "    print(f'\\nColumns: {df.columns.tolist()}')\n",
    "    print(f'\\nüìà Sentiment Distribution:')\n",
    "    print(df['sentiment'].value_counts())\n",
    "    \n",
    "    # Visualize\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "    \n",
    "    # Bar plot\n",
    "    colors = {'negative': '#e74c3c', 'neutral': '#95a5a6', 'positive': '#2ecc71'}\n",
    "    sentiment_counts = df['sentiment'].value_counts()\n",
    "    axes[0].bar(sentiment_counts.index, sentiment_counts.values, \n",
    "                color=[colors[s] for s in sentiment_counts.index])\n",
    "    axes[0].set_title('Sentiment Distribution')\n",
    "    axes[0].set_ylabel('Count')\n",
    "    \n",
    "    # Pie chart\n",
    "    axes[1].pie(sentiment_counts.values, labels=sentiment_counts.index, \n",
    "                autopct='%1.1f%%', colors=[colors[s] for s in sentiment_counts.index])\n",
    "    axes[1].set_title('Sentiment Percentage')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8c35280",
   "metadata": {},
   "source": [
    "## ‚öñÔ∏è 2. Balance Data (Undersampling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b96c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if data is already balanced\n",
    "counts = df['sentiment'].value_counts()\n",
    "min_count = counts.min()\n",
    "max_count = counts.max()\n",
    "\n",
    "# If already balanced (difference < 10%), skip undersampling\n",
    "if (max_count - min_count) / max_count < 0.1:\n",
    "    print('‚úì Data sudah balanced! Skip undersampling.')\n",
    "    df_balanced = df.copy()\n",
    "else:\n",
    "    # Balance data menggunakan undersampling\n",
    "    print(f'‚ö†Ô∏è Data tidak balanced. Melakukan undersampling...')\n",
    "    print(f'Kelas minoritas: {min_count} samples')\n",
    "    \n",
    "    df_balanced = pd.DataFrame()\n",
    "    for sentiment in ['negative', 'neutral', 'positive']:\n",
    "        df_class = df[df['sentiment'] == sentiment]\n",
    "        df_sampled = resample(df_class, replace=False, n_samples=min_count, random_state=42)\n",
    "        df_balanced = pd.concat([df_balanced, df_sampled])\n",
    "    \n",
    "    # Shuffle\n",
    "    df_balanced = df_balanced.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "print('\\n' + '=' * 60)\n",
    "print('‚öñÔ∏è  DATA UNTUK TRAINING')\n",
    "print('=' * 60)\n",
    "print(f'Total: {len(df_balanced):,}')\n",
    "print(df_balanced['sentiment'].value_counts())\n",
    "\n",
    "# Visualize balanced\n",
    "plt.figure(figsize=(8, 4))\n",
    "balanced_counts = df_balanced['sentiment'].value_counts()\n",
    "colors = {'negative': '#e74c3c', 'neutral': '#95a5a6', 'positive': '#2ecc71'}\n",
    "plt.bar(balanced_counts.index, balanced_counts.values, \n",
    "        color=[colors[s] for s in balanced_counts.index])\n",
    "plt.title('Sentiment Distribution for Training')\n",
    "plt.ylabel('Count')\n",
    "for i, (label, count) in enumerate(balanced_counts.items()):\n",
    "    plt.text(i, count + 50, str(count), ha='center', fontweight='bold')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd5a9692",
   "metadata": {},
   "source": [
    "## üè∑Ô∏è 3. Prepare Labels & Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf731b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label mapping\n",
    "LABEL_MAP = {'negative': 0, 'neutral': 1, 'positive': 2}\n",
    "LABEL_NAMES = ['negative', 'neutral', 'positive']\n",
    "NUM_CLASSES = 3\n",
    "\n",
    "df_balanced['label'] = df_balanced['sentiment'].map(LABEL_MAP)\n",
    "\n",
    "# Split: 70% train, 15% validation, 15% test (stratified)\n",
    "# Stratified split memastikan distribusi kelas sama di setiap split\n",
    "train_df, temp_df = train_test_split(\n",
    "    df_balanced, test_size=0.3, random_state=42, \n",
    "    stratify=df_balanced['label']\n",
    ")\n",
    "val_df, test_df = train_test_split(\n",
    "    temp_df, test_size=0.5, random_state=42, \n",
    "    stratify=temp_df['label']\n",
    ")\n",
    "\n",
    "print('=' * 60)\n",
    "print('üìÇ DATA SPLITS')\n",
    "print('=' * 60)\n",
    "print(f'Train: {len(train_df):,} samples ({len(train_df)/len(df_balanced)*100:.1f}%)')\n",
    "print(f'Val:   {len(val_df):,} samples ({len(val_df)/len(df_balanced)*100:.1f}%)')\n",
    "print(f'Test:  {len(test_df):,} samples ({len(test_df)/len(df_balanced)*100:.1f}%)')\n",
    "\n",
    "print(f'\\nüìä Train label distribution:')\n",
    "print(train_df['sentiment'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b7adbaf",
   "metadata": {},
   "source": [
    "## üîß 4. Hyperparameters & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d7aaccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === HYPERPARAMETERS ===\n",
    "# ULTRA-OPTIMIZED untuk menghindari overfitting dan meningkatkan akurasi\n",
    "# Strategi: Freeze lebih banyak layer, regularisasi lebih kuat, learning rate lebih kecil\n",
    "\n",
    "CONFIG = {\n",
    "    # Model\n",
    "    'model_name': 'indobenchmark/indobert-base-p1',\n",
    "    'max_length': 128,\n",
    "    'num_classes': NUM_CLASSES,\n",
    "    \n",
    "    # Training - CONSERVATIVE untuk hindari overfitting\n",
    "    'batch_size': 32,\n",
    "    'epochs': 30,  # Lebih banyak epoch dengan early stopping\n",
    "    'learning_rate': 5e-6,  # SANGAT KECIL - kunci mengurangi overfitting\n",
    "    \n",
    "    # Anti-Overfitting - MAXIMUM REGULARIZATION\n",
    "    'dropout_rate': 0.6,  # Tinggi\n",
    "    'attention_dropout': 0.3,  # Dropout di attention juga\n",
    "    'weight_decay': 0.05,  # L2 regularization lebih kuat\n",
    "    'label_smoothing': 0.2,  # Lebih tinggi\n",
    "    'warmup_ratio': 0.15,  # Warmup lebih lama\n",
    "    'max_grad_norm': 0.5,  # Gradient clipping lebih ketat\n",
    "    'early_stopping_patience': 7,  # Lebih sabar\n",
    "    \n",
    "    # Data Augmentation - Enhanced\n",
    "    'word_dropout_prob': 0.2,\n",
    "    'mixup_alpha': 0.2,  # Mixup augmentation\n",
    "    \n",
    "    # Layer Freezing - FREEZE MORE LAYERS\n",
    "    'freeze_layers': 9,  # Freeze 9 dari 12 layer (hanya 3 layer trainable)\n",
    "    \n",
    "    # R-Drop regularization\n",
    "    'rdrop_alpha': 0.5,  # KL divergence loss weight\n",
    "}\n",
    "\n",
    "print('=' * 60)\n",
    "print('‚öôÔ∏è  ULTRA-OPTIMIZED CONFIGURATION')\n",
    "print('=' * 60)\n",
    "print('üéØ Strategy: Maximum regularization + Minimal trainable params')\n",
    "print('-' * 60)\n",
    "for key, value in CONFIG.items():\n",
    "    print(f'{key}: {value}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e2d2e8",
   "metadata": {},
   "source": [
    "## üì¶ 5. Dataset Class with Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f78d58e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(CONFIG['model_name'])\n",
    "print(f'‚úì Tokenizer loaded: {CONFIG[\"model_name\"]}')\n",
    "\n",
    "class SentimentDataset(Dataset):\n",
    "    \"\"\"Dataset dengan ENHANCED augmentation + Mixup support\"\"\"\n",
    "    \n",
    "    def __init__(self, texts, labels, tokenizer, max_length=128, \n",
    "                 augment=False, word_dropout_prob=0.2):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.augment = augment\n",
    "        self.word_dropout_prob = word_dropout_prob\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def _augment_text(self, text):\n",
    "        \"\"\"Enhanced augmentation dengan multiple techniques\"\"\"\n",
    "        if not self.augment:\n",
    "            return text\n",
    "            \n",
    "        text = str(text)\n",
    "        words = text.split()\n",
    "        \n",
    "        if len(words) <= 3:\n",
    "            return text\n",
    "        \n",
    "        # Randomly choose augmentation technique\n",
    "        aug_type = random.random()\n",
    "        \n",
    "        if aug_type < 0.3:\n",
    "            # Word dropout - hapus beberapa kata\n",
    "            words = [w for w in words if random.random() > self.word_dropout_prob]\n",
    "        elif aug_type < 0.5:\n",
    "            # Word swap - tukar posisi kata\n",
    "            if len(words) > 2:\n",
    "                idx = random.randint(0, len(words) - 2)\n",
    "                words[idx], words[idx + 1] = words[idx + 1], words[idx]\n",
    "        elif aug_type < 0.7:\n",
    "            # Random deletion - hapus 1 kata random\n",
    "            if len(words) > 4:\n",
    "                del_idx = random.randint(0, len(words) - 1)\n",
    "                words.pop(del_idx)\n",
    "        elif aug_type < 0.85:\n",
    "            # Shuffle middle words (keep first and last)\n",
    "            if len(words) > 4:\n",
    "                middle = words[1:-1]\n",
    "                random.shuffle(middle)\n",
    "                words = [words[0]] + middle + [words[-1]]\n",
    "        # else: no augmentation (15% chance)\n",
    "        \n",
    "        return ' '.join(words) if words else text\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = self._augment_text(self.texts[idx])\n",
    "        \n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            str(text),\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'label': torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = SentimentDataset(\n",
    "    train_df['content_clean'].values,\n",
    "    train_df['label'].values,\n",
    "    tokenizer,\n",
    "    max_length=CONFIG['max_length'],\n",
    "    augment=True,\n",
    "    word_dropout_prob=CONFIG['word_dropout_prob']\n",
    ")\n",
    "\n",
    "val_dataset = SentimentDataset(\n",
    "    val_df['content_clean'].values,\n",
    "    val_df['label'].values,\n",
    "    tokenizer,\n",
    "    max_length=CONFIG['max_length'],\n",
    "    augment=False\n",
    ")\n",
    "\n",
    "test_dataset = SentimentDataset(\n",
    "    test_df['content_clean'].values,\n",
    "    test_df['label'].values,\n",
    "    tokenizer,\n",
    "    max_length=CONFIG['max_length'],\n",
    "    augment=False\n",
    ")\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=CONFIG['batch_size'], shuffle=True, drop_last=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=CONFIG['batch_size'], shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=CONFIG['batch_size'], shuffle=False)\n",
    "\n",
    "print(f'\\n‚úì Datasets created:')\n",
    "print(f'  Train: {len(train_dataset)} samples, {len(train_loader)} batches')\n",
    "print(f'  Val:   {len(val_dataset)} samples, {len(val_loader)} batches')\n",
    "print(f'  Test:  {len(test_dataset)} samples, {len(test_loader)} batches')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "097b6f0c",
   "metadata": {},
   "source": [
    "## üß† 6. Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "630ce4ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IndoBERTSentimentClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    IndoBERT dengan regularisasi MAKSIMAL untuk mencegah overfitting:\n",
    "    - Freeze 9 dari 12 layer BERT (hanya 3 layer trainable)\n",
    "    - Multiple dropout layers\n",
    "    - Attention dropout\n",
    "    - Simple classifier (hindari overfitting di classifier)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model_name, num_classes, dropout_rate=0.6, \n",
    "                 attention_dropout=0.3, freeze_layers=9):\n",
    "        super(IndoBERTSentimentClassifier, self).__init__()\n",
    "        \n",
    "        # Load pretrained BERT\n",
    "        self.bert = BertModel.from_pretrained(model_name)\n",
    "        self.hidden_size = self.bert.config.hidden_size\n",
    "        \n",
    "        # === FREEZE BERT LAYERS - MORE AGGRESSIVE ===\n",
    "        # Freeze embeddings\n",
    "        for param in self.bert.embeddings.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        # Freeze first N encoder layers\n",
    "        for i in range(freeze_layers):\n",
    "            for param in self.bert.encoder.layer[i].parameters():\n",
    "                param.requires_grad = False\n",
    "        \n",
    "        # Also add dropout to attention in unfrozen layers\n",
    "        for i in range(freeze_layers, 12):\n",
    "            self.bert.encoder.layer[i].attention.self.dropout = nn.Dropout(attention_dropout)\n",
    "            self.bert.encoder.layer[i].attention.output.dropout = nn.Dropout(attention_dropout)\n",
    "        \n",
    "        print(f'‚úì Froze embeddings and first {freeze_layers} encoder layers')\n",
    "        print(f'  Only layers {freeze_layers}-11 are trainable (3 layers)')\n",
    "        \n",
    "        # Regularization - AGGRESSIVE\n",
    "        self.dropout1 = nn.Dropout(dropout_rate)\n",
    "        self.dropout2 = nn.Dropout(dropout_rate)\n",
    "        self.dropout3 = nn.Dropout(dropout_rate * 0.5)\n",
    "        self.layer_norm = nn.LayerNorm(self.hidden_size)\n",
    "        \n",
    "        # SIMPLER Classifier - hindari overfitting\n",
    "        # Langsung ke output, tanpa hidden layer kompleks\n",
    "        self.fc = nn.Linear(self.hidden_size, num_classes)\n",
    "        \n",
    "        # Initialize weights\n",
    "        nn.init.xavier_uniform_(self.fc.weight)\n",
    "        nn.init.zeros_(self.fc.bias)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask, return_hidden=False):\n",
    "        # Get BERT output\n",
    "        outputs = self.bert(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask\n",
    "        )\n",
    "        \n",
    "        # Use [CLS] token representation\n",
    "        pooled_output = outputs.pooler_output\n",
    "        \n",
    "        # Apply regularization pipeline dengan multiple dropout\n",
    "        x = self.layer_norm(pooled_output)\n",
    "        x = self.dropout1(x)\n",
    "        \n",
    "        # Monte Carlo Dropout - apply dropout multiple times during training\n",
    "        if self.training:\n",
    "            x = self.dropout2(x)\n",
    "        \n",
    "        logits = self.fc(x)\n",
    "        \n",
    "        if return_hidden:\n",
    "            return logits, pooled_output\n",
    "        return logits\n",
    "\n",
    "# Initialize model\n",
    "model = IndoBERTSentimentClassifier(\n",
    "    model_name=CONFIG['model_name'],\n",
    "    num_classes=CONFIG['num_classes'],\n",
    "    dropout_rate=CONFIG['dropout_rate'],\n",
    "    attention_dropout=CONFIG['attention_dropout'],\n",
    "    freeze_layers=CONFIG['freeze_layers']\n",
    ").to(device)\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "frozen_params = total_params - trainable_params\n",
    "\n",
    "print(f'\\n‚úì Model initialized')\n",
    "print(f'  Total parameters: {total_params:,}')\n",
    "print(f'  Trainable parameters: {trainable_params:,} ({trainable_params/total_params*100:.1f}%)')\n",
    "print(f'  Frozen parameters: {frozen_params:,} ({frozen_params/total_params*100:.1f}%)')\n",
    "print(f'\\n‚ö†Ô∏è Note: Trainable params sangat sedikit = lebih sulit overfitting')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbd46a93",
   "metadata": {},
   "source": [
    "## üìâ 7. Loss Function, Optimizer & Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e71b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function dengan label smoothing\n",
    "criterion = nn.CrossEntropyLoss(label_smoothing=CONFIG['label_smoothing'])\n",
    "\n",
    "# Optimizer - ONLY for trainable parameters\n",
    "# Pisahkan parameter yang perlu weight decay dan yang tidak\n",
    "no_decay = ['bias', 'LayerNorm.weight', 'layer_norm.weight']\n",
    "\n",
    "# Filter hanya parameter yang requires_grad=True\n",
    "trainable_params_list = [(n, p) for n, p in model.named_parameters() if p.requires_grad]\n",
    "\n",
    "optimizer_grouped_parameters = [\n",
    "    {\n",
    "        'params': [p for n, p in trainable_params_list if not any(nd in n for nd in no_decay)],\n",
    "        'weight_decay': CONFIG['weight_decay']\n",
    "    },\n",
    "    {\n",
    "        'params': [p for n, p in trainable_params_list if any(nd in n for nd in no_decay)],\n",
    "        'weight_decay': 0.0\n",
    "    }\n",
    "]\n",
    "\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=CONFIG['learning_rate'])\n",
    "\n",
    "# Learning rate scheduler dengan warmup\n",
    "total_steps = len(train_loader) * CONFIG['epochs']\n",
    "warmup_steps = int(total_steps * CONFIG['warmup_ratio'])\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=warmup_steps,\n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "print(f'‚úì Optimizer: AdamW (lr={CONFIG[\"learning_rate\"]}, weight_decay={CONFIG[\"weight_decay\"]})')\n",
    "print(f'‚úì Scheduler: Linear warmup ({warmup_steps} warmup steps, {total_steps} total steps)')\n",
    "print(f'‚úì Loss: CrossEntropy with label_smoothing={CONFIG[\"label_smoothing\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3339bb5",
   "metadata": {},
   "source": [
    "## üèãÔ∏è 8. Training Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5077bb3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def compute_kl_loss(p, q):\n",
    "    \"\"\"Compute KL divergence loss for R-Drop\"\"\"\n",
    "    p_loss = F.kl_div(F.log_softmax(p, dim=-1), F.softmax(q, dim=-1), reduction='batchmean')\n",
    "    q_loss = F.kl_div(F.log_softmax(q, dim=-1), F.softmax(p, dim=-1), reduction='batchmean')\n",
    "    return (p_loss + q_loss) / 2\n",
    "\n",
    "def train_epoch_rdrop(model, dataloader, criterion, optimizer, scheduler, device, \n",
    "                      max_grad_norm, rdrop_alpha=0.5):\n",
    "    \"\"\"Train dengan R-Drop regularization untuk mengurangi overfitting\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    progress_bar = tqdm(dataloader, desc='Training', leave=False)\n",
    "    \n",
    "    for batch in progress_bar:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # R-Drop: Forward pass 2 kali dengan dropout berbeda\n",
    "        logits1 = model(input_ids, attention_mask)\n",
    "        logits2 = model(input_ids, attention_mask)\n",
    "        \n",
    "        # Cross entropy loss\n",
    "        ce_loss = (criterion(logits1, labels) + criterion(logits2, labels)) / 2\n",
    "        \n",
    "        # KL divergence loss (R-Drop)\n",
    "        kl_loss = compute_kl_loss(logits1, logits2)\n",
    "        \n",
    "        # Total loss\n",
    "        loss = ce_loss + rdrop_alpha * kl_loss\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient clipping - lebih ketat\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "        \n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        # Use average logits for prediction\n",
    "        avg_logits = (logits1 + logits2) / 2\n",
    "        preds = torch.argmax(avg_logits, dim=1).cpu().numpy()\n",
    "        all_preds.extend(preds)\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        progress_bar.set_postfix({'loss': f'{loss.item():.4f}', 'ce': f'{ce_loss.item():.4f}'})\n",
    "    \n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "    \n",
    "    return avg_loss, accuracy, f1\n",
    "\n",
    "\n",
    "def evaluate(model, dataloader, criterion, device):\n",
    "    \"\"\"Evaluasi model dengan Monte Carlo Dropout untuk uncertainty estimation\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    all_probs = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "            \n",
    "            logits = model(input_ids, attention_mask)\n",
    "            loss = criterion(logits, labels)\n",
    "            \n",
    "            probs = F.softmax(logits, dim=1)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            preds = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "            all_preds.extend(preds)\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_probs.extend(probs.cpu().numpy())\n",
    "    \n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "    \n",
    "    return avg_loss, accuracy, f1, all_preds, all_labels\n",
    "\n",
    "\n",
    "class EarlyStopping:\n",
    "    \"\"\"Early stopping yang memonitor gap antara train dan val\"\"\"\n",
    "    \n",
    "    def __init__(self, patience=7, min_delta=0.001, mode='max', max_gap=0.08):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.mode = mode\n",
    "        self.max_gap = max_gap  # Maximum allowed train-val gap\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.best_model = None\n",
    "        self.best_gap = float('inf')\n",
    "    \n",
    "    def __call__(self, score, model, train_score=None):\n",
    "        # Check if overfitting (gap too large)\n",
    "        if train_score is not None:\n",
    "            gap = train_score - score\n",
    "            if gap > self.max_gap:\n",
    "                print(f'   ‚ö†Ô∏è Gap {gap:.4f} > {self.max_gap} - potential overfitting')\n",
    "        \n",
    "        if self.mode == 'min':\n",
    "            is_improvement = self.best_score is None or score < self.best_score - self.min_delta\n",
    "        else:\n",
    "            is_improvement = self.best_score is None or score > self.best_score + self.min_delta\n",
    "        \n",
    "        if is_improvement:\n",
    "            self.best_score = score\n",
    "            self.best_model = copy.deepcopy(model.state_dict())\n",
    "            self.counter = 0\n",
    "            if train_score is not None:\n",
    "                self.best_gap = train_score - score\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        \n",
    "        return self.early_stop\n",
    "\n",
    "print('‚úì Training functions with R-Drop regularization defined')\n",
    "print('  R-Drop helps reduce overfitting by enforcing consistency between dropout samples')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a3e23c",
   "metadata": {},
   "source": [
    "## üöÄ 9. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16cda797",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training history\n",
    "history = {\n",
    "    'train_loss': [], 'train_acc': [], 'train_f1': [],\n",
    "    'val_loss': [], 'val_acc': [], 'val_f1': [],\n",
    "    'gap': []  # Track gap untuk monitoring\n",
    "}\n",
    "\n",
    "# Early stopping - monitor validation F1 dengan gap monitoring\n",
    "early_stopping = EarlyStopping(\n",
    "    patience=CONFIG['early_stopping_patience'], \n",
    "    mode='max',\n",
    "    max_gap=0.08  # Stop jika gap > 8%\n",
    ")\n",
    "\n",
    "print('=' * 60)\n",
    "print('üöÄ TRAINING STARTED - ULTRA OPTIMIZED')\n",
    "print('=' * 60)\n",
    "print(f'Epochs: {CONFIG[\"epochs\"]} | Early Stopping Patience: {CONFIG[\"early_stopping_patience\"]}')\n",
    "print(f'Learning Rate: {CONFIG[\"learning_rate\"]} | Batch Size: {CONFIG[\"batch_size\"]}')\n",
    "print(f'Frozen Layers: {CONFIG[\"freeze_layers\"]}/12 | Dropout: {CONFIG[\"dropout_rate\"]}')\n",
    "print(f'R-Drop Alpha: {CONFIG[\"rdrop_alpha\"]} | Weight Decay: {CONFIG[\"weight_decay\"]}')\n",
    "print('-' * 60)\n",
    "\n",
    "best_val_f1 = 0\n",
    "best_epoch = 0\n",
    "best_gap = float('inf')\n",
    "\n",
    "for epoch in range(CONFIG['epochs']):\n",
    "    print(f'\\nüìç Epoch {epoch + 1}/{CONFIG[\"epochs\"]}')\n",
    "    \n",
    "    # Train dengan R-Drop\n",
    "    train_loss, train_acc, train_f1 = train_epoch_rdrop(\n",
    "        model, train_loader, criterion, optimizer, scheduler, \n",
    "        device, CONFIG['max_grad_norm'], CONFIG['rdrop_alpha']\n",
    "    )\n",
    "    \n",
    "    # Validate\n",
    "    val_loss, val_acc, val_f1, _, _ = evaluate(\n",
    "        model, val_loader, criterion, device\n",
    "    )\n",
    "    \n",
    "    # Calculate gap\n",
    "    gap = train_acc - val_acc\n",
    "    \n",
    "    # Save history\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['train_acc'].append(train_acc)\n",
    "    history['train_f1'].append(train_f1)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['val_acc'].append(val_acc)\n",
    "    history['val_f1'].append(val_f1)\n",
    "    history['gap'].append(gap)\n",
    "    \n",
    "    # Print metrics\n",
    "    print(f'  Train - Loss: {train_loss:.4f} | Acc: {train_acc:.4f} | F1: {train_f1:.4f}')\n",
    "    print(f'  Val   - Loss: {val_loss:.4f} | Acc: {val_acc:.4f} | F1: {val_f1:.4f}')\n",
    "    \n",
    "    # Track best model - prioritaskan model dengan gap kecil DAN F1 tinggi\n",
    "    if val_f1 > best_val_f1 and gap < 0.10:\n",
    "        best_val_f1 = val_f1\n",
    "        best_epoch = epoch + 1\n",
    "        best_gap = gap\n",
    "        print(f'  ‚≠ê New best! F1: {val_f1:.4f}, Gap: {gap:.4f}')\n",
    "    elif val_f1 > best_val_f1:\n",
    "        print(f'  üìà Higher F1 but gap too large ({gap:.4f})')\n",
    "    \n",
    "    # Check overfitting status\n",
    "    print(f'  üìä Train-Val Gap: {gap*100:.2f}%', end='')\n",
    "    if gap > 0.10:\n",
    "        print(' ‚ö†Ô∏è OVERFITTING!')\n",
    "    elif gap > 0.05:\n",
    "        print(' ‚ö° Slight gap')\n",
    "    else:\n",
    "        print(' ‚úÖ Good generalization')\n",
    "    \n",
    "    # Early stopping check\n",
    "    if early_stopping(val_f1, model, train_acc):\n",
    "        print(f'\\nüõë Early stopping triggered at epoch {epoch + 1}')\n",
    "        print(f'   Best F1 was at epoch {best_epoch} with gap {best_gap*100:.2f}%')\n",
    "        break\n",
    "\n",
    "# Load best model\n",
    "if early_stopping.best_model is not None:\n",
    "    model.load_state_dict(early_stopping.best_model)\n",
    "    print(f'\\n‚úì Loaded best model from epoch {best_epoch}')\n",
    "    print(f'  Best Val F1: {best_val_f1:.4f}')\n",
    "    print(f'  Best Gap: {best_gap*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "032317be",
   "metadata": {},
   "source": [
    "## üìà 10. Training Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e739f346",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "epochs_range = range(1, len(history['train_loss']) + 1)\n",
    "\n",
    "# Loss\n",
    "axes[0].plot(epochs_range, history['train_loss'], 'b-', label='Train Loss', marker='o')\n",
    "axes[0].plot(epochs_range, history['val_loss'], 'r-', label='Val Loss', marker='s')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('Training & Validation Loss')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Accuracy\n",
    "axes[1].plot(epochs_range, history['train_acc'], 'b-', label='Train Acc', marker='o')\n",
    "axes[1].plot(epochs_range, history['val_acc'], 'r-', label='Val Acc', marker='s')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Accuracy')\n",
    "axes[1].set_title('Training & Validation Accuracy')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# F1 Score\n",
    "axes[2].plot(epochs_range, history['train_f1'], 'b-', label='Train F1', marker='o')\n",
    "axes[2].plot(epochs_range, history['val_f1'], 'r-', label='Val F1', marker='s')\n",
    "axes[2].set_xlabel('Epoch')\n",
    "axes[2].set_ylabel('F1 Score')\n",
    "axes[2].set_title('Training & Validation F1 Score')\n",
    "axes[2].legend()\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('training_history.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Check for overfitting\n",
    "final_gap = history['train_acc'][-1] - history['val_acc'][-1]\n",
    "print(f'\\nüìä Overfitting Analysis:')\n",
    "print(f'  Final Train Accuracy: {history[\"train_acc\"][-1]:.4f}')\n",
    "print(f'  Final Val Accuracy:   {history[\"val_acc\"][-1]:.4f}')\n",
    "print(f'  Gap (Train - Val):    {final_gap:.4f}')\n",
    "\n",
    "if final_gap < 0.03:\n",
    "    print('  ‚úÖ Model is NOT overfitting (gap < 3%)')\n",
    "elif final_gap < 0.05:\n",
    "    print('  ‚ö†Ô∏è  Slight overfitting (gap 3-5%)')\n",
    "else:\n",
    "    print('  ‚ùå Model is overfitting (gap > 5%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c53cad3",
   "metadata": {},
   "source": [
    "## üß™ 11. Test Set Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f0ff16f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test set\n",
    "print('=' * 60)\n",
    "print('üß™ TEST SET EVALUATION')\n",
    "print('=' * 60)\n",
    "\n",
    "test_loss, test_acc, test_f1, test_preds, test_labels = evaluate(\n",
    "    model, test_loader, criterion, device\n",
    ")\n",
    "\n",
    "print(f'\\nüìä Test Results:')\n",
    "print(f'  Loss:     {test_loss:.4f}')\n",
    "print(f'  Accuracy: {test_acc:.4f} ({test_acc*100:.2f}%)')\n",
    "print(f'  F1 Score: {test_f1:.4f}')\n",
    "\n",
    "# Classification report\n",
    "print('\\n' + '=' * 60)\n",
    "print('üìã CLASSIFICATION REPORT')\n",
    "print('=' * 60)\n",
    "print(classification_report(test_labels, test_preds, target_names=LABEL_NAMES, digits=4))\n",
    "\n",
    "# Per-class metrics\n",
    "precision, recall, f1, support = precision_recall_fscore_support(\n",
    "    test_labels, test_preds, average=None, labels=[0, 1, 2]\n",
    ")\n",
    "\n",
    "print('\\nüìä Per-Class Metrics:')\n",
    "for i, label in enumerate(LABEL_NAMES):\n",
    "    print(f'  {label.upper():10} - P: {precision[i]:.4f} | R: {recall[i]:.4f} | F1: {f1[i]:.4f} | N: {support[i]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df405746",
   "metadata": {},
   "source": [
    "## üî• 12. Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "024ddfdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "cm = confusion_matrix(test_labels, test_preds)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Absolute numbers\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=LABEL_NAMES, yticklabels=LABEL_NAMES, ax=axes[0])\n",
    "axes[0].set_xlabel('Predicted')\n",
    "axes[0].set_ylabel('Actual')\n",
    "axes[0].set_title('Confusion Matrix (Counts)')\n",
    "\n",
    "# Normalized (percentages)\n",
    "cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] * 100\n",
    "sns.heatmap(cm_normalized, annot=True, fmt='.1f', cmap='Blues',\n",
    "            xticklabels=LABEL_NAMES, yticklabels=LABEL_NAMES, ax=axes[1])\n",
    "axes[1].set_xlabel('Predicted')\n",
    "axes[1].set_ylabel('Actual')\n",
    "axes[1].set_title('Confusion Matrix (Percentages %)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('confusion_matrix.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Analysis\n",
    "print('\\nüìä Confusion Matrix Analysis:')\n",
    "for i, label in enumerate(LABEL_NAMES):\n",
    "    correct = cm[i, i]\n",
    "    total = cm[i].sum()\n",
    "    print(f'  {label.upper():10} - Correct: {correct}/{total} ({correct/total*100:.1f}%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13cbe767",
   "metadata": {},
   "source": [
    "## üíæ 13. Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "748403d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create models directory di Kaggle output\n",
    "OUTPUT_DIR = '/kaggle/working/models'\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Save model\n",
    "model_path = f'{OUTPUT_DIR}/indobert_sentiment_3class.pt'\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'config': CONFIG,\n",
    "    'label_map': LABEL_MAP,\n",
    "    'label_names': LABEL_NAMES,\n",
    "    'test_accuracy': test_acc,\n",
    "    'test_f1': test_f1,\n",
    "    'history': history,\n",
    "}, model_path)\n",
    "print(f'‚úì Model saved to: {model_path}')\n",
    "\n",
    "# Save training history\n",
    "history_path = f'{OUTPUT_DIR}/training_history.json'\n",
    "with open(history_path, 'w') as f:\n",
    "    json.dump(history, f, indent=2)\n",
    "print(f'‚úì History saved to: {history_path}')\n",
    "\n",
    "# Save tokenizer\n",
    "tokenizer.save_pretrained(f'{OUTPUT_DIR}/tokenizer')\n",
    "print(f'‚úì Tokenizer saved to: {OUTPUT_DIR}/tokenizer/')\n",
    "\n",
    "# List saved files\n",
    "print(f'\\nüìÅ Saved files:')\n",
    "for f in os.listdir(OUTPUT_DIR):\n",
    "    filepath = os.path.join(OUTPUT_DIR, f)\n",
    "    if os.path.isfile(filepath):\n",
    "        size = os.path.getsize(filepath) / (1024*1024)\n",
    "        print(f'   ‚Ä¢ {f} ({size:.2f} MB)')\n",
    "    else:\n",
    "        print(f'   ‚Ä¢ {f}/')\n",
    "\n",
    "print(f'\\n‚úÖ Files saved in /kaggle/working/models/')\n",
    "print('üí° Download dari tab \"Output\" setelah notebook selesai')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e057cda6",
   "metadata": {},
   "source": [
    "## üîÆ 14. Inference Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "904a4469",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentiment(text, model, tokenizer, device, label_names):\n",
    "    \"\"\"Prediksi sentiment untuk satu teks\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    encoding = tokenizer.encode_plus(\n",
    "        text,\n",
    "        add_special_tokens=True,\n",
    "        max_length=128,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_attention_mask=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    \n",
    "    input_ids = encoding['input_ids'].to(device)\n",
    "    attention_mask = encoding['attention_mask'].to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        logits = model(input_ids, attention_mask)\n",
    "        probs = torch.softmax(logits, dim=1)\n",
    "        pred = torch.argmax(probs, dim=1).item()\n",
    "    \n",
    "    return {\n",
    "        'sentiment': label_names[pred],\n",
    "        'confidence': probs[0][pred].item(),\n",
    "        'probabilities': {\n",
    "            label_names[i]: probs[0][i].item() \n",
    "            for i in range(len(label_names))\n",
    "        }\n",
    "    }\n",
    "\n",
    "# Test dengan contoh\n",
    "test_reviews = [\n",
    "    \"Aplikasi gojek sangat membantu, driver ramah dan cepat\",\n",
    "    \"Driver nya lama banget, udah nunggu 1 jam gak datang datang\",\n",
    "    \"Biasa aja sih aplikasinya\",\n",
    "    \"Pelayanan buruk, driver tidak sopan, tidak akan pakai lagi\",\n",
    "    \"Mantap, makanan sampai dengan selamat dan masih hangat\",\n",
    "    \"Ongkirnya agak mahal tapi ya lumayan lah\"\n",
    "]\n",
    "\n",
    "print('=' * 60)\n",
    "print('üîÆ INFERENCE DEMO')\n",
    "print('=' * 60)\n",
    "\n",
    "for review in test_reviews:\n",
    "    result = predict_sentiment(review, model, tokenizer, device, LABEL_NAMES)\n",
    "    emoji = {'negative': 'üò†', 'neutral': 'üòê', 'positive': 'üòä'}[result['sentiment']]\n",
    "    print(f'\\nüìù \"{review[:50]}...\"' if len(review) > 50 else f'\\nüìù \"{review}\"')\n",
    "    print(f'   {emoji} Sentiment: {result[\"sentiment\"].upper()} (Confidence: {result[\"confidence\"]*100:.1f}%)')\n",
    "    print(f'   Probabilities: Neg={result[\"probabilities\"][\"negative\"]*100:.1f}% | '\n",
    "          f'Neu={result[\"probabilities\"][\"neutral\"]*100:.1f}% | '\n",
    "          f'Pos={result[\"probabilities\"][\"positive\"]*100:.1f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d417e4f",
   "metadata": {},
   "source": [
    "## üìä 15. Final Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9dca419",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary\n",
    "print('=' * 60)\n",
    "print('üìä FINAL SUMMARY')\n",
    "print('=' * 60)\n",
    "\n",
    "# Calculate metrics\n",
    "final_gap = history['train_acc'][-1] - history['val_acc'][-1]\n",
    "best_val_acc = max(history['val_acc'])\n",
    "min_gap = min(history['gap'])\n",
    "avg_gap = sum(history['gap']) / len(history['gap'])\n",
    "\n",
    "print(f'''\n",
    "üéØ MODEL PERFORMANCE:\n",
    "   ‚Ä¢ Test Accuracy: {test_acc*100:.2f}%\n",
    "   ‚Ä¢ Test F1 Score: {test_f1*100:.2f}%\n",
    "   ‚Ä¢ Best Validation Accuracy: {best_val_acc*100:.2f}%\n",
    "\n",
    "üìà OVERFITTING CHECK:\n",
    "   ‚Ä¢ Final Train-Val Gap: {final_gap*100:.2f}%\n",
    "   ‚Ä¢ Best Gap: {min_gap*100:.2f}%\n",
    "   ‚Ä¢ Average Gap: {avg_gap*100:.2f}%\n",
    "   ‚Ä¢ Status: {\"‚úÖ Good Generalization\" if final_gap < 0.05 else \"‚ö†Ô∏è Check Gap\" if final_gap < 0.10 else \"‚ùå Overfitting\"}\n",
    "\n",
    "‚öôÔ∏è ULTRA ANTI-OVERFITTING TECHNIQUES:\n",
    "   ‚Ä¢ Layer Freezing: {CONFIG['freeze_layers']}/12 layers frozen\n",
    "   ‚Ä¢ Dropout Rate: {CONFIG['dropout_rate']}\n",
    "   ‚Ä¢ Attention Dropout: {CONFIG['attention_dropout']}\n",
    "   ‚Ä¢ Label Smoothing: {CONFIG['label_smoothing']}\n",
    "   ‚Ä¢ Weight Decay: {CONFIG['weight_decay']}\n",
    "   ‚Ä¢ R-Drop Alpha: {CONFIG['rdrop_alpha']}\n",
    "   ‚Ä¢ Learning Rate: {CONFIG['learning_rate']} (very small)\n",
    "   ‚Ä¢ Gradient Clipping: {CONFIG['max_grad_norm']}\n",
    "   ‚Ä¢ Early Stopping: patience={CONFIG['early_stopping_patience']}\n",
    "   ‚Ä¢ Data Augmentation: word dropout, swap, shuffle\n",
    "\n",
    "üíæ SAVED FILES:\n",
    "   ‚Ä¢ Model: /kaggle/working/models/indobert_sentiment_3class.pt\n",
    "   ‚Ä¢ Tokenizer: /kaggle/working/models/tokenizer/\n",
    "   ‚Ä¢ History: /kaggle/working/models/training_history.json\n",
    "''')\n",
    "\n",
    "# Recommendation based on results\n",
    "if test_acc >= 0.75 and final_gap < 0.05:\n",
    "    print('üéâ EXCELLENT! Model has good accuracy and generalization!')\n",
    "elif test_acc >= 0.75:\n",
    "    print('‚ö†Ô∏è Good accuracy but check overfitting. Consider more regularization.')\n",
    "elif final_gap < 0.05:\n",
    "    print('‚úÖ Good generalization but accuracy could be improved. Try unfreezing more layers.')\n",
    "else:\n",
    "    print('‚ùå Both accuracy and generalization need improvement.')\n",
    "\n",
    "print('=' * 60)\n",
    "print('‚úÖ Training completed!')\n",
    "print('=' * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3575efe7",
   "metadata": {},
   "source": [
    "## üì• 16. Copy to Kaggle Output (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df3ac3f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zip model folder untuk download yang lebih mudah\n",
    "import shutil\n",
    "\n",
    "zip_path = '/kaggle/working/model_sentiment_3class'\n",
    "shutil.make_archive(zip_path, 'zip', '/kaggle/working/models')\n",
    "\n",
    "print('‚úì Model di-zip ke: /kaggle/working/model_sentiment_3class.zip')\n",
    "print('\\nüì• Cara download:')\n",
    "print('   1. Setelah notebook selesai, klik tab \"Output\" di kanan')\n",
    "print('   2. Download file model_sentiment_3class.zip')\n",
    "print('   3. Extract untuk mendapatkan model, tokenizer, dan history')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
