{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb2527d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# CELL 1: SETUP & GPU CHECK\n",
    "# ============================================\n",
    "\n",
    "!pip install transformers accelerate -q\n",
    "\n",
    "import torch\n",
    "import os\n",
    "\n",
    "print('='*60)\n",
    "print('üñ•Ô∏è  SYSTEM INFO')\n",
    "print('='*60)\n",
    "print(f'PyTorch: {torch.__version__}')\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    n_gpu = torch.cuda.device_count()\n",
    "    print(f'‚úÖ GPU Available: {n_gpu} GPU(s)')\n",
    "    for i in range(n_gpu):\n",
    "        gpu_name = torch.cuda.get_device_name(i)\n",
    "        gpu_mem = torch.cuda.get_device_properties(i).total_memory / 1024**3\n",
    "        print(f'   GPU {i}: {gpu_name} ({gpu_mem:.1f} GB)')\n",
    "else:\n",
    "    print('‚ö†Ô∏è GPU not available!')\n",
    "\n",
    "# List input files\n",
    "print('\\nüìÅ Input files:')\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(f'   {os.path.join(dirname, filename)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c5e57a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# CELL 2: IMPORTS & SEED\n",
    "# ============================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "from transformers import BertTokenizer, BertModel, get_linear_schedule_with_warmup\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_recall_fscore_support,\n",
    "    classification_report, confusion_matrix, f1_score\n",
    ")\n",
    "import random\n",
    "import copy\n",
    "import json\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Reproducibility\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "# Device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'üéÆ Using device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed78c90c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# CELL 3: LOAD DATA\n",
    "# ============================================\n",
    "\n",
    "# Auto-detect data file\n",
    "DATA_PATH = None\n",
    "search_patterns = ['gojek_reviews_3class_clean', 'gojek_reviews_3class', 'gojek']\n",
    "\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for pattern in search_patterns:\n",
    "        for filename in filenames:\n",
    "            if pattern in filename and filename.endswith('.csv'):\n",
    "                DATA_PATH = os.path.join(dirname, filename)\n",
    "                break\n",
    "        if DATA_PATH:\n",
    "            break\n",
    "    if DATA_PATH:\n",
    "        break\n",
    "\n",
    "if DATA_PATH:\n",
    "    print(f'‚úÖ Found: {DATA_PATH}')\n",
    "    df = pd.read_csv(DATA_PATH)\n",
    "else:\n",
    "    print('‚ùå Data not found! Available files:')\n",
    "    for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "        for f in filenames:\n",
    "            print(f'   {os.path.join(dirname, f)}')\n",
    "    raise FileNotFoundError('Please upload gojek_reviews_3class_clean.csv')\n",
    "\n",
    "# Data overview\n",
    "print('\\n' + '='*60)\n",
    "print('üìä DATA OVERVIEW')\n",
    "print('='*60)\n",
    "print(f'Total samples: {len(df):,}')\n",
    "print(f'Columns: {df.columns.tolist()}')\n",
    "\n",
    "# Check required columns\n",
    "text_col = 'content_clean' if 'content_clean' in df.columns else 'content'\n",
    "print(f'\\nText column: {text_col}')\n",
    "print(f'\\nüìà Sentiment Distribution:')\n",
    "print(df['sentiment'].value_counts())\n",
    "\n",
    "# Visualize\n",
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "colors = {'negative': '#e74c3c', 'neutral': '#f39c12', 'positive': '#27ae60'}\n",
    "counts = df['sentiment'].value_counts()\n",
    "bars = ax.bar(counts.index, counts.values, color=[colors.get(s, '#3498db') for s in counts.index])\n",
    "ax.set_title('Sentiment Distribution', fontsize=14, fontweight='bold')\n",
    "ax.set_ylabel('Count')\n",
    "for bar, count in zip(bars, counts.values):\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 100, \n",
    "            f'{count:,}', ha='center', fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fafaa3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# CELL 4: CONFIGURATION - OPTIMIZED v2\n",
    "# ============================================\n",
    "# PERBAIKAN: Lebih banyak layer trainable, regularisasi lebih kuat\n",
    "\n",
    "# Label mapping\n",
    "LABEL_MAP = {'negative': 0, 'neutral': 1, 'positive': 2}\n",
    "LABEL_NAMES = ['negative', 'neutral', 'positive']\n",
    "NUM_CLASSES = 3\n",
    "\n",
    "# OPTIMIZED CONFIG v2 - Fix overfitting & improve accuracy\n",
    "CONFIG = {\n",
    "    # Model\n",
    "    'model_name': 'indobenchmark/indobert-base-p1',\n",
    "    'max_length': 128,\n",
    "    'num_classes': NUM_CLASSES,\n",
    "    \n",
    "    # Training - ADJUSTED\n",
    "    'batch_size': 16,  # Smaller batch = more updates = better generalization\n",
    "    'epochs': 15,\n",
    "    'learning_rate': 1e-5,  # Lower LR for stability\n",
    "    \n",
    "    # Anti-Overfitting - STRENGTHENED\n",
    "    'dropout_rate': 0.5,  # Increased from 0.3\n",
    "    'hidden_dropout': 0.4,  # New: dropout for hidden layer\n",
    "    'weight_decay': 0.05,  # Increased from 0.01\n",
    "    'label_smoothing': 0.15,  # Increased from 0.1\n",
    "    'warmup_ratio': 0.2,  # Longer warmup\n",
    "    'max_grad_norm': 0.5,  # Tighter gradient clipping\n",
    "    'early_stopping_patience': 4,  # Stop earlier if no improvement\n",
    "    \n",
    "    # Layer Freezing - LESS FREEZING for better learning\n",
    "    'freeze_embeddings': True,\n",
    "    'freeze_layers': 4,  # CHANGED: Freeze only 0-3, train 4-11 (8 layers trainable)\n",
    "    \n",
    "    # R-Drop regularization - ADJUSTED\n",
    "    'use_rdrop': True,\n",
    "    'rdrop_alpha': 1.0,  # Increased for stronger regularization\n",
    "    \n",
    "    # Data augmentation - ENHANCED\n",
    "    'augment_train': True,\n",
    "    'word_dropout_prob': 0.15,  # Increased\n",
    "    'augment_prob': 0.7,  # Probability to augment each sample\n",
    "    \n",
    "    # New: Use hidden layer in classifier\n",
    "    'use_hidden_layer': True,\n",
    "    'hidden_size': 256,\n",
    "}\n",
    "\n",
    "print('='*60)\n",
    "print('‚öôÔ∏è  TRAINING CONFIGURATION v2 (OPTIMIZED)')\n",
    "print('='*60)\n",
    "print('üîß Key changes from v1:')\n",
    "print('   ‚Ä¢ freeze_layers: 8 ‚Üí 4 (more trainable)')\n",
    "print('   ‚Ä¢ dropout: 0.3 ‚Üí 0.5')\n",
    "print('   ‚Ä¢ weight_decay: 0.01 ‚Üí 0.05')\n",
    "print('   ‚Ä¢ learning_rate: 2e-5 ‚Üí 1e-5')\n",
    "print('   ‚Ä¢ batch_size: 32 ‚Üí 16')\n",
    "print('   ‚Ä¢ rdrop_alpha: 0.7 ‚Üí 1.0')\n",
    "print('-'*60)\n",
    "for k, v in CONFIG.items():\n",
    "    print(f'  {k}: {v}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c77272",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# CELL 5: PREPARE DATA SPLITS\n",
    "# ============================================\n",
    "\n",
    "# Add label column\n",
    "df['label'] = df['sentiment'].map(LABEL_MAP)\n",
    "\n",
    "# Stratified split: 80% train, 10% val, 10% test\n",
    "train_df, temp_df = train_test_split(\n",
    "    df, test_size=0.2, random_state=42, stratify=df['label']\n",
    ")\n",
    "val_df, test_df = train_test_split(\n",
    "    temp_df, test_size=0.5, random_state=42, stratify=temp_df['label']\n",
    ")\n",
    "\n",
    "print('='*60)\n",
    "print('üìÇ DATA SPLITS (Stratified)')\n",
    "print('='*60)\n",
    "print(f'Train: {len(train_df):,} ({len(train_df)/len(df)*100:.0f}%)')\n",
    "print(f'Val:   {len(val_df):,} ({len(val_df)/len(df)*100:.0f}%)')\n",
    "print(f'Test:  {len(test_df):,} ({len(test_df)/len(df)*100:.0f}%)')\n",
    "\n",
    "print(f'\\nüìä Distribution per split:')\n",
    "for name, split_df in [('Train', train_df), ('Val', val_df), ('Test', test_df)]:\n",
    "    dist = split_df['sentiment'].value_counts()\n",
    "    print(f'  {name}: {dict(dist)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb53cb04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# CELL 6: DATASET CLASS - ENHANCED AUGMENTATION\n",
    "# ============================================\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(CONFIG['model_name'])\n",
    "print(f'‚úÖ Tokenizer loaded: {CONFIG[\"model_name\"]}')\n",
    "\n",
    "class SentimentDataset(Dataset):\n",
    "    \"\"\"Dataset with ENHANCED text augmentation for better generalization\"\"\"\n",
    "    \n",
    "    def __init__(self, texts, labels, tokenizer, max_length=128, \n",
    "                 augment=False, word_dropout_prob=0.15, augment_prob=0.7):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.augment = augment\n",
    "        self.word_dropout_prob = word_dropout_prob\n",
    "        self.augment_prob = augment_prob\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def _augment_text(self, text):\n",
    "        \"\"\"Enhanced augmentation with multiple techniques\"\"\"\n",
    "        if not self.augment or random.random() > self.augment_prob:\n",
    "            return text\n",
    "        \n",
    "        words = str(text).split()\n",
    "        if len(words) <= 4:\n",
    "            return text\n",
    "        \n",
    "        # Apply multiple augmentation techniques\n",
    "        aug_type = random.random()\n",
    "        \n",
    "        if aug_type < 0.25:\n",
    "            # Word dropout - remove random words\n",
    "            words = [w for w in words if random.random() > self.word_dropout_prob]\n",
    "        elif aug_type < 0.45:\n",
    "            # Word swap - swap adjacent words\n",
    "            if len(words) > 2:\n",
    "                idx = random.randint(0, len(words) - 2)\n",
    "                words[idx], words[idx + 1] = words[idx + 1], words[idx]\n",
    "        elif aug_type < 0.60:\n",
    "            # Random deletion - remove 1-2 words\n",
    "            if len(words) > 5:\n",
    "                for _ in range(random.randint(1, 2)):\n",
    "                    if len(words) > 4:\n",
    "                        del_idx = random.randint(1, len(words) - 2)\n",
    "                        words.pop(del_idx)\n",
    "        elif aug_type < 0.75:\n",
    "            # Shuffle middle portion\n",
    "            if len(words) > 5:\n",
    "                mid_start = len(words) // 4\n",
    "                mid_end = 3 * len(words) // 4\n",
    "                middle = words[mid_start:mid_end]\n",
    "                random.shuffle(middle)\n",
    "                words = words[:mid_start] + middle + words[mid_end:]\n",
    "        elif aug_type < 0.90:\n",
    "            # Word duplication\n",
    "            if len(words) > 3:\n",
    "                dup_idx = random.randint(0, len(words) - 1)\n",
    "                words.insert(dup_idx, words[dup_idx])\n",
    "        # else: no augmentation (10%)\n",
    "        \n",
    "        return ' '.join(words) if words else text\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = self._augment_text(self.texts[idx])\n",
    "        \n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            str(text),\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'label': torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = SentimentDataset(\n",
    "    train_df[text_col].values, train_df['label'].values, tokenizer,\n",
    "    max_length=CONFIG['max_length'], augment=CONFIG['augment_train'],\n",
    "    word_dropout_prob=CONFIG['word_dropout_prob'],\n",
    "    augment_prob=CONFIG['augment_prob']\n",
    ")\n",
    "val_dataset = SentimentDataset(\n",
    "    val_df[text_col].values, val_df['label'].values, tokenizer,\n",
    "    max_length=CONFIG['max_length'], augment=False\n",
    ")\n",
    "test_dataset = SentimentDataset(\n",
    "    test_df[text_col].values, test_df['label'].values, tokenizer,\n",
    "    max_length=CONFIG['max_length'], augment=False\n",
    ")\n",
    "\n",
    "# Create dataloaders - smaller batch for better generalization\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, batch_size=CONFIG['batch_size'], \n",
    "    shuffle=True, num_workers=2, pin_memory=True, drop_last=True\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_dataset, batch_size=CONFIG['batch_size']*2, \n",
    "    shuffle=False, num_workers=2, pin_memory=True\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    test_dataset, batch_size=CONFIG['batch_size']*2, \n",
    "    shuffle=False, num_workers=2, pin_memory=True\n",
    ")\n",
    "\n",
    "print(f'\\n‚úÖ DataLoaders created:')\n",
    "print(f'  Train: {len(train_dataset):,} samples, {len(train_loader)} batches (batch_size={CONFIG[\"batch_size\"]})')\n",
    "print(f'  Val:   {len(val_dataset):,} samples, {len(val_loader)} batches')\n",
    "print(f'  Test:  {len(test_dataset):,} samples, {len(test_loader)} batches')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c9d192",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# CELL 7: MODEL ARCHITECTURE - IMPROVED\n",
    "# ============================================\n",
    "\n",
    "class IndoBERTClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    IndoBERT for Sentiment Classification - IMPROVED v2\n",
    "    - Less layer freezing for better learning\n",
    "    - Hidden layer for better representation\n",
    "    - Stronger dropout regularization\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model_name, num_classes, dropout_rate=0.5,\n",
    "                 hidden_dropout=0.4, freeze_embeddings=True, freeze_layers=4,\n",
    "                 use_hidden_layer=True, hidden_size=256):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.bert = BertModel.from_pretrained(model_name)\n",
    "        self.bert_hidden_size = self.bert.config.hidden_size\n",
    "        self.use_hidden_layer = use_hidden_layer\n",
    "        \n",
    "        # Freeze embeddings\n",
    "        if freeze_embeddings:\n",
    "            for param in self.bert.embeddings.parameters():\n",
    "                param.requires_grad = False\n",
    "        \n",
    "        # Freeze first N encoder layers\n",
    "        for i in range(freeze_layers):\n",
    "            for param in self.bert.encoder.layer[i].parameters():\n",
    "                param.requires_grad = False\n",
    "        \n",
    "        # Classifier head with optional hidden layer\n",
    "        self.dropout1 = nn.Dropout(dropout_rate)\n",
    "        \n",
    "        if use_hidden_layer:\n",
    "            self.hidden = nn.Linear(self.bert_hidden_size, hidden_size)\n",
    "            self.layer_norm = nn.LayerNorm(hidden_size)\n",
    "            self.dropout2 = nn.Dropout(hidden_dropout)\n",
    "            self.classifier = nn.Linear(hidden_size, num_classes)\n",
    "        else:\n",
    "            self.classifier = nn.Linear(self.bert_hidden_size, num_classes)\n",
    "        \n",
    "        # Initialize weights\n",
    "        self._init_weights()\n",
    "        \n",
    "        # Print info\n",
    "        trainable_layers = 12 - freeze_layers\n",
    "        print(f'‚úÖ Model initialized (IMPROVED v2)')\n",
    "        print(f'   Embeddings frozen: {freeze_embeddings}')\n",
    "        print(f'   Layers frozen: 0-{freeze_layers-1} ({freeze_layers} layers)')\n",
    "        print(f'   Layers trainable: {freeze_layers}-11 ({trainable_layers} layers)')\n",
    "        print(f'   Hidden layer: {use_hidden_layer} (size={hidden_size})')\n",
    "        print(f'   Dropout: {dropout_rate} (classifier), {hidden_dropout} (hidden)')\n",
    "    \n",
    "    def _init_weights(self):\n",
    "        \"\"\"Initialize classifier weights\"\"\"\n",
    "        if self.use_hidden_layer:\n",
    "            nn.init.xavier_uniform_(self.hidden.weight)\n",
    "            nn.init.zeros_(self.hidden.bias)\n",
    "        nn.init.xavier_uniform_(self.classifier.weight)\n",
    "        nn.init.zeros_(self.classifier.bias)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled = outputs.pooler_output\n",
    "        \n",
    "        x = self.dropout1(pooled)\n",
    "        \n",
    "        if self.use_hidden_layer:\n",
    "            x = self.hidden(x)\n",
    "            x = F.gelu(x)  # GELU activation\n",
    "            x = self.layer_norm(x)\n",
    "            x = self.dropout2(x)\n",
    "        \n",
    "        logits = self.classifier(x)\n",
    "        return logits\n",
    "\n",
    "# Initialize model with new config\n",
    "model = IndoBERTClassifier(\n",
    "    model_name=CONFIG['model_name'],\n",
    "    num_classes=CONFIG['num_classes'],\n",
    "    dropout_rate=CONFIG['dropout_rate'],\n",
    "    hidden_dropout=CONFIG['hidden_dropout'],\n",
    "    freeze_embeddings=CONFIG['freeze_embeddings'],\n",
    "    freeze_layers=CONFIG['freeze_layers'],\n",
    "    use_hidden_layer=CONFIG['use_hidden_layer'],\n",
    "    hidden_size=CONFIG['hidden_size']\n",
    ").to(device)\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'\\nüìä Parameters:')\n",
    "print(f'   Total: {total_params:,}')\n",
    "print(f'   Trainable: {trainable_params:,} ({trainable_params/total_params*100:.1f}%)')\n",
    "print(f'   Frozen: {total_params - trainable_params:,}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b84317f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# CELL 8: OPTIMIZER & SCHEDULER\n",
    "# ============================================\n",
    "\n",
    "# Loss with label smoothing\n",
    "criterion = nn.CrossEntropyLoss(label_smoothing=CONFIG['label_smoothing'])\n",
    "\n",
    "# Optimizer - only trainable parameters\n",
    "no_decay = ['bias', 'LayerNorm.weight']\n",
    "optimizer_params = [\n",
    "    {\n",
    "        'params': [p for n, p in model.named_parameters() \n",
    "                   if p.requires_grad and not any(nd in n for nd in no_decay)],\n",
    "        'weight_decay': CONFIG['weight_decay']\n",
    "    },\n",
    "    {\n",
    "        'params': [p for n, p in model.named_parameters() \n",
    "                   if p.requires_grad and any(nd in n for nd in no_decay)],\n",
    "        'weight_decay': 0.0\n",
    "    }\n",
    "]\n",
    "\n",
    "optimizer = AdamW(optimizer_params, lr=CONFIG['learning_rate'])\n",
    "\n",
    "# Scheduler with warmup\n",
    "total_steps = len(train_loader) * CONFIG['epochs']\n",
    "warmup_steps = int(total_steps * CONFIG['warmup_ratio'])\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer, num_warmup_steps=warmup_steps, num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "print('‚úÖ Optimizer & Scheduler configured')\n",
    "print(f'   LR: {CONFIG[\"learning_rate\"]}')\n",
    "print(f'   Weight Decay: {CONFIG[\"weight_decay\"]}')\n",
    "print(f'   Warmup Steps: {warmup_steps}')\n",
    "print(f'   Total Steps: {total_steps}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f6d5d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# CELL 9: TRAINING FUNCTIONS - IMPROVED\n",
    "# ============================================\n",
    "\n",
    "def compute_kl_loss(p, q):\n",
    "    \"\"\"KL divergence for R-Drop - symmetric\"\"\"\n",
    "    p_loss = F.kl_div(F.log_softmax(p, dim=-1), F.softmax(q, dim=-1), reduction='batchmean')\n",
    "    q_loss = F.kl_div(F.log_softmax(q, dim=-1), F.softmax(p, dim=-1), reduction='batchmean')\n",
    "    return (p_loss + q_loss) / 2\n",
    "\n",
    "def train_epoch(model, loader, criterion, optimizer, scheduler, device, \n",
    "                use_rdrop=True, rdrop_alpha=1.0, max_grad_norm=0.5):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_ce_loss = 0\n",
    "    total_kl_loss = 0\n",
    "    all_preds, all_labels = [], []\n",
    "    \n",
    "    pbar = tqdm(loader, desc='Training')\n",
    "    for batch in pbar:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        if use_rdrop:\n",
    "            # R-Drop: 2 forward passes with different dropout\n",
    "            logits1 = model(input_ids, attention_mask)\n",
    "            logits2 = model(input_ids, attention_mask)\n",
    "            \n",
    "            ce_loss = (criterion(logits1, labels) + criterion(logits2, labels)) / 2\n",
    "            kl_loss = compute_kl_loss(logits1, logits2)\n",
    "            loss = ce_loss + rdrop_alpha * kl_loss\n",
    "            \n",
    "            total_ce_loss += ce_loss.item()\n",
    "            total_kl_loss += kl_loss.item()\n",
    "            \n",
    "            logits = (logits1 + logits2) / 2\n",
    "        else:\n",
    "            logits = model(input_ids, attention_mask)\n",
    "            loss = criterion(logits, labels)\n",
    "            total_ce_loss += loss.item()\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient clipping - prevent exploding gradients\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "        \n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        preds = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "        all_preds.extend(preds)\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        pbar.set_postfix({\n",
    "            'loss': f'{loss.item():.4f}',\n",
    "            'ce': f'{ce_loss.item():.4f}' if use_rdrop else f'{loss.item():.4f}'\n",
    "        })\n",
    "    \n",
    "    avg_loss = total_loss / len(loader)\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "    \n",
    "    return avg_loss, accuracy, f1\n",
    "\n",
    "def evaluate(model, loader, criterion, device, mc_dropout=False, n_samples=5):\n",
    "    \"\"\"\n",
    "    Evaluate with optional Monte Carlo Dropout for uncertainty\n",
    "    \"\"\"\n",
    "    if mc_dropout:\n",
    "        model.train()  # Keep dropout active\n",
    "    else:\n",
    "        model.eval()\n",
    "    \n",
    "    total_loss = 0\n",
    "    all_preds, all_labels, all_probs = [], [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "            \n",
    "            if mc_dropout:\n",
    "                # Average over multiple dropout samples\n",
    "                logits_list = []\n",
    "                for _ in range(n_samples):\n",
    "                    logits = model(input_ids, attention_mask)\n",
    "                    logits_list.append(logits)\n",
    "                logits = torch.stack(logits_list).mean(dim=0)\n",
    "            else:\n",
    "                logits = model(input_ids, attention_mask)\n",
    "            \n",
    "            loss = criterion(logits, labels)\n",
    "            \n",
    "            probs = F.softmax(logits, dim=1)\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_probs.extend(probs.cpu().numpy())\n",
    "    \n",
    "    avg_loss = total_loss / len(loader)\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "    \n",
    "    return avg_loss, accuracy, f1, all_preds, all_labels, all_probs\n",
    "\n",
    "class EarlyStopping:\n",
    "    \"\"\"Early stopping with gap monitoring\"\"\"\n",
    "    \n",
    "    def __init__(self, patience=4, min_delta=0.001, max_gap=0.08):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.max_gap = max_gap  # Max allowed train-val gap\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.best_model = None\n",
    "        self.best_gap = None\n",
    "        self.early_stop = False\n",
    "    \n",
    "    def __call__(self, val_score, model, train_score=None):\n",
    "        gap = (train_score - val_score) if train_score else 0\n",
    "        \n",
    "        # Check if improved AND gap is acceptable\n",
    "        improved = False\n",
    "        if self.best_score is None:\n",
    "            improved = True\n",
    "        elif val_score > self.best_score + self.min_delta:\n",
    "            # Only accept if gap is not too large\n",
    "            if gap < self.max_gap or (self.best_gap and gap < self.best_gap):\n",
    "                improved = True\n",
    "        \n",
    "        if improved:\n",
    "            self.best_score = val_score\n",
    "            self.best_model = copy.deepcopy(model.state_dict())\n",
    "            self.best_gap = gap\n",
    "            self.counter = 0\n",
    "            return True\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "            return False\n",
    "\n",
    "print('‚úÖ Training functions defined (IMPROVED)')\n",
    "print('   ‚Ä¢ R-Drop with stronger alpha')\n",
    "print('   ‚Ä¢ Early stopping with gap monitoring')\n",
    "print('   ‚Ä¢ Optional MC Dropout for evaluation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba04c70c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# CELL 10: TRAINING LOOP - IMPROVED\n",
    "# ============================================\n",
    "\n",
    "history = {\n",
    "    'train_loss': [], 'train_acc': [], 'train_f1': [],\n",
    "    'val_loss': [], 'val_acc': [], 'val_f1': [],\n",
    "    'gap': [], 'lr': []\n",
    "}\n",
    "\n",
    "# Early stopping with gap monitoring\n",
    "early_stopping = EarlyStopping(\n",
    "    patience=CONFIG['early_stopping_patience'],\n",
    "    max_gap=0.08  # Stop if gap > 8% and no improvement\n",
    ")\n",
    "\n",
    "print('='*60)\n",
    "print('üöÄ TRAINING STARTED (OPTIMIZED v2)')\n",
    "print('='*60)\n",
    "print(f'Strategy: Less freezing + stronger regularization')\n",
    "print(f'Epochs: {CONFIG[\"epochs\"]} | Batch: {CONFIG[\"batch_size\"]} | LR: {CONFIG[\"learning_rate\"]}')\n",
    "print(f'Trainable layers: {12 - CONFIG[\"freeze_layers\"]}/12 | Dropout: {CONFIG[\"dropout_rate\"]}')\n",
    "print(f'R-Drop: Œ±={CONFIG[\"rdrop_alpha\"]} | Weight Decay: {CONFIG[\"weight_decay\"]}')\n",
    "print('-'*60)\n",
    "\n",
    "start_time = time.time()\n",
    "best_val_f1 = 0\n",
    "best_val_acc = 0\n",
    "best_epoch = 0\n",
    "best_gap = 1.0\n",
    "\n",
    "for epoch in range(CONFIG['epochs']):\n",
    "    print(f'\\nüìç Epoch {epoch + 1}/{CONFIG[\"epochs\"]}')\n",
    "    \n",
    "    # Get current learning rate\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "    \n",
    "    # Train\n",
    "    train_loss, train_acc, train_f1 = train_epoch(\n",
    "        model, train_loader, criterion, optimizer, scheduler, device,\n",
    "        use_rdrop=CONFIG['use_rdrop'], rdrop_alpha=CONFIG['rdrop_alpha'],\n",
    "        max_grad_norm=CONFIG['max_grad_norm']\n",
    "    )\n",
    "    \n",
    "    # Validate\n",
    "    val_loss, val_acc, val_f1, _, _, _ = evaluate(model, val_loader, criterion, device)\n",
    "    \n",
    "    # Calculate gap\n",
    "    gap = train_acc - val_acc\n",
    "    \n",
    "    # Save history\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['train_acc'].append(train_acc)\n",
    "    history['train_f1'].append(train_f1)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['val_acc'].append(val_acc)\n",
    "    history['val_f1'].append(val_f1)\n",
    "    history['gap'].append(gap)\n",
    "    history['lr'].append(current_lr)\n",
    "    \n",
    "    # Print metrics\n",
    "    print(f'  Train | Loss: {train_loss:.4f} | Acc: {train_acc:.4f} | F1: {train_f1:.4f}')\n",
    "    print(f'  Val   | Loss: {val_loss:.4f} | Acc: {val_acc:.4f} | F1: {val_f1:.4f}')\n",
    "    print(f'  Gap   | {gap*100:.2f}% | LR: {current_lr:.2e}', end='')\n",
    "    \n",
    "    # Gap status\n",
    "    if gap > 0.10:\n",
    "        print(' ‚ö†Ô∏è OVERFITTING!')\n",
    "    elif gap > 0.05:\n",
    "        print(' ‚ö° Watch gap')\n",
    "    else:\n",
    "        print(' ‚úÖ Good')\n",
    "    \n",
    "    # Early stopping check (considers both F1 and gap)\n",
    "    improved = early_stopping(val_f1, model, train_f1)\n",
    "    \n",
    "    if improved:\n",
    "        best_val_f1 = val_f1\n",
    "        best_val_acc = val_acc\n",
    "        best_epoch = epoch + 1\n",
    "        best_gap = gap\n",
    "        print(f'  ‚≠ê New best! F1: {val_f1:.4f}, Acc: {val_acc:.4f}, Gap: {gap*100:.1f}%')\n",
    "    else:\n",
    "        print(f'  üìä No improvement ({early_stopping.counter}/{early_stopping.patience})')\n",
    "    \n",
    "    if early_stopping.early_stop:\n",
    "        print(f'\\nüõë Early stopping at epoch {epoch + 1}')\n",
    "        print(f'   Best was epoch {best_epoch} with F1={best_val_f1:.4f}, Gap={best_gap*100:.1f}%')\n",
    "        break\n",
    "\n",
    "# Load best model\n",
    "if early_stopping.best_model is not None:\n",
    "    model.load_state_dict(early_stopping.best_model)\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "print(f'\\n‚úÖ Training completed in {total_time/60:.1f} minutes')\n",
    "print(f'   Best epoch: {best_epoch}')\n",
    "print(f'   Best val F1: {best_val_f1:.4f}')\n",
    "print(f'   Best val Acc: {best_val_acc:.4f}')\n",
    "print(f'   Best gap: {best_gap*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce85238",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# CELL 11: TRAINING VISUALIZATION\n",
    "# ============================================\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "epochs_range = range(1, len(history['train_loss']) + 1)\n",
    "\n",
    "# Loss\n",
    "axes[0].plot(epochs_range, history['train_loss'], 'b-o', label='Train', markersize=4)\n",
    "axes[0].plot(epochs_range, history['val_loss'], 'r-s', label='Val', markersize=4)\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('Loss', fontweight='bold')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Accuracy\n",
    "axes[1].plot(epochs_range, history['train_acc'], 'b-o', label='Train', markersize=4)\n",
    "axes[1].plot(epochs_range, history['val_acc'], 'r-s', label='Val', markersize=4)\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Accuracy')\n",
    "axes[1].set_title('Accuracy', fontweight='bold')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# F1 Score\n",
    "axes[2].plot(epochs_range, history['train_f1'], 'b-o', label='Train', markersize=4)\n",
    "axes[2].plot(epochs_range, history['val_f1'], 'r-s', label='Val', markersize=4)\n",
    "axes[2].axhline(y=best_val_f1, color='g', linestyle='--', label=f'Best: {best_val_f1:.4f}')\n",
    "axes[2].set_xlabel('Epoch')\n",
    "axes[2].set_ylabel('F1 Score')\n",
    "axes[2].set_title('F1 Score', fontweight='bold')\n",
    "axes[2].legend()\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('training_history_3class.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Gap analysis\n",
    "plt.figure(figsize=(8, 4))\n",
    "gaps = [t - v for t, v in zip(history['train_acc'], history['val_acc'])]\n",
    "colors = ['red' if g > 0.10 else 'orange' if g > 0.05 else 'green' for g in gaps]\n",
    "plt.bar(epochs_range, [g*100 for g in gaps], color=colors)\n",
    "plt.axhline(y=10, color='red', linestyle='--', label='Overfitting threshold (10%)')\n",
    "plt.axhline(y=5, color='orange', linestyle='--', label='Warning threshold (5%)')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Train-Val Gap (%)')\n",
    "plt.title('Overfitting Analysis', fontweight='bold')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig('overfitting_analysis_3class.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16be4a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# CELL 12: TEST EVALUATION\n",
    "# ============================================\n",
    "\n",
    "print('='*60)\n",
    "print('üß™ FINAL TEST EVALUATION')\n",
    "print('='*60)\n",
    "\n",
    "test_loss, test_acc, test_f1, test_preds, test_labels, test_probs = evaluate(\n",
    "    model, test_loader, criterion, device\n",
    ")\n",
    "\n",
    "print(f'\\nüìä Test Results:')\n",
    "print(f'   Loss: {test_loss:.4f}')\n",
    "print(f'   Accuracy: {test_acc:.4f} ({test_acc*100:.2f}%)')\n",
    "print(f'   F1 Score: {test_f1:.4f}')\n",
    "\n",
    "# Classification report\n",
    "print('\\nüìã Classification Report:')\n",
    "print(classification_report(test_labels, test_preds, target_names=LABEL_NAMES, digits=4))\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(test_labels, test_preds)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=LABEL_NAMES, yticklabels=LABEL_NAMES)\n",
    "plt.title('Confusion Matrix - Test Set', fontweight='bold')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.tight_layout()\n",
    "plt.savefig('confusion_matrix_3class.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Per-class accuracy\n",
    "print('\\nüìä Per-Class Accuracy:')\n",
    "for i, name in enumerate(LABEL_NAMES):\n",
    "    class_mask = np.array(test_labels) == i\n",
    "    class_acc = np.mean(np.array(test_preds)[class_mask] == i)\n",
    "    print(f'   {name}: {class_acc:.4f} ({class_acc*100:.2f}%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d07062e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# CELL 13: SAVE MODEL\n",
    "# ============================================\n",
    "\n",
    "# Save model\n",
    "save_path = '/kaggle/working/indobert_sentiment_3class.pt'\n",
    "\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'config': CONFIG,\n",
    "    'label_map': LABEL_MAP,\n",
    "    'label_names': LABEL_NAMES,\n",
    "    'metrics': {\n",
    "        'test_accuracy': test_acc,\n",
    "        'test_f1': test_f1,\n",
    "        'best_val_f1': best_val_f1,\n",
    "        'best_epoch': best_epoch\n",
    "    },\n",
    "    'history': history\n",
    "}, save_path)\n",
    "\n",
    "print(f'‚úÖ Model saved to: {save_path}')\n",
    "print(f'   File size: {os.path.getsize(save_path) / 1024 / 1024:.1f} MB')\n",
    "\n",
    "# Save training history\n",
    "history_path = '/kaggle/working/training_history_3class.json'\n",
    "with open(history_path, 'w') as f:\n",
    "    json.dump(history, f, indent=2)\n",
    "print(f'‚úÖ History saved to: {history_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72919812",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# CELL 14: INFERENCE FUNCTION\n",
    "# ============================================\n",
    "\n",
    "def predict_sentiment(text, model, tokenizer, device, label_names):\n",
    "    \"\"\"Predict sentiment for a single text\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    encoding = tokenizer.encode_plus(\n",
    "        str(text),\n",
    "        add_special_tokens=True,\n",
    "        max_length=128,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_attention_mask=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    \n",
    "    input_ids = encoding['input_ids'].to(device)\n",
    "    attention_mask = encoding['attention_mask'].to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        logits = model(input_ids, attention_mask)\n",
    "        probs = F.softmax(logits, dim=1)\n",
    "        pred = torch.argmax(logits, dim=1).item()\n",
    "    \n",
    "    return {\n",
    "        'sentiment': label_names[pred],\n",
    "        'confidence': probs[0][pred].item(),\n",
    "        'probabilities': {name: probs[0][i].item() for i, name in enumerate(label_names)}\n",
    "    }\n",
    "\n",
    "# Test predictions\n",
    "print('='*60)\n",
    "print('üîÆ SAMPLE PREDICTIONS')\n",
    "print('='*60)\n",
    "\n",
    "test_texts = [\n",
    "    \"Aplikasi sangat membantu, driver ramah dan cepat sampai\",\n",
    "    \"Biasa saja, tidak ada yang istimewa\",\n",
    "    \"Aplikasi error terus, driver tidak profesional, sangat mengecewakan\",\n",
    "    \"Gojek memudahkan transportasi sehari-hari saya\",\n",
    "    \"Harga mahal, promo tidak jelas\"\n",
    "]\n",
    "\n",
    "for text in test_texts:\n",
    "    result = predict_sentiment(text, model, tokenizer, device, LABEL_NAMES)\n",
    "    print(f'\\nüìù \"{text[:50]}...\"' if len(text) > 50 else f'\\nüìù \"{text}\"')\n",
    "    print(f'   Sentiment: {result[\"sentiment\"].upper()}')\n",
    "    print(f'   Confidence: {result[\"confidence\"]*100:.1f}%')\n",
    "    print(f'   Probs: {result[\"probabilities\"]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded47afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# CELL 15: FINAL SUMMARY\n",
    "# ============================================\n",
    "\n",
    "print('='*60)\n",
    "print('üìä TRAINING SUMMARY (OPTIMIZED v2)')\n",
    "print('='*60)\n",
    "\n",
    "summary = f\"\"\"\n",
    "üéØ MODEL: IndoBERT Sentiment 3-Class (Optimized v2)\n",
    "\n",
    "üìà METRICS:\n",
    "   ‚Ä¢ Test Accuracy: {test_acc*100:.2f}%\n",
    "   ‚Ä¢ Test F1 Score: {test_f1:.4f}\n",
    "   ‚Ä¢ Best Val F1:   {best_val_f1:.4f} (epoch {best_epoch})\n",
    "   ‚Ä¢ Best Val Acc:  {best_val_acc*100:.2f}%\n",
    "\n",
    "‚öôÔ∏è CONFIGURATION (Optimized):\n",
    "   ‚Ä¢ Model: {CONFIG['model_name']}\n",
    "   ‚Ä¢ Epochs trained: {len(history['train_loss'])}\n",
    "   ‚Ä¢ Batch size: {CONFIG['batch_size']}\n",
    "   ‚Ä¢ Learning rate: {CONFIG['learning_rate']}\n",
    "   ‚Ä¢ Frozen layers: {CONFIG['freeze_layers']}/12 (trainable: {12-CONFIG['freeze_layers']})\n",
    "   ‚Ä¢ Dropout: {CONFIG['dropout_rate']} + {CONFIG['hidden_dropout']}\n",
    "   ‚Ä¢ Hidden layer: {CONFIG['hidden_size']}\n",
    "   ‚Ä¢ R-Drop alpha: {CONFIG['rdrop_alpha']}\n",
    "   ‚Ä¢ Weight decay: {CONFIG['weight_decay']}\n",
    "   ‚Ä¢ Label smoothing: {CONFIG['label_smoothing']}\n",
    "\n",
    "üìÇ DATA:\n",
    "   ‚Ä¢ Train: {len(train_df):,}\n",
    "   ‚Ä¢ Val: {len(val_df):,}\n",
    "   ‚Ä¢ Test: {len(test_df):,}\n",
    "\n",
    "üíæ SAVED FILES:\n",
    "   ‚Ä¢ Model: indobert_sentiment_3class.pt\n",
    "   ‚Ä¢ History: training_history_3class.json\n",
    "   ‚Ä¢ Plots: training_history_3class.png, confusion_matrix_3class.png\n",
    "\"\"\"\n",
    "\n",
    "print(summary)\n",
    "\n",
    "# Overfitting check\n",
    "final_gap = history['train_acc'][-1] - history['val_acc'][-1]\n",
    "print(f'üîç OVERFITTING CHECK:')\n",
    "print(f'   Final train-val gap: {final_gap*100:.2f}%')\n",
    "print(f'   Best model gap: {best_gap*100:.2f}%')\n",
    "\n",
    "if best_gap < 0.05:\n",
    "    print('   ‚úÖ Excellent generalization!')\n",
    "elif best_gap < 0.08:\n",
    "    print('   ‚úÖ Good generalization')\n",
    "elif best_gap < 0.10:\n",
    "    print('   ‚ö° Acceptable gap')\n",
    "else:\n",
    "    print('   ‚ö†Ô∏è Some overfitting - consider more regularization')\n",
    "\n",
    "# Compare with baseline\n",
    "print(f'\\nüìä IMPROVEMENT vs BASELINE:')\n",
    "print(f'   Baseline: Acc=63.18%, F1=0.6354, Gap=20.70%')\n",
    "print(f'   Current:  Acc={test_acc*100:.2f}%, F1={test_f1:.4f}, Gap={best_gap*100:.2f}%')\n",
    "\n",
    "acc_change = (test_acc - 0.6318) * 100\n",
    "f1_change = test_f1 - 0.6354\n",
    "gap_change = 0.207 - best_gap\n",
    "\n",
    "print(f'   Accuracy change: {acc_change:+.2f}%')\n",
    "print(f'   F1 change: {f1_change:+.4f}')\n",
    "print(f'   Gap reduction: {gap_change*100:+.2f}%')\n",
    "\n",
    "print('\\n' + '='*60)\n",
    "print('‚úÖ Training complete! Download model from /kaggle/working/')\n",
    "print('='*60)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
