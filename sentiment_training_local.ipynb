{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "72c132ac",
   "metadata": {},
   "source": [
    "## üîß 1. Setup & Check GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d8b652",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install packages jika belum ada\n",
    "# !pip install torch transformers scikit-learn pandas numpy matplotlib seaborn tqdm pynvml\n",
    "\n",
    "import torch\n",
    "import sys\n",
    "import platform\n",
    "import subprocess\n",
    "\n",
    "print('=' * 60)\n",
    "print('üñ•Ô∏è  SYSTEM INFO')\n",
    "print('=' * 60)\n",
    "print(f'Python: {sys.version}')\n",
    "print(f'PyTorch: {torch.__version__}')\n",
    "print(f'Platform: {platform.system()} {platform.release()}')\n",
    "print()\n",
    "\n",
    "# Check GPU\n",
    "if torch.cuda.is_available():\n",
    "    print('‚úÖ CUDA is available!')\n",
    "    print(f'   GPU: {torch.cuda.get_device_name(0)}')\n",
    "    print(f'   CUDA Version: {torch.version.cuda}')\n",
    "    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "    print(f'   Total Memory: {gpu_memory:.1f} GB')\n",
    "    device = torch.device('cuda')\n",
    "    \n",
    "    # Check nvidia-smi untuk power info\n",
    "    try:\n",
    "        result = subprocess.run(['nvidia-smi', '--query-gpu=power.limit,power.draw', '--format=csv,noheader,nounits'], \n",
    "                               capture_output=True, text=True)\n",
    "        if result.returncode == 0:\n",
    "            power_info = result.stdout.strip().split(',')\n",
    "            print(f'   Power Limit: {float(power_info[0].strip()):.0f} W')\n",
    "            print(f'   Current Power: {float(power_info[1].strip()):.1f} W')\n",
    "    except:\n",
    "        print('   Power info: nvidia-smi not available')\n",
    "else:\n",
    "    print('‚ö†Ô∏è CUDA not available, using CPU')\n",
    "    print('   Training will be SLOW!')\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "print(f'\\nüéÆ Using device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f869daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from tqdm.auto import tqdm\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "from transformers import BertTokenizer, BertModel, get_linear_schedule_with_warmup\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_recall_fscore_support, \n",
    "    classification_report, confusion_matrix, f1_score\n",
    ")\n",
    "from sklearn.utils import resample\n",
    "import random\n",
    "import os\n",
    "import copy\n",
    "import json\n",
    "from datetime import datetime\n",
    "import time\n",
    "import subprocess\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# =====================================================\n",
    "# GPU MONITORING FUNCTIONS\n",
    "# =====================================================\n",
    "def get_gpu_memory_usage():\n",
    "    \"\"\"Get current GPU memory usage in MB\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        allocated = torch.cuda.memory_allocated(0) / 1024**2\n",
    "        reserved = torch.cuda.memory_reserved(0) / 1024**2\n",
    "        return allocated, reserved\n",
    "    return 0, 0\n",
    "\n",
    "def get_gpu_power():\n",
    "    \"\"\"Get current GPU power draw in Watts\"\"\"\n",
    "    try:\n",
    "        result = subprocess.run(\n",
    "            ['nvidia-smi', '--query-gpu=power.draw', '--format=csv,noheader,nounits'],\n",
    "            capture_output=True, text=True\n",
    "        )\n",
    "        if result.returncode == 0:\n",
    "            return float(result.stdout.strip())\n",
    "    except:\n",
    "        pass\n",
    "    return 0.0\n",
    "\n",
    "def get_gpu_stats():\n",
    "    \"\"\"Get comprehensive GPU stats\"\"\"\n",
    "    if not torch.cuda.is_available():\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        result = subprocess.run(\n",
    "            ['nvidia-smi', '--query-gpu=memory.used,memory.total,power.draw,temperature.gpu,utilization.gpu', \n",
    "             '--format=csv,noheader,nounits'],\n",
    "            capture_output=True, text=True\n",
    "        )\n",
    "        if result.returncode == 0:\n",
    "            values = [v.strip() for v in result.stdout.strip().split(',')]\n",
    "            return {\n",
    "                'memory_used_mb': float(values[0]),\n",
    "                'memory_total_mb': float(values[1]),\n",
    "                'power_draw_w': float(values[2]),\n",
    "                'temperature_c': float(values[3]),\n",
    "                'gpu_util_percent': float(values[4])\n",
    "            }\n",
    "    except:\n",
    "        pass\n",
    "    return None\n",
    "\n",
    "class GPUMonitor:\n",
    "    \"\"\"Monitor GPU usage during training\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "    \n",
    "    def reset(self):\n",
    "        self.memory_samples = []\n",
    "        self.power_samples = []\n",
    "        self.start_time = None\n",
    "        self.end_time = None\n",
    "        self.total_energy_wh = 0\n",
    "    \n",
    "    def start(self):\n",
    "        self.reset()\n",
    "        self.start_time = time.time()\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.reset_peak_memory_stats()\n",
    "    \n",
    "    def sample(self):\n",
    "        \"\"\"Take a sample of current GPU stats\"\"\"\n",
    "        if torch.cuda.is_available():\n",
    "            mem_allocated = torch.cuda.memory_allocated(0) / 1024**2\n",
    "            self.memory_samples.append(mem_allocated)\n",
    "            \n",
    "            power = get_gpu_power()\n",
    "            if power > 0:\n",
    "                self.power_samples.append(power)\n",
    "    \n",
    "    def stop(self):\n",
    "        self.end_time = time.time()\n",
    "        \n",
    "        # Calculate total energy consumption\n",
    "        if self.power_samples and self.start_time:\n",
    "            duration_hours = (self.end_time - self.start_time) / 3600\n",
    "            avg_power = np.mean(self.power_samples)\n",
    "            self.total_energy_wh = avg_power * duration_hours\n",
    "    \n",
    "    def get_summary(self):\n",
    "        \"\"\"Get summary of GPU usage\"\"\"\n",
    "        summary = {\n",
    "            'duration_seconds': 0,\n",
    "            'duration_formatted': '0:00:00',\n",
    "            'peak_memory_mb': 0,\n",
    "            'avg_memory_mb': 0,\n",
    "            'avg_power_w': 0,\n",
    "            'max_power_w': 0,\n",
    "            'total_energy_wh': 0,\n",
    "            'total_energy_kwh': 0\n",
    "        }\n",
    "        \n",
    "        if self.start_time and self.end_time:\n",
    "            duration = self.end_time - self.start_time\n",
    "            summary['duration_seconds'] = duration\n",
    "            hours, remainder = divmod(int(duration), 3600)\n",
    "            minutes, seconds = divmod(remainder, 60)\n",
    "            summary['duration_formatted'] = f'{hours}:{minutes:02d}:{seconds:02d}'\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            summary['peak_memory_mb'] = torch.cuda.max_memory_allocated(0) / 1024**2\n",
    "        \n",
    "        if self.memory_samples:\n",
    "            summary['avg_memory_mb'] = np.mean(self.memory_samples)\n",
    "        \n",
    "        if self.power_samples:\n",
    "            summary['avg_power_w'] = np.mean(self.power_samples)\n",
    "            summary['max_power_w'] = np.max(self.power_samples)\n",
    "            summary['total_energy_wh'] = self.total_energy_wh\n",
    "            summary['total_energy_kwh'] = self.total_energy_wh / 1000\n",
    "        \n",
    "        return summary\n",
    "\n",
    "# Initialize GPU monitor\n",
    "gpu_monitor = GPUMonitor()\n",
    "\n",
    "# Reproducibility\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(42)\n",
    "print('‚úì Libraries imported')\n",
    "print('‚úì GPU monitoring functions defined')\n",
    "print(f'‚úì Device: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2126b05d",
   "metadata": {},
   "source": [
    "## üìä 2. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9454df6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data dari folder data/\n",
    "DATA_PATH = 'data/gojek_reviews_final_augmented.csv'\n",
    "\n",
    "if not os.path.exists(DATA_PATH):\n",
    "    print(f'‚ùå File tidak ditemukan: {DATA_PATH}')\n",
    "    print('\\nüìÅ Files yang ada:')\n",
    "    if os.path.exists('data'):\n",
    "        for f in os.listdir('data'):\n",
    "            print(f'   - data/{f}')\n",
    "else:\n",
    "    df = pd.read_csv(DATA_PATH)\n",
    "    \n",
    "    print('=' * 60)\n",
    "    print('üìä DATA OVERVIEW')\n",
    "    print('=' * 60)\n",
    "    print(f'Total samples: {len(df):,}')\n",
    "    print(f'Columns: {df.columns.tolist()}')\n",
    "    print(f'\\nüìà Sentiment Distribution:')\n",
    "    print(df['sentiment'].value_counts())\n",
    "    \n",
    "    # Visualize\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(8, 4))\n",
    "    colors = {'negative': '#e74c3c', 'neutral': '#95a5a6', 'positive': '#2ecc71'}\n",
    "    counts = df['sentiment'].value_counts()\n",
    "    bars = ax.bar(counts.index, counts.values, color=[colors[s] for s in counts.index])\n",
    "    ax.set_title('Sentiment Distribution')\n",
    "    ax.set_ylabel('Count')\n",
    "    for bar, count in zip(bars, counts.values):\n",
    "        ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 50, \n",
    "                str(count), ha='center', fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be59c0e0",
   "metadata": {},
   "source": [
    "## ‚öñÔ∏è 3. Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "826ebe2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if data is already balanced\n",
    "counts = df['sentiment'].value_counts()\n",
    "min_count = counts.min()\n",
    "max_count = counts.max()\n",
    "\n",
    "if (max_count - min_count) / max_count < 0.1:\n",
    "    print('‚úì Data sudah balanced!')\n",
    "    df_balanced = df.copy()\n",
    "else:\n",
    "    print('‚ö†Ô∏è Melakukan undersampling...')\n",
    "    df_balanced = pd.DataFrame()\n",
    "    for sentiment in ['negative', 'neutral', 'positive']:\n",
    "        df_class = df[df['sentiment'] == sentiment]\n",
    "        df_sampled = resample(df_class, replace=False, n_samples=min_count, random_state=42)\n",
    "        df_balanced = pd.concat([df_balanced, df_sampled])\n",
    "    df_balanced = df_balanced.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "print(f'\\nüìä Data untuk training: {len(df_balanced):,} samples')\n",
    "print(df_balanced['sentiment'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b76450b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label mapping\n",
    "LABEL_MAP = {'negative': 0, 'neutral': 1, 'positive': 2}\n",
    "LABEL_NAMES = ['negative', 'neutral', 'positive']\n",
    "NUM_CLASSES = 3\n",
    "\n",
    "df_balanced['label'] = df_balanced['sentiment'].map(LABEL_MAP)\n",
    "\n",
    "# Split: 70% train, 15% val, 15% test\n",
    "train_df, temp_df = train_test_split(\n",
    "    df_balanced, test_size=0.3, random_state=42, stratify=df_balanced['label']\n",
    ")\n",
    "val_df, test_df = train_test_split(\n",
    "    temp_df, test_size=0.5, random_state=42, stratify=temp_df['label']\n",
    ")\n",
    "\n",
    "print('=' * 60)\n",
    "print('üìÇ DATA SPLITS')\n",
    "print('=' * 60)\n",
    "print(f'Train: {len(train_df):,} samples ({len(train_df)/len(df_balanced)*100:.1f}%)')\n",
    "print(f'Val:   {len(val_df):,} samples ({len(val_df)/len(df_balanced)*100:.1f}%)')\n",
    "print(f'Test:  {len(test_df):,} samples ({len(test_df)/len(df_balanced)*100:.1f}%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d40d14b1",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è 4. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2411265f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === HYPERPARAMETERS ===\n",
    "# Optimized untuk menghindari overfitting\n",
    "\n",
    "CONFIG = {\n",
    "    # Model\n",
    "    'model_name': 'indobenchmark/indobert-base-p1',\n",
    "    'max_length': 128,\n",
    "    'num_classes': NUM_CLASSES,\n",
    "    \n",
    "    # Training\n",
    "    'batch_size': 32,\n",
    "    'epochs': 30,\n",
    "    'learning_rate': 5e-6,  # Sangat kecil untuk hindari overfitting\n",
    "    \n",
    "    # Anti-Overfitting\n",
    "    'dropout_rate': 0.6,\n",
    "    'attention_dropout': 0.3,\n",
    "    'weight_decay': 0.05,\n",
    "    'label_smoothing': 0.2,\n",
    "    'warmup_ratio': 0.15,\n",
    "    'max_grad_norm': 0.5,\n",
    "    'early_stopping_patience': 7,\n",
    "    \n",
    "    # Data Augmentation\n",
    "    'word_dropout_prob': 0.2,\n",
    "    \n",
    "    # Layer Freezing\n",
    "    'freeze_layers': 9,  # Freeze 9 dari 12 layer\n",
    "    \n",
    "    # R-Drop\n",
    "    'rdrop_alpha': 0.5,\n",
    "}\n",
    "\n",
    "print('=' * 60)\n",
    "print('‚öôÔ∏è  CONFIGURATION')\n",
    "print('=' * 60)\n",
    "for key, value in CONFIG.items():\n",
    "    print(f'{key}: {value}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "225d303e",
   "metadata": {},
   "source": [
    "## üì¶ 5. Dataset & DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cef18596",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer\n",
    "print('Loading tokenizer...')\n",
    "tokenizer = BertTokenizer.from_pretrained(CONFIG['model_name'])\n",
    "print(f'‚úì Tokenizer loaded: {CONFIG[\"model_name\"]}')\n",
    "\n",
    "class SentimentDataset(Dataset):\n",
    "    \"\"\"Dataset dengan augmentation\"\"\"\n",
    "    \n",
    "    def __init__(self, texts, labels, tokenizer, max_length=128, \n",
    "                 augment=False, word_dropout_prob=0.2):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.augment = augment\n",
    "        self.word_dropout_prob = word_dropout_prob\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def _augment_text(self, text):\n",
    "        if not self.augment:\n",
    "            return text\n",
    "        \n",
    "        text = str(text)\n",
    "        words = text.split()\n",
    "        \n",
    "        if len(words) <= 3:\n",
    "            return text\n",
    "        \n",
    "        aug_type = random.random()\n",
    "        \n",
    "        if aug_type < 0.3:\n",
    "            # Word dropout\n",
    "            words = [w for w in words if random.random() > self.word_dropout_prob]\n",
    "        elif aug_type < 0.5:\n",
    "            # Word swap\n",
    "            if len(words) > 2:\n",
    "                idx = random.randint(0, len(words) - 2)\n",
    "                words[idx], words[idx + 1] = words[idx + 1], words[idx]\n",
    "        elif aug_type < 0.7:\n",
    "            # Random deletion\n",
    "            if len(words) > 4:\n",
    "                del_idx = random.randint(0, len(words) - 1)\n",
    "                words.pop(del_idx)\n",
    "        \n",
    "        return ' '.join(words) if words else text\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = self._augment_text(self.texts[idx])\n",
    "        \n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            str(text),\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'label': torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b8f477e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datasets\n",
    "train_dataset = SentimentDataset(\n",
    "    train_df['content_clean'].values,\n",
    "    train_df['label'].values,\n",
    "    tokenizer,\n",
    "    max_length=CONFIG['max_length'],\n",
    "    augment=True,\n",
    "    word_dropout_prob=CONFIG['word_dropout_prob']\n",
    ")\n",
    "\n",
    "val_dataset = SentimentDataset(\n",
    "    val_df['content_clean'].values,\n",
    "    val_df['label'].values,\n",
    "    tokenizer,\n",
    "    max_length=CONFIG['max_length'],\n",
    "    augment=False\n",
    ")\n",
    "\n",
    "test_dataset = SentimentDataset(\n",
    "    test_df['content_clean'].values,\n",
    "    test_df['label'].values,\n",
    "    tokenizer,\n",
    "    max_length=CONFIG['max_length'],\n",
    "    augment=False\n",
    ")\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=CONFIG['batch_size'], shuffle=True, drop_last=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=CONFIG['batch_size'], shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=CONFIG['batch_size'], shuffle=False)\n",
    "\n",
    "print(f'\\n‚úì Datasets created:')\n",
    "print(f'  Train: {len(train_dataset)} samples, {len(train_loader)} batches')\n",
    "print(f'  Val:   {len(val_dataset)} samples, {len(val_loader)} batches')\n",
    "print(f'  Test:  {len(test_dataset)} samples, {len(test_loader)} batches')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6496c97",
   "metadata": {},
   "source": [
    "## üß† 6. Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34bc7ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IndoBERTSentimentClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    IndoBERT dengan regularisasi maksimal:\n",
    "    - Freeze 9/12 layer BERT\n",
    "    - Multiple dropout\n",
    "    - Simple classifier\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model_name, num_classes, dropout_rate=0.6, \n",
    "                 attention_dropout=0.3, freeze_layers=9):\n",
    "        super(IndoBERTSentimentClassifier, self).__init__()\n",
    "        \n",
    "        # Load pretrained BERT\n",
    "        self.bert = BertModel.from_pretrained(model_name)\n",
    "        self.hidden_size = self.bert.config.hidden_size\n",
    "        \n",
    "        # Freeze embeddings\n",
    "        for param in self.bert.embeddings.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        # Freeze first N encoder layers\n",
    "        for i in range(freeze_layers):\n",
    "            for param in self.bert.encoder.layer[i].parameters():\n",
    "                param.requires_grad = False\n",
    "        \n",
    "        # Add dropout to attention in unfrozen layers\n",
    "        for i in range(freeze_layers, 12):\n",
    "            self.bert.encoder.layer[i].attention.self.dropout = nn.Dropout(attention_dropout)\n",
    "            self.bert.encoder.layer[i].attention.output.dropout = nn.Dropout(attention_dropout)\n",
    "        \n",
    "        print(f'‚úì Froze embeddings and first {freeze_layers} encoder layers')\n",
    "        print(f'  Only layers {freeze_layers}-11 are trainable (3 layers)')\n",
    "        \n",
    "        # Regularization\n",
    "        self.dropout1 = nn.Dropout(dropout_rate)\n",
    "        self.dropout2 = nn.Dropout(dropout_rate)\n",
    "        self.layer_norm = nn.LayerNorm(self.hidden_size)\n",
    "        \n",
    "        # Simple classifier\n",
    "        self.fc = nn.Linear(self.hidden_size, num_classes)\n",
    "        \n",
    "        # Initialize\n",
    "        nn.init.xavier_uniform_(self.fc.weight)\n",
    "        nn.init.zeros_(self.fc.bias)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask\n",
    "        )\n",
    "        \n",
    "        pooled_output = outputs.pooler_output\n",
    "        \n",
    "        x = self.layer_norm(pooled_output)\n",
    "        x = self.dropout1(x)\n",
    "        \n",
    "        if self.training:\n",
    "            x = self.dropout2(x)\n",
    "        \n",
    "        logits = self.fc(x)\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b7d68b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "print('Loading IndoBERT model...')\n",
    "model = IndoBERTSentimentClassifier(\n",
    "    model_name=CONFIG['model_name'],\n",
    "    num_classes=CONFIG['num_classes'],\n",
    "    dropout_rate=CONFIG['dropout_rate'],\n",
    "    attention_dropout=CONFIG['attention_dropout'],\n",
    "    freeze_layers=CONFIG['freeze_layers']\n",
    ").to(device)\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "frozen_params = total_params - trainable_params\n",
    "\n",
    "print(f'\\n‚úì Model loaded to {device}')\n",
    "print(f'  Total parameters: {total_params:,}')\n",
    "print(f'  Trainable: {trainable_params:,} ({trainable_params/total_params*100:.1f}%)')\n",
    "print(f'  Frozen: {frozen_params:,} ({frozen_params/total_params*100:.1f}%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8e17cac",
   "metadata": {},
   "source": [
    "## üìâ 7. Loss, Optimizer & Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c4c140",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function dengan label smoothing\n",
    "criterion = nn.CrossEntropyLoss(label_smoothing=CONFIG['label_smoothing'])\n",
    "\n",
    "# Optimizer - hanya untuk trainable parameters\n",
    "no_decay = ['bias', 'LayerNorm.weight', 'layer_norm.weight']\n",
    "trainable_params_list = [(n, p) for n, p in model.named_parameters() if p.requires_grad]\n",
    "\n",
    "optimizer_grouped_parameters = [\n",
    "    {\n",
    "        'params': [p for n, p in trainable_params_list if not any(nd in n for nd in no_decay)],\n",
    "        'weight_decay': CONFIG['weight_decay']\n",
    "    },\n",
    "    {\n",
    "        'params': [p for n, p in trainable_params_list if any(nd in n for nd in no_decay)],\n",
    "        'weight_decay': 0.0\n",
    "    }\n",
    "]\n",
    "\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=CONFIG['learning_rate'])\n",
    "\n",
    "# Scheduler\n",
    "total_steps = len(train_loader) * CONFIG['epochs']\n",
    "warmup_steps = int(total_steps * CONFIG['warmup_ratio'])\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=warmup_steps,\n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "print(f'‚úì Optimizer: AdamW (lr={CONFIG[\"learning_rate\"]}, wd={CONFIG[\"weight_decay\"]})')\n",
    "print(f'‚úì Scheduler: Linear warmup ({warmup_steps} warmup, {total_steps} total)')\n",
    "print(f'‚úì Loss: CrossEntropy with label_smoothing={CONFIG[\"label_smoothing\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "522ed0e8",
   "metadata": {},
   "source": [
    "## üèãÔ∏è 8. Training Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "821bd204",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_kl_loss(p, q):\n",
    "    \"\"\"KL divergence for R-Drop\"\"\"\n",
    "    p_loss = F.kl_div(F.log_softmax(p, dim=-1), F.softmax(q, dim=-1), reduction='batchmean')\n",
    "    q_loss = F.kl_div(F.log_softmax(q, dim=-1), F.softmax(p, dim=-1), reduction='batchmean')\n",
    "    return (p_loss + q_loss) / 2\n",
    "\n",
    "def train_epoch(model, dataloader, criterion, optimizer, scheduler, device, \n",
    "                max_grad_norm, rdrop_alpha=0.5):\n",
    "    \"\"\"Train dengan R-Drop regularization\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    progress_bar = tqdm(dataloader, desc='Training', leave=False)\n",
    "    \n",
    "    for batch in progress_bar:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # R-Drop: 2 forward passes\n",
    "        logits1 = model(input_ids, attention_mask)\n",
    "        logits2 = model(input_ids, attention_mask)\n",
    "        \n",
    "        # Cross entropy loss\n",
    "        ce_loss = (criterion(logits1, labels) + criterion(logits2, labels)) / 2\n",
    "        \n",
    "        # KL divergence loss\n",
    "        kl_loss = compute_kl_loss(logits1, logits2)\n",
    "        \n",
    "        # Total loss\n",
    "        loss = ce_loss + rdrop_alpha * kl_loss\n",
    "        \n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "        \n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        avg_logits = (logits1 + logits2) / 2\n",
    "        preds = torch.argmax(avg_logits, dim=1).cpu().numpy()\n",
    "        all_preds.extend(preds)\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        progress_bar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "    \n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "    \n",
    "    return avg_loss, accuracy, f1\n",
    "\n",
    "def evaluate(model, dataloader, criterion, device):\n",
    "    \"\"\"Evaluate model\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "            \n",
    "            logits = model(input_ids, attention_mask)\n",
    "            loss = criterion(logits, labels)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            preds = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "            all_preds.extend(preds)\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "    \n",
    "    return avg_loss, accuracy, f1, all_preds, all_labels\n",
    "\n",
    "class EarlyStopping:\n",
    "    \"\"\"Early stopping dengan gap monitoring\"\"\"\n",
    "    \n",
    "    def __init__(self, patience=7, min_delta=0.001, mode='max', max_gap=0.08):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.mode = mode\n",
    "        self.max_gap = max_gap\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.best_model = None\n",
    "        self.best_gap = float('inf')\n",
    "    \n",
    "    def __call__(self, score, model, train_score=None):\n",
    "        if self.mode == 'max':\n",
    "            is_improvement = self.best_score is None or score > self.best_score + self.min_delta\n",
    "        else:\n",
    "            is_improvement = self.best_score is None or score < self.best_score - self.min_delta\n",
    "        \n",
    "        if is_improvement:\n",
    "            self.best_score = score\n",
    "            self.best_model = copy.deepcopy(model.state_dict())\n",
    "            self.counter = 0\n",
    "            if train_score is not None:\n",
    "                self.best_gap = train_score - score\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        \n",
    "        return self.early_stop\n",
    "\n",
    "print('‚úì Training functions defined')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6c1b80a",
   "metadata": {},
   "source": [
    "## üöÄ 9. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c5e8d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training history\n",
    "history = {\n",
    "    'train_loss': [], 'train_acc': [], 'train_f1': [],\n",
    "    'val_loss': [], 'val_acc': [], 'val_f1': [],\n",
    "    'gap': [],\n",
    "    'epoch_time': [],\n",
    "    'memory_used_mb': [],\n",
    "    'power_draw_w': []\n",
    "}\n",
    "\n",
    "early_stopping = EarlyStopping(\n",
    "    patience=CONFIG['early_stopping_patience'], \n",
    "    mode='max',\n",
    "    max_gap=0.08\n",
    ")\n",
    "\n",
    "print('=' * 60)\n",
    "print('üöÄ TRAINING STARTED')\n",
    "print('=' * 60)\n",
    "print(f'Device: {device}')\n",
    "print(f'Epochs: {CONFIG[\"epochs\"]} | Patience: {CONFIG[\"early_stopping_patience\"]}')\n",
    "print(f'LR: {CONFIG[\"learning_rate\"]} | Batch: {CONFIG[\"batch_size\"]}')\n",
    "print(f'Frozen Layers: {CONFIG[\"freeze_layers\"]}/12 | Dropout: {CONFIG[\"dropout_rate\"]}')\n",
    "print('-' * 60)\n",
    "\n",
    "# Show initial GPU stats\n",
    "if torch.cuda.is_available():\n",
    "    gpu_stats = get_gpu_stats()\n",
    "    if gpu_stats:\n",
    "        print(f'üéÆ GPU: {torch.cuda.get_device_name(0)}')\n",
    "        print(f'   Memory: {gpu_stats[\"memory_used_mb\"]:.0f}/{gpu_stats[\"memory_total_mb\"]:.0f} MB')\n",
    "        print(f'   Power: {gpu_stats[\"power_draw_w\"]:.1f} W | Temp: {gpu_stats[\"temperature_c\"]:.0f}¬∞C')\n",
    "print('-' * 60)\n",
    "\n",
    "best_val_f1 = 0\n",
    "best_epoch = 0\n",
    "best_gap = float('inf')\n",
    "\n",
    "# Start GPU monitoring\n",
    "gpu_monitor.start()\n",
    "training_start_time = datetime.now()\n",
    "\n",
    "for epoch in range(CONFIG['epochs']):\n",
    "    epoch_start = time.time()\n",
    "    print(f'\\nüìç Epoch {epoch + 1}/{CONFIG[\"epochs\"]}')\n",
    "    \n",
    "    # Train\n",
    "    train_loss, train_acc, train_f1 = train_epoch(\n",
    "        model, train_loader, criterion, optimizer, scheduler, \n",
    "        device, CONFIG['max_grad_norm'], CONFIG['rdrop_alpha']\n",
    "    )\n",
    "    \n",
    "    # Sample GPU stats after training\n",
    "    gpu_monitor.sample()\n",
    "    \n",
    "    # Validate\n",
    "    val_loss, val_acc, val_f1, _, _ = evaluate(\n",
    "        model, val_loader, criterion, device\n",
    "    )\n",
    "    \n",
    "    gap = train_acc - val_acc\n",
    "    epoch_time = time.time() - epoch_start\n",
    "    \n",
    "    # Get current GPU stats\n",
    "    current_memory = 0\n",
    "    current_power = 0\n",
    "    if torch.cuda.is_available():\n",
    "        current_memory = torch.cuda.memory_allocated(0) / 1024**2\n",
    "        current_power = get_gpu_power()\n",
    "    \n",
    "    # Save history\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['train_acc'].append(train_acc)\n",
    "    history['train_f1'].append(train_f1)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['val_acc'].append(val_acc)\n",
    "    history['val_f1'].append(val_f1)\n",
    "    history['gap'].append(gap)\n",
    "    history['epoch_time'].append(epoch_time)\n",
    "    history['memory_used_mb'].append(current_memory)\n",
    "    history['power_draw_w'].append(current_power)\n",
    "    \n",
    "    # Print metrics\n",
    "    print(f'  Train - Loss: {train_loss:.4f} | Acc: {train_acc:.4f} | F1: {train_f1:.4f}')\n",
    "    print(f'  Val   - Loss: {val_loss:.4f} | Acc: {val_acc:.4f} | F1: {val_f1:.4f}')\n",
    "    \n",
    "    # Track best\n",
    "    if val_f1 > best_val_f1 and gap < 0.10:\n",
    "        best_val_f1 = val_f1\n",
    "        best_epoch = epoch + 1\n",
    "        best_gap = gap\n",
    "        print(f'  ‚≠ê New best! F1: {val_f1:.4f}, Gap: {gap*100:.2f}%')\n",
    "    \n",
    "    # Gap status\n",
    "    gap_status = '‚úÖ Good' if gap < 0.05 else '‚ö° OK' if gap < 0.10 else '‚ö†Ô∏è High'\n",
    "    print(f'  üìä Gap: {gap*100:.2f}% {gap_status}')\n",
    "    print(f'  ‚è±Ô∏è  Time: {epoch_time:.1f}s | üíæ Mem: {current_memory:.0f}MB | ‚ö° Power: {current_power:.1f}W')\n",
    "    \n",
    "    # Early stopping\n",
    "    if early_stopping(val_f1, model, train_acc):\n",
    "        print(f'\\nüõë Early stopping at epoch {epoch + 1}')\n",
    "        break\n",
    "\n",
    "# Stop GPU monitoring\n",
    "gpu_monitor.stop()\n",
    "training_end_time = datetime.now()\n",
    "total_time = training_end_time - training_start_time\n",
    "\n",
    "# Load best model\n",
    "if early_stopping.best_model is not None:\n",
    "    model.load_state_dict(early_stopping.best_model)\n",
    "\n",
    "# Get GPU summary\n",
    "gpu_summary = gpu_monitor.get_summary()\n",
    "\n",
    "print('\\n' + '=' * 60)\n",
    "print('‚úÖ TRAINING COMPLETED')\n",
    "print('=' * 60)\n",
    "print(f'  Best epoch: {best_epoch}')\n",
    "print(f'  Best Val F1: {best_val_f1:.4f}')\n",
    "print(f'  Best Gap: {best_gap*100:.2f}%')\n",
    "print()\n",
    "print('‚è±Ô∏è  TIME STATISTICS:')\n",
    "print(f'  Total Duration: {gpu_summary[\"duration_formatted\"]}')\n",
    "print(f'  Avg Time/Epoch: {np.mean(history[\"epoch_time\"]):.1f}s')\n",
    "print()\n",
    "print('üíæ MEMORY STATISTICS:')\n",
    "print(f'  Peak Memory: {gpu_summary[\"peak_memory_mb\"]:.0f} MB')\n",
    "print(f'  Avg Memory: {gpu_summary[\"avg_memory_mb\"]:.0f} MB')\n",
    "print()\n",
    "print('‚ö° POWER STATISTICS:')\n",
    "print(f'  Avg Power: {gpu_summary[\"avg_power_w\"]:.1f} W')\n",
    "print(f'  Max Power: {gpu_summary[\"max_power_w\"]:.1f} W')\n",
    "print(f'  Total Energy: {gpu_summary[\"total_energy_wh\"]:.2f} Wh ({gpu_summary[\"total_energy_kwh\"]:.4f} kWh)')\n",
    "print('=' * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f28c01",
   "metadata": {},
   "source": [
    "## üìà 10. Training Visualization & Resource Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59655b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================\n",
    "# FIGURE 1: Training Metrics\n",
    "# =====================================================\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "epochs_range = range(1, len(history['train_loss']) + 1)\n",
    "\n",
    "# Loss\n",
    "axes[0, 0].plot(epochs_range, history['train_loss'], 'b-o', label='Train', markersize=4)\n",
    "axes[0, 0].plot(epochs_range, history['val_loss'], 'r-s', label='Val', markersize=4)\n",
    "axes[0, 0].set_xlabel('Epoch')\n",
    "axes[0, 0].set_ylabel('Loss')\n",
    "axes[0, 0].set_title('Training & Validation Loss')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Accuracy\n",
    "axes[0, 1].plot(epochs_range, history['train_acc'], 'b-o', label='Train', markersize=4)\n",
    "axes[0, 1].plot(epochs_range, history['val_acc'], 'r-s', label='Val', markersize=4)\n",
    "axes[0, 1].set_xlabel('Epoch')\n",
    "axes[0, 1].set_ylabel('Accuracy')\n",
    "axes[0, 1].set_title('Training & Validation Accuracy')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# F1 Score\n",
    "axes[1, 0].plot(epochs_range, history['train_f1'], 'b-o', label='Train', markersize=4)\n",
    "axes[1, 0].plot(epochs_range, history['val_f1'], 'r-s', label='Val', markersize=4)\n",
    "axes[1, 0].set_xlabel('Epoch')\n",
    "axes[1, 0].set_ylabel('F1 Score')\n",
    "axes[1, 0].set_title('Training & Validation F1 Score')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Gap\n",
    "axes[1, 1].plot(epochs_range, [g*100 for g in history['gap']], 'g-o', markersize=4)\n",
    "axes[1, 1].axhline(y=5, color='orange', linestyle='--', label='Good threshold (5%)')\n",
    "axes[1, 1].axhline(y=10, color='red', linestyle='--', label='Warning threshold (10%)')\n",
    "axes[1, 1].set_xlabel('Epoch')\n",
    "axes[1, 1].set_ylabel('Train-Val Gap (%)')\n",
    "axes[1, 1].set_title('Overfitting Monitor (Train-Val Gap)')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('training_history.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print('‚úì Saved: training_history.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fea65ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================\n",
    "# FIGURE 2: Resource Usage (Time, Memory, Power)\n",
    "# =====================================================\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "epochs_range = range(1, len(history['epoch_time']) + 1)\n",
    "\n",
    "# Time per epoch\n",
    "axes[0, 0].bar(epochs_range, history['epoch_time'], color='steelblue', alpha=0.7)\n",
    "axes[0, 0].axhline(y=np.mean(history['epoch_time']), color='red', linestyle='--', \n",
    "                   label=f'Avg: {np.mean(history[\"epoch_time\"]):.1f}s')\n",
    "axes[0, 0].set_xlabel('Epoch')\n",
    "axes[0, 0].set_ylabel('Time (seconds)')\n",
    "axes[0, 0].set_title('‚è±Ô∏è Training Time per Epoch')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Memory usage\n",
    "if any(history['memory_used_mb']):\n",
    "    axes[0, 1].plot(epochs_range, history['memory_used_mb'], 'purple', marker='o', markersize=4)\n",
    "    axes[0, 1].axhline(y=gpu_summary['peak_memory_mb'], color='red', linestyle='--', \n",
    "                       label=f'Peak: {gpu_summary[\"peak_memory_mb\"]:.0f} MB')\n",
    "    axes[0, 1].fill_between(epochs_range, 0, history['memory_used_mb'], alpha=0.3, color='purple')\n",
    "    axes[0, 1].set_xlabel('Epoch')\n",
    "    axes[0, 1].set_ylabel('Memory (MB)')\n",
    "    axes[0, 1].set_title('üíæ GPU Memory Usage')\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "else:\n",
    "    axes[0, 1].text(0.5, 0.5, 'No GPU Memory Data', ha='center', va='center', fontsize=14)\n",
    "    axes[0, 1].set_title('üíæ GPU Memory Usage')\n",
    "\n",
    "# Power consumption\n",
    "if any(history['power_draw_w']):\n",
    "    axes[1, 0].plot(epochs_range, history['power_draw_w'], 'orange', marker='s', markersize=4)\n",
    "    axes[1, 0].axhline(y=gpu_summary['avg_power_w'], color='red', linestyle='--', \n",
    "                       label=f'Avg: {gpu_summary[\"avg_power_w\"]:.1f} W')\n",
    "    axes[1, 0].fill_between(epochs_range, 0, history['power_draw_w'], alpha=0.3, color='orange')\n",
    "    axes[1, 0].set_xlabel('Epoch')\n",
    "    axes[1, 0].set_ylabel('Power (Watts)')\n",
    "    axes[1, 0].set_title('‚ö° GPU Power Consumption')\n",
    "    axes[1, 0].legend()\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "else:\n",
    "    axes[1, 0].text(0.5, 0.5, 'No Power Data', ha='center', va='center', fontsize=14)\n",
    "    axes[1, 0].set_title('‚ö° GPU Power Consumption')\n",
    "\n",
    "# Summary statistics as text\n",
    "axes[1, 1].axis('off')\n",
    "summary_text = f\"\"\"\n",
    "‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó\n",
    "‚ïë         üìä TRAINING RESOURCE SUMMARY         ‚ïë\n",
    "‚ï†‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï£\n",
    "‚ïë                                              ‚ïë\n",
    "‚ïë  ‚è±Ô∏è  TIME STATISTICS                         ‚ïë\n",
    "‚ïë  ‚îú‚îÄ Total Duration: {gpu_summary['duration_formatted']:>20}  ‚ïë\n",
    "‚ïë  ‚îú‚îÄ Total Seconds:  {gpu_summary['duration_seconds']:>17.1f}s  ‚ïë\n",
    "‚ïë  ‚îî‚îÄ Avg per Epoch:  {np.mean(history['epoch_time']):>17.1f}s  ‚ïë\n",
    "‚ïë                                              ‚ïë\n",
    "‚ïë  üíæ MEMORY STATISTICS                        ‚ïë\n",
    "‚ïë  ‚îú‚îÄ Peak Memory:    {gpu_summary['peak_memory_mb']:>15.0f} MB  ‚ïë\n",
    "‚ïë  ‚îî‚îÄ Avg Memory:     {gpu_summary['avg_memory_mb']:>15.0f} MB  ‚ïë\n",
    "‚ïë                                              ‚ïë\n",
    "‚ïë  ‚ö° POWER STATISTICS                         ‚ïë\n",
    "‚ïë  ‚îú‚îÄ Avg Power:      {gpu_summary['avg_power_w']:>16.1f} W  ‚ïë\n",
    "‚ïë  ‚îú‚îÄ Max Power:      {gpu_summary['max_power_w']:>16.1f} W  ‚ïë\n",
    "‚ïë  ‚îú‚îÄ Total Energy:   {gpu_summary['total_energy_wh']:>14.2f} Wh  ‚ïë\n",
    "‚ïë  ‚îî‚îÄ Total Energy:   {gpu_summary['total_energy_kwh']:>12.4f} kWh  ‚ïë\n",
    "‚ïë                                              ‚ïë\n",
    "‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù\n",
    "\"\"\"\n",
    "axes[1, 1].text(0.1, 0.5, summary_text, fontsize=11, fontfamily='monospace',\n",
    "                verticalalignment='center', bbox=dict(boxstyle='round', facecolor='lightgray', alpha=0.8))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('resource_usage.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print('‚úì Saved: resource_usage.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0424dd31",
   "metadata": {},
   "source": [
    "## üß™ 11. Evaluation on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1718cba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test set\n",
    "test_loss, test_acc, test_f1, test_preds, test_labels = evaluate(\n",
    "    model, test_loader, criterion, device\n",
    ")\n",
    "\n",
    "print('=' * 60)\n",
    "print('üß™ TEST SET EVALUATION')\n",
    "print('=' * 60)\n",
    "print(f'Test Accuracy: {test_acc*100:.2f}%')\n",
    "print(f'Test F1 Score: {test_f1*100:.2f}%')\n",
    "print(f'Test Loss: {test_loss:.4f}')\n",
    "\n",
    "# Classification report\n",
    "print('\\nüìä Classification Report:')\n",
    "print(classification_report(test_labels, test_preds, target_names=LABEL_NAMES))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2904eee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "cm = confusion_matrix(test_labels, test_preds)\n",
    "cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] * 100\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Counts\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=LABEL_NAMES, yticklabels=LABEL_NAMES, ax=axes[0])\n",
    "axes[0].set_xlabel('Predicted')\n",
    "axes[0].set_ylabel('Actual')\n",
    "axes[0].set_title('Confusion Matrix (Counts)')\n",
    "\n",
    "# Percentages\n",
    "sns.heatmap(cm_normalized, annot=True, fmt='.1f', cmap='Blues',\n",
    "            xticklabels=LABEL_NAMES, yticklabels=LABEL_NAMES, ax=axes[1])\n",
    "axes[1].set_xlabel('Predicted')\n",
    "axes[1].set_ylabel('Actual')\n",
    "axes[1].set_title('Confusion Matrix (%)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('confusion_matrix.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print('‚úì Saved: confusion_matrix.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de2423e",
   "metadata": {},
   "source": [
    "## üíæ 12. Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb83a48f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create models directory\n",
    "os.makedirs('models', exist_ok=True)\n",
    "\n",
    "# Save model\n",
    "model_path = 'models/indobert_sentiment_3class.pt'\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'config': CONFIG,\n",
    "    'label_map': LABEL_MAP,\n",
    "    'label_names': LABEL_NAMES,\n",
    "    'test_accuracy': test_acc,\n",
    "    'test_f1': test_f1,\n",
    "    'best_val_f1': best_val_f1,\n",
    "    'best_gap': best_gap,\n",
    "    'history': history,\n",
    "}, model_path)\n",
    "print(f'‚úì Model saved: {model_path}')\n",
    "\n",
    "# Save tokenizer\n",
    "tokenizer.save_pretrained('models/tokenizer')\n",
    "print(f'‚úì Tokenizer saved: models/tokenizer/')\n",
    "\n",
    "# Save history\n",
    "with open('models/training_history.json', 'w') as f:\n",
    "    json.dump(history, f, indent=2)\n",
    "print(f'‚úì History saved: models/training_history.json')\n",
    "\n",
    "# List saved files\n",
    "print('\\nüìÅ Saved files:')\n",
    "for root, dirs, files in os.walk('models'):\n",
    "    for file in files:\n",
    "        filepath = os.path.join(root, file)\n",
    "        size = os.path.getsize(filepath) / (1024*1024)\n",
    "        print(f'   {filepath} ({size:.2f} MB)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f5ead6",
   "metadata": {},
   "source": [
    "## üîÆ 13. Inference Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd9bd9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentiment(text, model, tokenizer, device, label_names):\n",
    "    \"\"\"Predict sentiment untuk satu teks\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    encoding = tokenizer.encode_plus(\n",
    "        text,\n",
    "        add_special_tokens=True,\n",
    "        max_length=128,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    \n",
    "    input_ids = encoding['input_ids'].to(device)\n",
    "    attention_mask = encoding['attention_mask'].to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        logits = model(input_ids, attention_mask)\n",
    "        probs = torch.softmax(logits, dim=1)\n",
    "        pred = torch.argmax(probs, dim=1).item()\n",
    "    \n",
    "    return {\n",
    "        'sentiment': label_names[pred],\n",
    "        'confidence': probs[0][pred].item(),\n",
    "        'probabilities': {\n",
    "            label_names[i]: probs[0][i].item() \n",
    "            for i in range(len(label_names))\n",
    "        }\n",
    "    }\n",
    "\n",
    "# Test\n",
    "test_reviews = [\n",
    "    \"Aplikasi gojek sangat membantu, driver ramah dan cepat\",\n",
    "    \"Driver nya lama banget, udah nunggu 1 jam gak datang\",\n",
    "    \"Biasa aja sih aplikasinya\",\n",
    "    \"Pelayanan buruk, tidak akan pakai lagi\",\n",
    "    \"Mantap, makanan sampai dengan selamat dan masih hangat\",\n",
    "]\n",
    "\n",
    "print('=' * 60)\n",
    "print('üîÆ INFERENCE DEMO')\n",
    "print('=' * 60)\n",
    "\n",
    "for review in test_reviews:\n",
    "    result = predict_sentiment(review, model, tokenizer, device, LABEL_NAMES)\n",
    "    emoji = {'negative': 'üò†', 'neutral': 'üòê', 'positive': 'üòä'}[result['sentiment']]\n",
    "    print(f'\\nüìù \"{review[:50]}...\"' if len(review) > 50 else f'\\nüìù \"{review}\"')\n",
    "    print(f'   {emoji} {result[\"sentiment\"].upper()} ({result[\"confidence\"]*100:.1f}%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5541f9c",
   "metadata": {},
   "source": [
    "## üìä 14. Final Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e3cd4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_gap = history['train_acc'][-1] - history['val_acc'][-1]\n",
    "min_gap = min(history['gap'])\n",
    "\n",
    "print('=' * 70)\n",
    "print('üìä FINAL TRAINING SUMMARY')\n",
    "print('=' * 70)\n",
    "\n",
    "print(f'''\n",
    "üéØ MODEL PERFORMANCE:\n",
    "   ‚Ä¢ Test Accuracy: {test_acc*100:.2f}%\n",
    "   ‚Ä¢ Test F1 Score: {test_f1*100:.2f}%\n",
    "   ‚Ä¢ Best Val F1: {best_val_f1*100:.2f}%\n",
    "\n",
    "üìà OVERFITTING CHECK:\n",
    "   ‚Ä¢ Final Gap: {final_gap*100:.2f}%\n",
    "   ‚Ä¢ Best Gap: {best_gap*100:.2f}%\n",
    "   ‚Ä¢ Status: {\"‚úÖ Good\" if final_gap < 0.05 else \"‚ö†Ô∏è Check\" if final_gap < 0.10 else \"‚ùå Overfitting\"}\n",
    "\n",
    "‚è±Ô∏è  TIME & RESOURCE USAGE:\n",
    "   ‚Ä¢ Total Training Time: {gpu_summary['duration_formatted']}\n",
    "   ‚Ä¢ Average Time per Epoch: {np.mean(history['epoch_time']):.1f} seconds\n",
    "   ‚Ä¢ Peak GPU Memory: {gpu_summary['peak_memory_mb']:.0f} MB\n",
    "   ‚Ä¢ Average GPU Memory: {gpu_summary['avg_memory_mb']:.0f} MB\n",
    "   ‚Ä¢ Average Power Draw: {gpu_summary['avg_power_w']:.1f} W\n",
    "   ‚Ä¢ Max Power Draw: {gpu_summary['max_power_w']:.1f} W\n",
    "   ‚Ä¢ Total Energy Consumed: {gpu_summary['total_energy_wh']:.2f} Wh ({gpu_summary['total_energy_kwh']:.4f} kWh)\n",
    "\n",
    "‚öôÔ∏è TECHNIQUES USED:\n",
    "   ‚Ä¢ Layer Freezing: {CONFIG['freeze_layers']}/12\n",
    "   ‚Ä¢ Dropout: {CONFIG['dropout_rate']}\n",
    "   ‚Ä¢ R-Drop Alpha: {CONFIG['rdrop_alpha']}\n",
    "   ‚Ä¢ Weight Decay: {CONFIG['weight_decay']}\n",
    "   ‚Ä¢ Label Smoothing: {CONFIG['label_smoothing']}\n",
    "   ‚Ä¢ Learning Rate: {CONFIG['learning_rate']}\n",
    "\n",
    "üíæ SAVED FILES:\n",
    "   ‚Ä¢ models/indobert_sentiment_3class.pt\n",
    "   ‚Ä¢ models/tokenizer/\n",
    "   ‚Ä¢ models/training_history.json\n",
    "   ‚Ä¢ training_history.png\n",
    "   ‚Ä¢ resource_usage.png\n",
    "   ‚Ä¢ confusion_matrix.png\n",
    "''')\n",
    "\n",
    "if test_acc >= 0.75 and final_gap < 0.05:\n",
    "    print('üéâ EXCELLENT! Model has good accuracy and generalization!')\n",
    "elif test_acc >= 0.75:\n",
    "    print('‚ö†Ô∏è Good accuracy but watch for overfitting.')\n",
    "elif final_gap < 0.05:\n",
    "    print('‚úÖ Good generalization but accuracy could improve.')\n",
    "else:\n",
    "    print('‚ùå Consider adjusting hyperparameters.')\n",
    "\n",
    "print('=' * 70)\n",
    "print('‚úÖ Training completed!')\n",
    "print('=' * 70)\n",
    "\n",
    "# Save complete training report\n",
    "training_report = {\n",
    "    'model_performance': {\n",
    "        'test_accuracy': float(test_acc),\n",
    "        'test_f1_score': float(test_f1),\n",
    "        'best_val_f1': float(best_val_f1),\n",
    "        'best_epoch': best_epoch,\n",
    "        'final_gap': float(final_gap),\n",
    "        'best_gap': float(best_gap)\n",
    "    },\n",
    "    'resource_usage': {\n",
    "        'total_duration_seconds': gpu_summary['duration_seconds'],\n",
    "        'total_duration_formatted': gpu_summary['duration_formatted'],\n",
    "        'avg_epoch_time_seconds': float(np.mean(history['epoch_time'])),\n",
    "        'peak_memory_mb': gpu_summary['peak_memory_mb'],\n",
    "        'avg_memory_mb': gpu_summary['avg_memory_mb'],\n",
    "        'avg_power_watts': gpu_summary['avg_power_w'],\n",
    "        'max_power_watts': gpu_summary['max_power_w'],\n",
    "        'total_energy_wh': gpu_summary['total_energy_wh'],\n",
    "        'total_energy_kwh': gpu_summary['total_energy_kwh']\n",
    "    },\n",
    "    'config': CONFIG,\n",
    "    'history': history\n",
    "}\n",
    "\n",
    "with open('models/training_report.json', 'w') as f:\n",
    "    json.dump(training_report, f, indent=2)\n",
    "print('\\n‚úì Complete training report saved: models/training_report.json')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
