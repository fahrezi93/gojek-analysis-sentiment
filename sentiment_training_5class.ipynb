{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69423317",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# SETUP GOOGLE COLAB\n",
    "# ============================================\n",
    "\n",
    "# Install dependencies\n",
    "!pip install transformers torch pandas numpy scikit-learn matplotlib seaborn tqdm -q\n",
    "\n",
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Path ke folder skripsi di Google Drive\n",
    "DRIVE_PATH = '/content/drive/MyDrive/skripsi'\n",
    "\n",
    "import os\n",
    "\n",
    "# Check apakah folder exists\n",
    "if os.path.exists(DRIVE_PATH):\n",
    "    os.chdir(DRIVE_PATH)\n",
    "    print(f'‚úì Working directory: {os.getcwd()}')\n",
    "    print(f'‚úì Files in folder skripsi:')\n",
    "    for f in os.listdir('.'):\n",
    "        print(f'   - {f}')\n",
    "else:\n",
    "    print(f'‚ùå Folder tidak ditemukan: {DRIVE_PATH}')\n",
    "    print('Pastikan folder \"skripsi\" ada di Google Drive kamu')\n",
    "\n",
    "# Check GPU\n",
    "import torch\n",
    "if torch.cuda.is_available():\n",
    "    print(f'\\n‚úì GPU Available: {torch.cuda.get_device_name(0)}')\n",
    "    print(f'‚úì GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB')\n",
    "else:\n",
    "    print('\\n‚ö†Ô∏è GPU not available, using CPU (akan lebih lambat)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11fd8d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "from transformers import BertTokenizer, BertModel, get_linear_schedule_with_warmup\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_recall_fscore_support, \n",
    "    classification_report, confusion_matrix, f1_score\n",
    ")\n",
    "from sklearn.utils import resample\n",
    "import random\n",
    "import os\n",
    "import copy\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Reproducibility\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "# Device setup\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'üñ•Ô∏è  Device: {device}')\n",
    "if torch.cuda.is_available():\n",
    "    print(f'üéÆ GPU: {torch.cuda.get_device_name(0)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27c4b229",
   "metadata": {},
   "source": [
    "## üìä 1. Load & Explore Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01feb7bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load cleaned data dari Google Drive folder skripsi\n",
    "DATA_FILES = [\n",
    "    'gojek_reviews_5class_clean.csv',\n",
    "    'data/gojek_reviews_5class_clean.csv',\n",
    "]\n",
    "\n",
    "DATA_PATH = None\n",
    "for f in DATA_FILES:\n",
    "    if os.path.exists(f):\n",
    "        DATA_PATH = f\n",
    "        break\n",
    "\n",
    "if DATA_PATH is None:\n",
    "    print('‚ùå Data file tidak ditemukan!')\n",
    "    print(f'\\nüìÅ Files yang ada di folder skripsi:')\n",
    "    for f in os.listdir('.'):\n",
    "        print(f'   - {f}')\n",
    "    if os.path.exists('data'):\n",
    "        print(f'\\nüìÅ Files di folder data:')\n",
    "        for f in os.listdir('data'):\n",
    "            print(f'   - data/{f}')\n",
    "    print(f'\\nüí° Upload file \"gojek_reviews_5class_clean.csv\" ke folder skripsi')\n",
    "else:\n",
    "    print(f'‚úì Using data file: {DATA_PATH}')\n",
    "    df = pd.read_csv(DATA_PATH)\n",
    "    \n",
    "    # Label mapping untuk 5 kelas\n",
    "    LABEL_MAP = {\n",
    "        'sangat_negatif': 0,\n",
    "        'negatif': 1,\n",
    "        'netral': 2,\n",
    "        'positif': 3,\n",
    "        'sangat_positif': 4\n",
    "    }\n",
    "    LABEL_NAMES = ['sangat_negatif', 'negatif', 'netral', 'positif', 'sangat_positif']\n",
    "    NUM_CLASSES = 5\n",
    "    \n",
    "    # Create label column\n",
    "    df['label'] = df['sentiment_label'].map(LABEL_MAP)\n",
    "    \n",
    "    print('=' * 60)\n",
    "    print('üìä DATA OVERVIEW')\n",
    "    print('=' * 60)\n",
    "    print(f'Total samples: {len(df):,}')\n",
    "    print(f'\\nColumns: {df.columns.tolist()}')\n",
    "    print(f'\\nüìà Sentiment Distribution:')\n",
    "    print(df['sentiment_label'].value_counts())\n",
    "    \n",
    "    # Visualize\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Bar plot\n",
    "    colors = ['#e74c3c', '#f39c12', '#95a5a6', '#3498db', '#2ecc71']\n",
    "    sentiment_counts = df['sentiment_label'].value_counts().reindex(LABEL_NAMES)\n",
    "    axes[0].bar(range(5), sentiment_counts.values, color=colors)\n",
    "    axes[0].set_xticks(range(5))\n",
    "    axes[0].set_xticklabels(LABEL_NAMES, rotation=45, ha='right')\n",
    "    axes[0].set_title('Sentiment Distribution (5 Classes)')\n",
    "    axes[0].set_ylabel('Count')\n",
    "    \n",
    "    # Pie chart\n",
    "    axes[1].pie(sentiment_counts.values, labels=LABEL_NAMES, \n",
    "                autopct='%1.1f%%', colors=colors)\n",
    "    axes[1].set_title('Sentiment Percentage')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Show sample reviews\n",
    "    print('\\nüìù Sample Reviews per Class:')\n",
    "    for label_name in LABEL_NAMES:\n",
    "        sample = df[df['sentiment_label'] == label_name].sample(1).iloc[0]\n",
    "        print(f'\\n[{label_name.upper()}] Rating {sample[\"rating\"]}:')\n",
    "        print(f'   \"{sample[\"review\"][:100]}...\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b842f9",
   "metadata": {},
   "source": [
    "## ‚öñÔ∏è 2. Data Preparation & Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b6442b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stratified split: Train (70%), Val (15%), Test (15%)\n",
    "# Stratified memastikan proporsi kelas sama di semua split\n",
    "\n",
    "# First split: Train vs (Val+Test)\n",
    "train_df, temp_df = train_test_split(\n",
    "    df, \n",
    "    test_size=0.3, \n",
    "    random_state=42, \n",
    "    stratify=df['label']\n",
    ")\n",
    "\n",
    "# Second split: Val vs Test\n",
    "val_df, test_df = train_test_split(\n",
    "    temp_df, \n",
    "    test_size=0.5, \n",
    "    random_state=42, \n",
    "    stratify=temp_df['label']\n",
    ")\n",
    "\n",
    "print('=' * 60)\n",
    "print('üìä DATA SPLIT')\n",
    "print('=' * 60)\n",
    "print(f'Training set: {len(train_df):,} samples ({len(train_df)/len(df)*100:.1f}%)')\n",
    "print(f'Validation set: {len(val_df):,} samples ({len(val_df)/len(df)*100:.1f}%)')\n",
    "print(f'Test set: {len(test_df):,} samples ({len(test_df)/len(df)*100:.1f}%)')\n",
    "\n",
    "# Verify stratification\n",
    "print('\\nüìà Distribution per split:')\n",
    "for name, data in [('Train', train_df), ('Val', val_df), ('Test', test_df)]:\n",
    "    dist = data['label'].value_counts(normalize=True).sort_index() * 100\n",
    "    print(f'{name}: ' + ' | '.join([f'{LABEL_NAMES[i][:4]}: {dist[i]:.1f}%' for i in range(5)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea61d0f",
   "metadata": {},
   "source": [
    "## üîß 3. Load IndoBERT Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a5f618",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load IndoBERT tokenizer\n",
    "MODEL_NAME = 'indobenchmark/indobert-base-p1'\n",
    "tokenizer = BertTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# Analyze text lengths untuk menentukan MAX_LEN optimal\n",
    "text_lengths = df['review'].apply(lambda x: len(tokenizer.encode(str(x), add_special_tokens=True)))\n",
    "\n",
    "print('=' * 60)\n",
    "print('üìè TEXT LENGTH ANALYSIS')\n",
    "print('=' * 60)\n",
    "print(f'Min tokens: {text_lengths.min()}')\n",
    "print(f'Max tokens: {text_lengths.max()}')\n",
    "print(f'Mean tokens: {text_lengths.mean():.1f}')\n",
    "print(f'Median tokens: {text_lengths.median()}')\n",
    "print(f'95th percentile: {text_lengths.quantile(0.95):.0f}')\n",
    "print(f'99th percentile: {text_lengths.quantile(0.99):.0f}')\n",
    "\n",
    "# Set MAX_LEN based on 95th percentile (to capture most texts)\n",
    "MAX_LEN = min(int(text_lengths.quantile(0.95)) + 10, 128)  # Cap at 128\n",
    "print(f'\\n‚úì Using MAX_LEN = {MAX_LEN}')\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.hist(text_lengths, bins=50, edgecolor='black', alpha=0.7)\n",
    "plt.axvline(x=MAX_LEN, color='r', linestyle='--', label=f'MAX_LEN = {MAX_LEN}')\n",
    "plt.xlabel('Token Count')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Text Lengths')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a654dc",
   "metadata": {},
   "source": [
    "## üì¶ 4. Dataset Class dengan Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46bc1ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentDataset(Dataset):\n",
    "    def __init__(self, reviews, labels, tokenizer, max_len, augment=False):\n",
    "        self.reviews = reviews\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.augment = augment\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.reviews)\n",
    "    \n",
    "    def _augment_text(self, text):\n",
    "        \"\"\"Random word dropout for augmentation\"\"\"\n",
    "        if not self.augment or random.random() > 0.3:  # 30% chance to augment\n",
    "            return text\n",
    "        \n",
    "        words = text.split()\n",
    "        if len(words) <= 3:\n",
    "            return text\n",
    "        \n",
    "        # Randomly drop 10-20% of words\n",
    "        drop_rate = random.uniform(0.1, 0.2)\n",
    "        keep_words = [w for w in words if random.random() > drop_rate]\n",
    "        \n",
    "        if len(keep_words) < 2:\n",
    "            return text\n",
    "        \n",
    "        return ' '.join(keep_words)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        review = str(self.reviews[idx])\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        # Apply augmentation\n",
    "        review = self._augment_text(review)\n",
    "        \n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            review,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'label': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = SentimentDataset(\n",
    "    train_df['review'].values,\n",
    "    train_df['label'].values,\n",
    "    tokenizer,\n",
    "    MAX_LEN,\n",
    "    augment=True  # Enable augmentation for training\n",
    ")\n",
    "\n",
    "val_dataset = SentimentDataset(\n",
    "    val_df['review'].values,\n",
    "    val_df['label'].values,\n",
    "    tokenizer,\n",
    "    MAX_LEN,\n",
    "    augment=False\n",
    ")\n",
    "\n",
    "test_dataset = SentimentDataset(\n",
    "    test_df['review'].values,\n",
    "    test_df['label'].values,\n",
    "    tokenizer,\n",
    "    MAX_LEN,\n",
    "    augment=False\n",
    ")\n",
    "\n",
    "# Create data loaders\n",
    "BATCH_SIZE = 16  # Smaller batch for 5 classes\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
    "\n",
    "print(f'‚úì Train batches: {len(train_loader)}')\n",
    "print(f'‚úì Val batches: {len(val_loader)}')\n",
    "print(f'‚úì Test batches: {len(test_loader)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2cdbfa4",
   "metadata": {},
   "source": [
    "## üèóÔ∏è 5. Model Architecture dengan Anti-Overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa2d7474",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IndoBERTSentiment5Class(nn.Module):\n",
    "    def __init__(self, model_name, num_classes=5, dropout_rate=0.3, freeze_bert_layers=6):\n",
    "        super(IndoBERTSentiment5Class, self).__init__()\n",
    "        \n",
    "        self.bert = BertModel.from_pretrained(model_name)\n",
    "        self.hidden_size = self.bert.config.hidden_size\n",
    "        \n",
    "        # Freeze lower BERT layers untuk mencegah overfitting\n",
    "        # Hanya fine-tune upper layers\n",
    "        if freeze_bert_layers > 0:\n",
    "            # Freeze embeddings\n",
    "            for param in self.bert.embeddings.parameters():\n",
    "                param.requires_grad = False\n",
    "            \n",
    "            # Freeze first N layers\n",
    "            for i in range(freeze_bert_layers):\n",
    "                for param in self.bert.encoder.layer[i].parameters():\n",
    "                    param.requires_grad = False\n",
    "        \n",
    "        # Multi-layer classifier dengan dropout\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(self.hidden_size, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.Dropout(dropout_rate * 0.5),  # Lower dropout before final layer\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "        \n",
    "        # Initialize classifier weights\n",
    "        self._init_weights()\n",
    "    \n",
    "    def _init_weights(self):\n",
    "        for module in self.classifier.modules():\n",
    "            if isinstance(module, nn.Linear):\n",
    "                nn.init.xavier_uniform_(module.weight)\n",
    "                if module.bias is not None:\n",
    "                    nn.init.zeros_(module.bias)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask\n",
    "        )\n",
    "        \n",
    "        # Use [CLS] token representation\n",
    "        pooled_output = outputs.pooler_output\n",
    "        \n",
    "        # Classify\n",
    "        logits = self.classifier(pooled_output)\n",
    "        \n",
    "        return logits\n",
    "\n",
    "# Initialize model\n",
    "model = IndoBERTSentiment5Class(\n",
    "    MODEL_NAME, \n",
    "    num_classes=NUM_CLASSES, \n",
    "    dropout_rate=0.3,\n",
    "    freeze_bert_layers=6  # Freeze first 6 layers (of 12)\n",
    ")\n",
    "model = model.to(device)\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "frozen_params = total_params - trainable_params\n",
    "\n",
    "print('=' * 60)\n",
    "print('üèóÔ∏è  MODEL ARCHITECTURE')\n",
    "print('=' * 60)\n",
    "print(f'Total parameters: {total_params:,}')\n",
    "print(f'Trainable parameters: {trainable_params:,} ({trainable_params/total_params*100:.1f}%)')\n",
    "print(f'Frozen parameters: {frozen_params:,} ({frozen_params/total_params*100:.1f}%)')\n",
    "print(f'\\n‚úì Freezing helps prevent overfitting by limiting trainable params')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d7385a",
   "metadata": {},
   "source": [
    "## üìâ 6. Loss Function dengan Label Smoothing dan Focal Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01132ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLossWithLabelSmoothing(nn.Module):\n",
    "    \"\"\"\n",
    "    Combines Focal Loss and Label Smoothing for better multi-class classification.\n",
    "    - Focal Loss: Focuses on hard examples, reduces weight on easy examples\n",
    "    - Label Smoothing: Prevents overconfidence, improves generalization\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes=5, gamma=2.0, alpha=None, smoothing=0.1):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.gamma = gamma  # Focusing parameter\n",
    "        self.smoothing = smoothing\n",
    "        \n",
    "        # Alpha for class weighting (optional)\n",
    "        if alpha is not None:\n",
    "            self.alpha = torch.tensor(alpha).float()\n",
    "        else:\n",
    "            self.alpha = None\n",
    "    \n",
    "    def forward(self, inputs, targets):\n",
    "        # Apply label smoothing\n",
    "        confidence = 1.0 - self.smoothing\n",
    "        smooth_labels = torch.zeros_like(inputs).scatter_(\n",
    "            1, targets.unsqueeze(1), confidence\n",
    "        )\n",
    "        smooth_labels += self.smoothing / self.num_classes\n",
    "        \n",
    "        # Compute probabilities\n",
    "        log_probs = F.log_softmax(inputs, dim=1)\n",
    "        probs = torch.exp(log_probs)\n",
    "        \n",
    "        # Focal weight: (1 - p_t)^gamma\n",
    "        focal_weight = (1 - probs) ** self.gamma\n",
    "        \n",
    "        # Compute focal loss with label smoothing\n",
    "        focal_loss = -focal_weight * smooth_labels * log_probs\n",
    "        \n",
    "        # Apply class weighting if provided\n",
    "        if self.alpha is not None:\n",
    "            alpha = self.alpha.to(inputs.device)\n",
    "            focal_loss = alpha.unsqueeze(0) * focal_loss\n",
    "        \n",
    "        return focal_loss.sum(dim=1).mean()\n",
    "\n",
    "# Calculate class weights (inverse frequency)\n",
    "class_counts = train_df['label'].value_counts().sort_index().values\n",
    "class_weights = 1.0 / class_counts\n",
    "class_weights = class_weights / class_weights.sum() * NUM_CLASSES  # Normalize\n",
    "\n",
    "print('üìä Class Weights:')\n",
    "for i, (name, weight) in enumerate(zip(LABEL_NAMES, class_weights)):\n",
    "    print(f'   {name}: {weight:.4f}')\n",
    "\n",
    "# Initialize loss function\n",
    "criterion = FocalLossWithLabelSmoothing(\n",
    "    num_classes=NUM_CLASSES,\n",
    "    gamma=2.0,  # Focal parameter\n",
    "    alpha=class_weights.tolist(),  # Class weights\n",
    "    smoothing=0.1  # Label smoothing\n",
    ")\n",
    "\n",
    "print('\\n‚úì Loss Function: Focal Loss + Label Smoothing')\n",
    "print(f'   - Gamma (focal): 2.0')\n",
    "print(f'   - Label Smoothing: 0.1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09944541",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è 7. Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab0433d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters yang dioptimasi untuk anti-overfitting\n",
    "EPOCHS = 10\n",
    "LEARNING_RATE = 2e-5\n",
    "WEIGHT_DECAY = 0.01  # L2 regularization\n",
    "WARMUP_RATIO = 0.1\n",
    "MAX_GRAD_NORM = 1.0  # Gradient clipping\n",
    "PATIENCE = 3  # Early stopping patience\n",
    "\n",
    "# Optimizer dengan weight decay (L2 regularization)\n",
    "optimizer = AdamW(\n",
    "    model.parameters(),\n",
    "    lr=LEARNING_RATE,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    "    eps=1e-8\n",
    ")\n",
    "\n",
    "# Learning rate scheduler dengan warmup\n",
    "total_steps = len(train_loader) * EPOCHS\n",
    "warmup_steps = int(total_steps * WARMUP_RATIO)\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=warmup_steps,\n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "print('=' * 60)\n",
    "print('‚öôÔ∏è  TRAINING CONFIGURATION')\n",
    "print('=' * 60)\n",
    "print(f'Epochs: {EPOCHS}')\n",
    "print(f'Batch size: {BATCH_SIZE}')\n",
    "print(f'Learning rate: {LEARNING_RATE}')\n",
    "print(f'Weight decay (L2): {WEIGHT_DECAY}')\n",
    "print(f'Warmup steps: {warmup_steps}')\n",
    "print(f'Total steps: {total_steps}')\n",
    "print(f'Gradient clipping: {MAX_GRAD_NORM}')\n",
    "print(f'Early stopping patience: {PATIENCE}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96239498",
   "metadata": {},
   "source": [
    "## üöÄ 8. Training Loop dengan Early Stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc96c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, data_loader, criterion, optimizer, scheduler, device, max_grad_norm):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    predictions = []\n",
    "    actual_labels = []\n",
    "    \n",
    "    progress_bar = tqdm(data_loader, desc='Training', leave=False)\n",
    "    \n",
    "    for batch in progress_bar:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(input_ids, attention_mask)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "        \n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        _, preds = torch.max(outputs, dim=1)\n",
    "        predictions.extend(preds.cpu().numpy())\n",
    "        actual_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        progress_bar.set_postfix({'loss': loss.item()})\n",
    "    \n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    accuracy = accuracy_score(actual_labels, predictions)\n",
    "    f1 = f1_score(actual_labels, predictions, average='macro')\n",
    "    \n",
    "    return avg_loss, accuracy, f1\n",
    "\n",
    "def eval_model(model, data_loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    predictions = []\n",
    "    actual_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(data_loader, desc='Evaluating', leave=False):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "            \n",
    "            outputs = model(input_ids, attention_mask)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            _, preds = torch.max(outputs, dim=1)\n",
    "            predictions.extend(preds.cpu().numpy())\n",
    "            actual_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    accuracy = accuracy_score(actual_labels, predictions)\n",
    "    f1 = f1_score(actual_labels, predictions, average='macro')\n",
    "    \n",
    "    return avg_loss, accuracy, f1, predictions, actual_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dc9ed47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training dengan Early Stopping\n",
    "print('=' * 60)\n",
    "print('üöÄ STARTING TRAINING')\n",
    "print('=' * 60)\n",
    "\n",
    "best_val_f1 = 0\n",
    "best_model_state = None\n",
    "patience_counter = 0\n",
    "history = {\n",
    "    'train_loss': [], 'train_acc': [], 'train_f1': [],\n",
    "    'val_loss': [], 'val_acc': [], 'val_f1': []\n",
    "}\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f'\\nüìÖ Epoch {epoch + 1}/{EPOCHS}')\n",
    "    print('-' * 40)\n",
    "    \n",
    "    # Training\n",
    "    train_loss, train_acc, train_f1 = train_epoch(\n",
    "        model, train_loader, criterion, optimizer, scheduler, device, MAX_GRAD_NORM\n",
    "    )\n",
    "    \n",
    "    # Validation\n",
    "    val_loss, val_acc, val_f1, _, _ = eval_model(\n",
    "        model, val_loader, criterion, device\n",
    "    )\n",
    "    \n",
    "    # Record history\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['train_acc'].append(train_acc)\n",
    "    history['train_f1'].append(train_f1)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['val_acc'].append(val_acc)\n",
    "    history['val_f1'].append(val_f1)\n",
    "    \n",
    "    # Print metrics\n",
    "    print(f'Train - Loss: {train_loss:.4f} | Acc: {train_acc:.4f} | F1: {train_f1:.4f}')\n",
    "    print(f'Val   - Loss: {val_loss:.4f} | Acc: {val_acc:.4f} | F1: {val_f1:.4f}')\n",
    "    \n",
    "    # Check overfitting indicator\n",
    "    overfit_gap = train_acc - val_acc\n",
    "    if overfit_gap > 0.1:\n",
    "        print(f'‚ö†Ô∏è  Overfitting warning! Gap: {overfit_gap:.4f}')\n",
    "    \n",
    "    # Early stopping check\n",
    "    if val_f1 > best_val_f1:\n",
    "        best_val_f1 = val_f1\n",
    "        best_model_state = copy.deepcopy(model.state_dict())\n",
    "        patience_counter = 0\n",
    "        print(f'‚úì New best model! Val F1: {val_f1:.4f}')\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        print(f'No improvement. Patience: {patience_counter}/{PATIENCE}')\n",
    "        \n",
    "        if patience_counter >= PATIENCE:\n",
    "            print(f'\\n‚èπÔ∏è  Early stopping triggered at epoch {epoch + 1}')\n",
    "            break\n",
    "\n",
    "# Load best model\n",
    "model.load_state_dict(best_model_state)\n",
    "print(f'\\n‚úì Loaded best model with Val F1: {best_val_f1:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e99a114",
   "metadata": {},
   "source": [
    "## üìä 9. Training History Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4140d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "epochs_range = range(1, len(history['train_loss']) + 1)\n",
    "\n",
    "# Loss\n",
    "axes[0].plot(epochs_range, history['train_loss'], 'b-', label='Train Loss', marker='o')\n",
    "axes[0].plot(epochs_range, history['val_loss'], 'r-', label='Val Loss', marker='o')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('Training vs Validation Loss')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Accuracy\n",
    "axes[1].plot(epochs_range, history['train_acc'], 'b-', label='Train Acc', marker='o')\n",
    "axes[1].plot(epochs_range, history['val_acc'], 'r-', label='Val Acc', marker='o')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Accuracy')\n",
    "axes[1].set_title('Training vs Validation Accuracy')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# F1 Score\n",
    "axes[2].plot(epochs_range, history['train_f1'], 'b-', label='Train F1', marker='o')\n",
    "axes[2].plot(epochs_range, history['val_f1'], 'r-', label='Val F1', marker='o')\n",
    "axes[2].set_xlabel('Epoch')\n",
    "axes[2].set_ylabel('F1 Score')\n",
    "axes[2].set_title('Training vs Validation F1 Score')\n",
    "axes[2].legend()\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('training_history_5class.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Overfitting analysis\n",
    "final_train_acc = history['train_acc'][-1]\n",
    "final_val_acc = history['val_acc'][-1]\n",
    "gap = final_train_acc - final_val_acc\n",
    "\n",
    "print('\\nüìä OVERFITTING ANALYSIS')\n",
    "print('=' * 40)\n",
    "print(f'Final Train Accuracy: {final_train_acc:.4f}')\n",
    "print(f'Final Val Accuracy: {final_val_acc:.4f}')\n",
    "print(f'Gap (Train - Val): {gap:.4f}')\n",
    "\n",
    "if gap < 0.05:\n",
    "    print('‚úì Model is NOT overfitting (gap < 5%)')\n",
    "elif gap < 0.10:\n",
    "    print('‚ö†Ô∏è  Model shows slight overfitting (gap 5-10%)')\n",
    "else:\n",
    "    print('‚ùå Model is overfitting (gap > 10%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "708e2b5c",
   "metadata": {},
   "source": [
    "## üß™ 10. Final Evaluation on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c4e5187",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test set\n",
    "print('=' * 60)\n",
    "print('üß™ FINAL EVALUATION ON TEST SET')\n",
    "print('=' * 60)\n",
    "\n",
    "test_loss, test_acc, test_f1, test_preds, test_labels = eval_model(\n",
    "    model, test_loader, criterion, device\n",
    ")\n",
    "\n",
    "print(f'\\nTest Results:')\n",
    "print(f'  Loss: {test_loss:.4f}')\n",
    "print(f'  Accuracy: {test_acc:.4f} ({test_acc*100:.2f}%)')\n",
    "print(f'  Macro F1: {test_f1:.4f}')\n",
    "\n",
    "# Detailed classification report\n",
    "print('\\nüìã CLASSIFICATION REPORT')\n",
    "print('=' * 60)\n",
    "print(classification_report(test_labels, test_preds, target_names=LABEL_NAMES, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e472f552",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "cm = confusion_matrix(test_labels, test_preds)\n",
    "cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Raw counts\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=LABEL_NAMES, yticklabels=LABEL_NAMES, ax=axes[0])\n",
    "axes[0].set_xlabel('Predicted')\n",
    "axes[0].set_ylabel('Actual')\n",
    "axes[0].set_title('Confusion Matrix (Counts)')\n",
    "\n",
    "# Normalized (percentages)\n",
    "sns.heatmap(cm_normalized, annot=True, fmt='.2%', cmap='Blues',\n",
    "            xticklabels=LABEL_NAMES, yticklabels=LABEL_NAMES, ax=axes[1])\n",
    "axes[1].set_xlabel('Predicted')\n",
    "axes[1].set_ylabel('Actual')\n",
    "axes[1].set_title('Confusion Matrix (Normalized)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('confusion_matrix_5class.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Analysis of confusion\n",
    "print('\\nüìä CONFUSION ANALYSIS')\n",
    "print('=' * 60)\n",
    "for i, label in enumerate(LABEL_NAMES):\n",
    "    correct = cm[i, i]\n",
    "    total = cm[i].sum()\n",
    "    acc = correct / total\n",
    "    print(f'{label}: {acc:.2%} correct ({correct}/{total})')\n",
    "    \n",
    "    # Show main confusions\n",
    "    for j, other_label in enumerate(LABEL_NAMES):\n",
    "        if i != j and cm[i, j] > 0:\n",
    "            conf_rate = cm[i, j] / total\n",
    "            if conf_rate > 0.05:  # Show if > 5% confusion\n",
    "                print(f'   ‚îî‚îÄ {conf_rate:.1%} confused with {other_label}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b14b38f8",
   "metadata": {},
   "source": [
    "## üíæ 11. Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf6f32c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create models directory\n",
    "os.makedirs('models', exist_ok=True)\n",
    "\n",
    "# Save model\n",
    "model_path = 'models/indobert_sentiment_5class_best.pt'\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'model_config': {\n",
    "        'model_name': MODEL_NAME,\n",
    "        'num_classes': NUM_CLASSES,\n",
    "        'max_len': MAX_LEN,\n",
    "        'label_map': LABEL_MAP,\n",
    "        'label_names': LABEL_NAMES\n",
    "    },\n",
    "    'training_history': history,\n",
    "    'test_metrics': {\n",
    "        'accuracy': test_acc,\n",
    "        'f1_score': test_f1,\n",
    "        'loss': test_loss\n",
    "    }\n",
    "}, model_path)\n",
    "\n",
    "print(f'‚úì Model saved to: {model_path}')\n",
    "\n",
    "# Save training config\n",
    "config = {\n",
    "    'model_name': MODEL_NAME,\n",
    "    'num_classes': NUM_CLASSES,\n",
    "    'max_len': MAX_LEN,\n",
    "    'batch_size': BATCH_SIZE,\n",
    "    'epochs_trained': len(history['train_loss']),\n",
    "    'learning_rate': LEARNING_RATE,\n",
    "    'weight_decay': WEIGHT_DECAY,\n",
    "    'dropout_rate': 0.3,\n",
    "    'label_smoothing': 0.1,\n",
    "    'focal_gamma': 2.0,\n",
    "    'frozen_layers': 6,\n",
    "    'test_accuracy': test_acc,\n",
    "    'test_f1': test_f1,\n",
    "    'label_map': LABEL_MAP,\n",
    "    'label_names': LABEL_NAMES\n",
    "}\n",
    "\n",
    "with open('models/training_config_5class.json', 'w') as f:\n",
    "    json.dump(config, f, indent=2)\n",
    "\n",
    "print(f'‚úì Config saved to: models/training_config_5class.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42c5cb31",
   "metadata": {},
   "source": [
    "## üîÆ 12. Prediction Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b3e5ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentiment(text, model, tokenizer, device, max_len=MAX_LEN):\n",
    "    \"\"\"\n",
    "    Predict sentiment for a single text.\n",
    "    Returns: (predicted_label, confidence, all_probabilities)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    encoding = tokenizer.encode_plus(\n",
    "        text,\n",
    "        add_special_tokens=True,\n",
    "        max_length=max_len,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_attention_mask=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    \n",
    "    input_ids = encoding['input_ids'].to(device)\n",
    "    attention_mask = encoding['attention_mask'].to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, attention_mask)\n",
    "        probs = F.softmax(outputs, dim=1)\n",
    "        confidence, predicted = torch.max(probs, dim=1)\n",
    "    \n",
    "    predicted_label = LABEL_NAMES[predicted.item()]\n",
    "    all_probs = {name: probs[0][i].item() for i, name in enumerate(LABEL_NAMES)}\n",
    "    \n",
    "    return predicted_label, confidence.item(), all_probs\n",
    "\n",
    "# Test predictions\n",
    "print('=' * 60)\n",
    "print('üîÆ SAMPLE PREDICTIONS')\n",
    "print('=' * 60)\n",
    "\n",
    "test_texts = [\n",
    "    \"Aplikasi ini sangat bagus dan membantu sekali!\",\n",
    "    \"Lumayan lah aplikasinya, cukup membantu\",\n",
    "    \"Biasa aja, tidak ada yang istimewa\",\n",
    "    \"Kurang bagus, sering error dan lambat\",\n",
    "    \"Aplikasi sampah! Sangat mengecewakan, tidak akan pakai lagi!\"\n",
    "]\n",
    "\n",
    "for text in test_texts:\n",
    "    label, conf, probs = predict_sentiment(text, model, tokenizer, device)\n",
    "    print(f'\\nüìù \"{text[:50]}...\"')\n",
    "    print(f'   Prediction: {label.upper()} (confidence: {conf:.2%})')\n",
    "    print(f'   Probabilities:')\n",
    "    for name, prob in sorted(probs.items(), key=lambda x: -x[1]):\n",
    "        bar = '‚ñà' * int(prob * 20)\n",
    "        print(f'      {name:15s}: {prob:.2%} {bar}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a88a0e",
   "metadata": {},
   "source": [
    "## üìà 13. Final Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60303d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('=' * 60)\n",
    "print('üìà FINAL TRAINING SUMMARY')\n",
    "print('=' * 60)\n",
    "\n",
    "print(f'''\n",
    "üéØ MODEL PERFORMANCE:\n",
    "   ‚Ä¢ Test Accuracy: {test_acc:.4f} ({test_acc*100:.2f}%)\n",
    "   ‚Ä¢ Test Macro F1: {test_f1:.4f}\n",
    "   \n",
    "üõ°Ô∏è ANTI-OVERFITTING TECHNIQUES USED:\n",
    "   ‚úì Balanced Dataset (2,500 per class)\n",
    "   ‚úì Dropout (0.3)\n",
    "   ‚úì Label Smoothing (0.1)\n",
    "   ‚úì Focal Loss (gamma=2.0)\n",
    "   ‚úì Weight Decay / L2 Regularization (0.01)\n",
    "   ‚úì Learning Rate Warmup\n",
    "   ‚úì Gradient Clipping (max_norm=1.0)\n",
    "   ‚úì Early Stopping (patience=3)\n",
    "   ‚úì Layer Freezing (6 of 12 BERT layers)\n",
    "   ‚úì Data Augmentation (random word dropout)\n",
    "   ‚úì Stratified Train/Val/Test Split\n",
    "\n",
    "üìä OVERFITTING CHECK:\n",
    "   ‚Ä¢ Train Accuracy: {history[\"train_acc\"][-1]:.4f}\n",
    "   ‚Ä¢ Val Accuracy: {history[\"val_acc\"][-1]:.4f}\n",
    "   ‚Ä¢ Gap: {history[\"train_acc\"][-1] - history[\"val_acc\"][-1]:.4f}\n",
    "   ‚Ä¢ Status: {\"‚úì Not Overfitting\" if (history[\"train_acc\"][-1] - history[\"val_acc\"][-1]) < 0.05 else \"‚ö†Ô∏è Check overfitting\"}\n",
    "\n",
    "üíæ SAVED FILES:\n",
    "   ‚Ä¢ Model: models/indobert_sentiment_5class_best.pt\n",
    "   ‚Ä¢ Config: models/training_config_5class.json\n",
    "   ‚Ä¢ Plots: training_history_5class.png, confusion_matrix_5class.png\n",
    "''')\n",
    "\n",
    "print('\\nüéâ Training Complete!')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
